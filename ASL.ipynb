{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ffB_uo1855IX"
      },
      "outputs": [],
      "source": [
        "#!gdown 1DswV_tU1--JlB9fkxJPPRL2dLCB7hCa7\n",
        "#!unzip \"/content/ASL.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQt_n3hVMa-I",
        "outputId": "04ec505c-1b87-4ea8-f872-3619607a8f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F:\\Insaid\\ASL\n"
          ]
        }
      ],
      "source": [
        "cd \"F:\\\\Insaid\\\\ASL\\\\\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1vsimJHn55Ia"
      },
      "outputs": [],
      "source": [
        "import keras,os\n",
        "from tensorflow.keras import layers,regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import BatchNormalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SzaQwXb8S_QR"
      },
      "outputs": [],
      "source": [
        "#!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FiIUM-NjRh1X"
      },
      "outputs": [],
      "source": [
        "#import splitfolders # or import splitfolders\n",
        "#input_folder = \"F:\\\\Insaid\\\\ASL\\data\\\\asl_alphabet_train\\\\asl_alphabet_train\\\\\"\n",
        "#output = \"F:\\\\Insaid\\\\ASL\\\\data\\\\op\\\\\" #where you want the split datasets saved. one will be created if it does not exist or none is set\n",
        "\n",
        "#splitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suzt5Xlr55Ib",
        "outputId": "7f53ca4f-510f-4618-9ed5-58f10da2217e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 69600 images belonging to 29 classes.\n",
            "Found 17400 images belonging to 29 classes.\n"
          ]
        }
      ],
      "source": [
        "trdata = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
        "tsdata = ImageDataGenerator( rescale = 1.0/255.)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator  = trdata.flow_from_directory(\"F:\\\\Insaid\\\\ASL\\\\data\\\\op\\\\train\\\\\", batch_size = 20,target_size = (224, 224))\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator  = tsdata.flow_from_directory( \"F:\\\\Insaid\\\\ASL\\\\data\\\\op\\\\val\\\\\",  batch_size = 20, target_size = (224, 224))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CeWydFVVhx9",
        "outputId": "7b4255b0-6758-48fa-9730-9d918bfbed97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26, 'nothing': 27, 'space': 28}\n"
          ]
        }
      ],
      "source": [
        "print(validation_generator.class_indices) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7bwvAPeN55Ib"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "base_model = VGG16(input_shape = (224, 224, 3), # Shape of our images\n",
        "include_top = False, # Leave out the last fully connected layer\n",
        "weights = 'imagenet')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jtN9NR_j55Ic"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2UM3PdO55Ic",
        "outputId": "650d9897-3473-4c77-bbbc-e32a8eecd3a9"
      },
      "outputs": [],
      "source": [
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(base_model.output)\n",
        "\n",
        "# Add a fully connected layer with 512 hidden units and ReLU activation\n",
        "x = layers.Dense(512, activation=(keras.layers.LeakyReLU(alpha=0.01)),kernel_regularizer=(regularizers.l2(0.001)))(x)\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = layers.Dense(256, activation=(keras.layers.LeakyReLU(alpha=0.01)),kernel_regularizer=(regularizers.l2(0.001)))(x)\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(.2)(x)\n",
        "x = layers.Dense(128, activation=(keras.layers.LeakyReLU(alpha=0.01)),kernel_regularizer=(regularizers.l2(0.001)))(x)\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(.2)(x)\n",
        "\n",
        "# Add a final sigmoid layer with 1 node for classification output\n",
        "x = layers.Dense(29, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.models.Model(base_model.input, x)\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(lr=0.0001), loss = 'categorical_crossentropy',metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn01kaY_JS3v",
        "outputId": "47e7a370-c20b-4674-c0e3-370a08cb1d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "checkpoint = keras.callbacks.ModelCheckpoint(\"Vgg16_ASL.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", min_delta=0, patience=100, verbose=1, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SgaDYAr55Id",
        "outputId": "ab87ddbd-33f7-4529-8039-42b121ec0d5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mailp\\AppData\\Local\\Temp\\ipykernel_3420\\2786403754.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  hist = model.fit_generator(generator= train_generator, steps_per_epoch= 10, epochs= 1000, validation_data= validation_generator, validation_steps=2, callbacks=[checkpoint,early])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 6.1229 - accuracy: 0.0500\n",
            "Epoch 1: val_loss improved from inf to 5.49043, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 21s 1s/step - loss: 6.1229 - accuracy: 0.0500 - val_loss: 5.4904 - val_accuracy: 0.0500\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 6.0116 - accuracy: 0.0350\n",
            "Epoch 2: val_loss improved from 5.49043 to 5.47610, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 6.0116 - accuracy: 0.0350 - val_loss: 5.4761 - val_accuracy: 0.0500\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 6.1416 - accuracy: 0.0300\n",
            "Epoch 3: val_loss improved from 5.47610 to 5.42529, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 6.1416 - accuracy: 0.0300 - val_loss: 5.4253 - val_accuracy: 0.0750\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 6.1084 - accuracy: 0.0150\n",
            "Epoch 4: val_loss did not improve from 5.42529\n",
            "10/10 [==============================] - 14s 1s/step - loss: 6.1084 - accuracy: 0.0150 - val_loss: 5.4295 - val_accuracy: 0.1000\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 6.0192 - accuracy: 0.0300\n",
            "Epoch 5: val_loss did not improve from 5.42529\n",
            "10/10 [==============================] - 17s 2s/step - loss: 6.0192 - accuracy: 0.0300 - val_loss: 5.5373 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 6.0229 - accuracy: 0.0350\n",
            "Epoch 6: val_loss improved from 5.42529 to 5.42372, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 6.0229 - accuracy: 0.0350 - val_loss: 5.4237 - val_accuracy: 0.0750\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.9261 - accuracy: 0.0350\n",
            "Epoch 7: val_loss did not improve from 5.42372\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.9261 - accuracy: 0.0350 - val_loss: 5.4295 - val_accuracy: 0.0500\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.9945 - accuracy: 0.0450\n",
            "Epoch 8: val_loss improved from 5.42372 to 5.34077, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 9s 867ms/step - loss: 5.9945 - accuracy: 0.0450 - val_loss: 5.3408 - val_accuracy: 0.1000\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.8610 - accuracy: 0.0400\n",
            "Epoch 9: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 7s 685ms/step - loss: 5.8610 - accuracy: 0.0400 - val_loss: 5.4454 - val_accuracy: 0.0750\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.9340 - accuracy: 0.0300\n",
            "Epoch 10: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 7s 698ms/step - loss: 5.9340 - accuracy: 0.0300 - val_loss: 5.5026 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.8932 - accuracy: 0.0450\n",
            "Epoch 11: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 11s 1s/step - loss: 5.8932 - accuracy: 0.0450 - val_loss: 5.5454 - val_accuracy: 0.0250\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.9557 - accuracy: 0.0500\n",
            "Epoch 12: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 10s 1s/step - loss: 5.9557 - accuracy: 0.0500 - val_loss: 5.5551 - val_accuracy: 0.0250\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7270 - accuracy: 0.0550\n",
            "Epoch 13: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 6s 586ms/step - loss: 5.7270 - accuracy: 0.0550 - val_loss: 5.4973 - val_accuracy: 0.0250\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.8887 - accuracy: 0.0250\n",
            "Epoch 14: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 8s 795ms/step - loss: 5.8887 - accuracy: 0.0250 - val_loss: 5.3770 - val_accuracy: 0.0250\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.8027 - accuracy: 0.0550\n",
            "Epoch 15: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 7s 701ms/step - loss: 5.8027 - accuracy: 0.0550 - val_loss: 5.4964 - val_accuracy: 0.0500\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.8085 - accuracy: 0.0700\n",
            "Epoch 16: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 11s 1s/step - loss: 5.8085 - accuracy: 0.0700 - val_loss: 5.4251 - val_accuracy: 0.0500\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7465 - accuracy: 0.0650\n",
            "Epoch 17: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 6s 549ms/step - loss: 5.7465 - accuracy: 0.0650 - val_loss: 5.4127 - val_accuracy: 0.0250\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.8534 - accuracy: 0.0450\n",
            "Epoch 18: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 10s 1s/step - loss: 5.8534 - accuracy: 0.0450 - val_loss: 5.3811 - val_accuracy: 0.0750\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7114 - accuracy: 0.0350\n",
            "Epoch 19: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 6s 565ms/step - loss: 5.7114 - accuracy: 0.0350 - val_loss: 5.5334 - val_accuracy: 0.0750\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.8017 - accuracy: 0.0200\n",
            "Epoch 20: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 11s 1s/step - loss: 5.8017 - accuracy: 0.0200 - val_loss: 5.4322 - val_accuracy: 0.0750\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6439 - accuracy: 0.0450\n",
            "Epoch 21: val_loss did not improve from 5.34077\n",
            "10/10 [==============================] - 10s 858ms/step - loss: 5.6439 - accuracy: 0.0450 - val_loss: 5.6066 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7802 - accuracy: 0.0650\n",
            "Epoch 22: val_loss improved from 5.34077 to 5.34001, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 5.7802 - accuracy: 0.0650 - val_loss: 5.3400 - val_accuracy: 0.1250\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7716 - accuracy: 0.0400\n",
            "Epoch 23: val_loss did not improve from 5.34001\n",
            "10/10 [==============================] - 9s 918ms/step - loss: 5.7716 - accuracy: 0.0400 - val_loss: 5.6707 - val_accuracy: 0.0500\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6712 - accuracy: 0.0650\n",
            "Epoch 24: val_loss did not improve from 5.34001\n",
            "10/10 [==============================] - 6s 588ms/step - loss: 5.6712 - accuracy: 0.0650 - val_loss: 5.6598 - val_accuracy: 0.0250\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.8018 - accuracy: 0.0750\n",
            "Epoch 25: val_loss did not improve from 5.34001\n",
            "10/10 [==============================] - 8s 807ms/step - loss: 5.8018 - accuracy: 0.0750 - val_loss: 5.5780 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6136 - accuracy: 0.0500\n",
            "Epoch 26: val_loss improved from 5.34001 to 5.33326, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 22s 2s/step - loss: 5.6136 - accuracy: 0.0500 - val_loss: 5.3333 - val_accuracy: 0.1250\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7275 - accuracy: 0.0450\n",
            "Epoch 27: val_loss improved from 5.33326 to 5.31575, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 5.7275 - accuracy: 0.0450 - val_loss: 5.3157 - val_accuracy: 0.1250\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.8948 - accuracy: 0.0450\n",
            "Epoch 28: val_loss did not improve from 5.31575\n",
            "10/10 [==============================] - 7s 721ms/step - loss: 5.8948 - accuracy: 0.0450 - val_loss: 5.4460 - val_accuracy: 0.1250\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7731 - accuracy: 0.0450\n",
            "Epoch 29: val_loss did not improve from 5.31575\n",
            "10/10 [==============================] - 8s 725ms/step - loss: 5.7731 - accuracy: 0.0450 - val_loss: 5.4955 - val_accuracy: 0.0750\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6713 - accuracy: 0.0700\n",
            "Epoch 30: val_loss did not improve from 5.31575\n",
            "10/10 [==============================] - 14s 1s/step - loss: 5.6713 - accuracy: 0.0700 - val_loss: 5.3411 - val_accuracy: 0.0750\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6900 - accuracy: 0.0600\n",
            "Epoch 31: val_loss improved from 5.31575 to 5.12243, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.6900 - accuracy: 0.0600 - val_loss: 5.1224 - val_accuracy: 0.2000\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6620 - accuracy: 0.0400\n",
            "Epoch 32: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 8s 773ms/step - loss: 5.6620 - accuracy: 0.0400 - val_loss: 5.4110 - val_accuracy: 0.1250\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7612 - accuracy: 0.0650\n",
            "Epoch 33: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 9s 936ms/step - loss: 5.7612 - accuracy: 0.0650 - val_loss: 5.4843 - val_accuracy: 0.0500\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6711 - accuracy: 0.0500\n",
            "Epoch 34: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 8s 847ms/step - loss: 5.6711 - accuracy: 0.0500 - val_loss: 5.4071 - val_accuracy: 0.0750\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6045 - accuracy: 0.0650\n",
            "Epoch 35: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 8s 792ms/step - loss: 5.6045 - accuracy: 0.0650 - val_loss: 5.1886 - val_accuracy: 0.1750\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5630 - accuracy: 0.0800\n",
            "Epoch 36: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.5630 - accuracy: 0.0800 - val_loss: 5.2309 - val_accuracy: 0.1000\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6001 - accuracy: 0.0600\n",
            "Epoch 37: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 8s 771ms/step - loss: 5.6001 - accuracy: 0.0600 - val_loss: 5.3287 - val_accuracy: 0.0250\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5567 - accuracy: 0.0600\n",
            "Epoch 38: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 8s 747ms/step - loss: 5.5567 - accuracy: 0.0600 - val_loss: 5.4590 - val_accuracy: 0.1000\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7141 - accuracy: 0.0700\n",
            "Epoch 39: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 15s 2s/step - loss: 5.7141 - accuracy: 0.0700 - val_loss: 5.4937 - val_accuracy: 0.0500\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5747 - accuracy: 0.0500\n",
            "Epoch 40: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 10s 985ms/step - loss: 5.5747 - accuracy: 0.0500 - val_loss: 5.2945 - val_accuracy: 0.1500\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6580 - accuracy: 0.0900\n",
            "Epoch 41: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 11s 1s/step - loss: 5.6580 - accuracy: 0.0900 - val_loss: 5.5117 - val_accuracy: 0.0250\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6102 - accuracy: 0.0750\n",
            "Epoch 42: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 8s 756ms/step - loss: 5.6102 - accuracy: 0.0750 - val_loss: 5.2742 - val_accuracy: 0.0750\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6585 - accuracy: 0.0750\n",
            "Epoch 43: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 9s 785ms/step - loss: 5.6585 - accuracy: 0.0750 - val_loss: 5.4034 - val_accuracy: 0.0250\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5912 - accuracy: 0.0600\n",
            "Epoch 44: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 7s 644ms/step - loss: 5.5912 - accuracy: 0.0600 - val_loss: 5.2002 - val_accuracy: 0.2000\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.7219 - accuracy: 0.0800\n",
            "Epoch 45: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 7s 679ms/step - loss: 5.7219 - accuracy: 0.0800 - val_loss: 5.2288 - val_accuracy: 0.1250\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4862 - accuracy: 0.0600\n",
            "Epoch 46: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 10s 1s/step - loss: 5.4862 - accuracy: 0.0600 - val_loss: 5.3911 - val_accuracy: 0.1500\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5550 - accuracy: 0.0750\n",
            "Epoch 47: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 6s 606ms/step - loss: 5.5550 - accuracy: 0.0750 - val_loss: 5.3147 - val_accuracy: 0.1250\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5592 - accuracy: 0.0750\n",
            "Epoch 48: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 9s 827ms/step - loss: 5.5592 - accuracy: 0.0750 - val_loss: 5.2798 - val_accuracy: 0.1000\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6226 - accuracy: 0.0500\n",
            "Epoch 49: val_loss did not improve from 5.12243\n",
            "10/10 [==============================] - 11s 1s/step - loss: 5.6226 - accuracy: 0.0500 - val_loss: 5.5876 - val_accuracy: 0.0500\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5947 - accuracy: 0.0850\n",
            "Epoch 50: val_loss improved from 5.12243 to 5.01417, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 17s 2s/step - loss: 5.5947 - accuracy: 0.0850 - val_loss: 5.0142 - val_accuracy: 0.1750\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5274 - accuracy: 0.0650\n",
            "Epoch 51: val_loss did not improve from 5.01417\n",
            "10/10 [==============================] - 9s 926ms/step - loss: 5.5274 - accuracy: 0.0650 - val_loss: 5.0223 - val_accuracy: 0.1750\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4488 - accuracy: 0.0650\n",
            "Epoch 52: val_loss did not improve from 5.01417\n",
            "10/10 [==============================] - 6s 564ms/step - loss: 5.4488 - accuracy: 0.0650 - val_loss: 5.2945 - val_accuracy: 0.1000\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4541 - accuracy: 0.0850\n",
            "Epoch 53: val_loss did not improve from 5.01417\n",
            "10/10 [==============================] - 7s 726ms/step - loss: 5.4541 - accuracy: 0.0850 - val_loss: 5.3184 - val_accuracy: 0.1250\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.6088 - accuracy: 0.0500\n",
            "Epoch 54: val_loss improved from 5.01417 to 4.85494, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 21s 2s/step - loss: 5.6088 - accuracy: 0.0500 - val_loss: 4.8549 - val_accuracy: 0.1250\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5739 - accuracy: 0.0650\n",
            "Epoch 55: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 7s 643ms/step - loss: 5.5739 - accuracy: 0.0650 - val_loss: 5.2162 - val_accuracy: 0.1000\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3883 - accuracy: 0.1000\n",
            "Epoch 56: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 12s 976ms/step - loss: 5.3883 - accuracy: 0.1000 - val_loss: 5.0434 - val_accuracy: 0.1750\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4611 - accuracy: 0.0650\n",
            "Epoch 57: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 7s 624ms/step - loss: 5.4611 - accuracy: 0.0650 - val_loss: 5.2181 - val_accuracy: 0.1000\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4498 - accuracy: 0.0700\n",
            "Epoch 58: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 7s 607ms/step - loss: 5.4498 - accuracy: 0.0700 - val_loss: 5.1729 - val_accuracy: 0.0750\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4576 - accuracy: 0.0600\n",
            "Epoch 59: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 13s 1s/step - loss: 5.4576 - accuracy: 0.0600 - val_loss: 5.4435 - val_accuracy: 0.1000\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4532 - accuracy: 0.0950\n",
            "Epoch 60: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 6s 600ms/step - loss: 5.4532 - accuracy: 0.0950 - val_loss: 4.9427 - val_accuracy: 0.1500\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4611 - accuracy: 0.0800\n",
            "Epoch 61: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 11s 1s/step - loss: 5.4611 - accuracy: 0.0800 - val_loss: 5.1051 - val_accuracy: 0.0500\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3418 - accuracy: 0.0850\n",
            "Epoch 62: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 8s 792ms/step - loss: 5.3418 - accuracy: 0.0850 - val_loss: 5.0906 - val_accuracy: 0.1500\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4019 - accuracy: 0.0950\n",
            "Epoch 63: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.4019 - accuracy: 0.0950 - val_loss: 5.1095 - val_accuracy: 0.1250\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2812 - accuracy: 0.1350\n",
            "Epoch 64: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 10s 828ms/step - loss: 5.2812 - accuracy: 0.1350 - val_loss: 5.3445 - val_accuracy: 0.1500\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3627 - accuracy: 0.1100\n",
            "Epoch 65: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 11s 1s/step - loss: 5.3627 - accuracy: 0.1100 - val_loss: 4.8786 - val_accuracy: 0.2750\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4721 - accuracy: 0.1050\n",
            "Epoch 66: val_loss did not improve from 4.85494\n",
            "10/10 [==============================] - 9s 824ms/step - loss: 5.4721 - accuracy: 0.1050 - val_loss: 5.1767 - val_accuracy: 0.2000\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4752 - accuracy: 0.0900\n",
            "Epoch 67: val_loss improved from 4.85494 to 4.76137, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.4752 - accuracy: 0.0900 - val_loss: 4.7614 - val_accuracy: 0.1750\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5897 - accuracy: 0.0700\n",
            "Epoch 68: val_loss did not improve from 4.76137\n",
            "10/10 [==============================] - 8s 774ms/step - loss: 5.5897 - accuracy: 0.0700 - val_loss: 4.9379 - val_accuracy: 0.1750\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4952 - accuracy: 0.0800\n",
            "Epoch 69: val_loss did not improve from 4.76137\n",
            "10/10 [==============================] - 10s 893ms/step - loss: 5.4952 - accuracy: 0.0800 - val_loss: 5.0258 - val_accuracy: 0.1000\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2790 - accuracy: 0.1250\n",
            "Epoch 70: val_loss did not improve from 4.76137\n",
            "10/10 [==============================] - 16s 2s/step - loss: 5.2790 - accuracy: 0.1250 - val_loss: 4.9514 - val_accuracy: 0.1750\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5494 - accuracy: 0.0800\n",
            "Epoch 71: val_loss improved from 4.76137 to 4.61730, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 9s 887ms/step - loss: 5.5494 - accuracy: 0.0800 - val_loss: 4.6173 - val_accuracy: 0.3000\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.5454 - accuracy: 0.0700\n",
            "Epoch 72: val_loss did not improve from 4.61730\n",
            "10/10 [==============================] - 10s 1s/step - loss: 5.5454 - accuracy: 0.0700 - val_loss: 4.9960 - val_accuracy: 0.1250\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3733 - accuracy: 0.1450\n",
            "Epoch 73: val_loss did not improve from 4.61730\n",
            "10/10 [==============================] - 6s 540ms/step - loss: 5.3733 - accuracy: 0.1450 - val_loss: 4.7454 - val_accuracy: 0.2250\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2845 - accuracy: 0.0850\n",
            "Epoch 74: val_loss improved from 4.61730 to 4.58317, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 15s 1s/step - loss: 5.2845 - accuracy: 0.0850 - val_loss: 4.5832 - val_accuracy: 0.2500\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4041 - accuracy: 0.1000\n",
            "Epoch 75: val_loss did not improve from 4.58317\n",
            "10/10 [==============================] - 9s 861ms/step - loss: 5.4041 - accuracy: 0.1000 - val_loss: 4.8901 - val_accuracy: 0.1750\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4404 - accuracy: 0.0600\n",
            "Epoch 76: val_loss did not improve from 4.58317\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.4404 - accuracy: 0.0600 - val_loss: 5.1179 - val_accuracy: 0.1000\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1923 - accuracy: 0.1250\n",
            "Epoch 77: val_loss did not improve from 4.58317\n",
            "10/10 [==============================] - 14s 1s/step - loss: 5.1923 - accuracy: 0.1250 - val_loss: 4.7401 - val_accuracy: 0.2500\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4925 - accuracy: 0.0600\n",
            "Epoch 78: val_loss did not improve from 4.58317\n",
            "10/10 [==============================] - 10s 973ms/step - loss: 5.4925 - accuracy: 0.0600 - val_loss: 4.9984 - val_accuracy: 0.1250\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3172 - accuracy: 0.0950\n",
            "Epoch 79: val_loss did not improve from 4.58317\n",
            "10/10 [==============================] - 7s 606ms/step - loss: 5.3172 - accuracy: 0.0950 - val_loss: 5.1368 - val_accuracy: 0.1000\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2984 - accuracy: 0.1300\n",
            "Epoch 80: val_loss did not improve from 4.58317\n",
            "10/10 [==============================] - 11s 984ms/step - loss: 5.2984 - accuracy: 0.1300 - val_loss: 4.8735 - val_accuracy: 0.1500\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3773 - accuracy: 0.0950\n",
            "Epoch 81: val_loss did not improve from 4.58317\n",
            "10/10 [==============================] - 8s 813ms/step - loss: 5.3773 - accuracy: 0.0950 - val_loss: 4.8945 - val_accuracy: 0.1250\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3150 - accuracy: 0.1100\n",
            "Epoch 82: val_loss improved from 4.58317 to 4.57837, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 5.3150 - accuracy: 0.1100 - val_loss: 4.5784 - val_accuracy: 0.2750\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3071 - accuracy: 0.0500\n",
            "Epoch 83: val_loss did not improve from 4.57837\n",
            "10/10 [==============================] - 10s 954ms/step - loss: 5.3071 - accuracy: 0.0500 - val_loss: 4.8395 - val_accuracy: 0.1250\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4774 - accuracy: 0.0800\n",
            "Epoch 84: val_loss did not improve from 4.57837\n",
            "10/10 [==============================] - 6s 551ms/step - loss: 5.4774 - accuracy: 0.0800 - val_loss: 4.6459 - val_accuracy: 0.2750\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2512 - accuracy: 0.1050\n",
            "Epoch 85: val_loss did not improve from 4.57837\n",
            "10/10 [==============================] - 14s 1s/step - loss: 5.2512 - accuracy: 0.1050 - val_loss: 4.7279 - val_accuracy: 0.1750\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1705 - accuracy: 0.1150\n",
            "Epoch 86: val_loss did not improve from 4.57837\n",
            "10/10 [==============================] - 9s 939ms/step - loss: 5.1705 - accuracy: 0.1150 - val_loss: 4.8306 - val_accuracy: 0.1750\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2287 - accuracy: 0.0900\n",
            "Epoch 87: val_loss improved from 4.57837 to 4.57236, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 14s 1s/step - loss: 5.2287 - accuracy: 0.0900 - val_loss: 4.5724 - val_accuracy: 0.2250\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3297 - accuracy: 0.0950\n",
            "Epoch 88: val_loss did not improve from 4.57236\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.3297 - accuracy: 0.0950 - val_loss: 4.7194 - val_accuracy: 0.1500\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2142 - accuracy: 0.1500\n",
            "Epoch 89: val_loss did not improve from 4.57236\n",
            "10/10 [==============================] - 9s 883ms/step - loss: 5.2142 - accuracy: 0.1500 - val_loss: 4.8085 - val_accuracy: 0.2500\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.4805 - accuracy: 0.0700\n",
            "Epoch 90: val_loss improved from 4.57236 to 4.49321, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 5.4805 - accuracy: 0.0700 - val_loss: 4.4932 - val_accuracy: 0.2750\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.3171 - accuracy: 0.1250\n",
            "Epoch 91: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 7s 735ms/step - loss: 5.3171 - accuracy: 0.1250 - val_loss: 4.5006 - val_accuracy: 0.2750\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2128 - accuracy: 0.0900\n",
            "Epoch 92: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 13s 1s/step - loss: 5.2128 - accuracy: 0.0900 - val_loss: 4.5963 - val_accuracy: 0.2250\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2965 - accuracy: 0.1350\n",
            "Epoch 93: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 6s 549ms/step - loss: 5.2965 - accuracy: 0.1350 - val_loss: 4.8984 - val_accuracy: 0.2000\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2519 - accuracy: 0.1050\n",
            "Epoch 94: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 14s 1s/step - loss: 5.2519 - accuracy: 0.1050 - val_loss: 4.6706 - val_accuracy: 0.1750\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2138 - accuracy: 0.0850\n",
            "Epoch 95: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 9s 940ms/step - loss: 5.2138 - accuracy: 0.0850 - val_loss: 4.5850 - val_accuracy: 0.2250\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1609 - accuracy: 0.1450\n",
            "Epoch 96: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 11s 790ms/step - loss: 5.1609 - accuracy: 0.1450 - val_loss: 4.9752 - val_accuracy: 0.1000\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2072 - accuracy: 0.0800\n",
            "Epoch 97: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 10s 888ms/step - loss: 5.2072 - accuracy: 0.0800 - val_loss: 4.4972 - val_accuracy: 0.2250\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2636 - accuracy: 0.1300\n",
            "Epoch 98: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 8s 793ms/step - loss: 5.2636 - accuracy: 0.1300 - val_loss: 4.6269 - val_accuracy: 0.2750\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1881 - accuracy: 0.1250\n",
            "Epoch 99: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 7s 662ms/step - loss: 5.1881 - accuracy: 0.1250 - val_loss: 4.6010 - val_accuracy: 0.2750\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1661 - accuracy: 0.1050\n",
            "Epoch 100: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 9s 907ms/step - loss: 5.1661 - accuracy: 0.1050 - val_loss: 4.5909 - val_accuracy: 0.2000\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0985 - accuracy: 0.1300\n",
            "Epoch 101: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 13s 1s/step - loss: 5.0985 - accuracy: 0.1300 - val_loss: 4.6462 - val_accuracy: 0.2250\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1736 - accuracy: 0.1450\n",
            "Epoch 102: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 13s 1s/step - loss: 5.1736 - accuracy: 0.1450 - val_loss: 4.7407 - val_accuracy: 0.1750\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2365 - accuracy: 0.1050\n",
            "Epoch 103: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 8s 754ms/step - loss: 5.2365 - accuracy: 0.1050 - val_loss: 4.5686 - val_accuracy: 0.2750\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1337 - accuracy: 0.1250\n",
            "Epoch 104: val_loss did not improve from 4.49321\n",
            "10/10 [==============================] - 7s 625ms/step - loss: 5.1337 - accuracy: 0.1250 - val_loss: 4.7812 - val_accuracy: 0.1750\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1951 - accuracy: 0.1250\n",
            "Epoch 105: val_loss improved from 4.49321 to 4.46596, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 16s 1s/step - loss: 5.1951 - accuracy: 0.1250 - val_loss: 4.4660 - val_accuracy: 0.3250\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1976 - accuracy: 0.1300\n",
            "Epoch 106: val_loss did not improve from 4.46596\n",
            "10/10 [==============================] - 9s 914ms/step - loss: 5.1976 - accuracy: 0.1300 - val_loss: 4.5572 - val_accuracy: 0.2750\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.2693 - accuracy: 0.0850\n",
            "Epoch 107: val_loss did not improve from 4.46596\n",
            "10/10 [==============================] - 16s 1s/step - loss: 5.2693 - accuracy: 0.0850 - val_loss: 4.4794 - val_accuracy: 0.2250\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1608 - accuracy: 0.1800\n",
            "Epoch 108: val_loss did not improve from 4.46596\n",
            "10/10 [==============================] - 7s 605ms/step - loss: 5.1608 - accuracy: 0.1800 - val_loss: 4.6653 - val_accuracy: 0.2000\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0890 - accuracy: 0.1450\n",
            "Epoch 109: val_loss did not improve from 4.46596\n",
            "10/10 [==============================] - 9s 854ms/step - loss: 5.0890 - accuracy: 0.1450 - val_loss: 4.7483 - val_accuracy: 0.1500\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1393 - accuracy: 0.1350\n",
            "Epoch 110: val_loss did not improve from 4.46596\n",
            "10/10 [==============================] - 8s 752ms/step - loss: 5.1393 - accuracy: 0.1350 - val_loss: 4.6718 - val_accuracy: 0.1750\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1228 - accuracy: 0.1100\n",
            "Epoch 111: val_loss improved from 4.46596 to 4.37463, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 5.1228 - accuracy: 0.1100 - val_loss: 4.3746 - val_accuracy: 0.2250\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1693 - accuracy: 0.0850\n",
            "Epoch 112: val_loss did not improve from 4.37463\n",
            "10/10 [==============================] - 7s 653ms/step - loss: 5.1693 - accuracy: 0.0850 - val_loss: 4.4016 - val_accuracy: 0.2250\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1488 - accuracy: 0.1250\n",
            "Epoch 113: val_loss did not improve from 4.37463\n",
            "10/10 [==============================] - 6s 587ms/step - loss: 5.1488 - accuracy: 0.1250 - val_loss: 4.5632 - val_accuracy: 0.2750\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.1582 - accuracy: 0.1100\n",
            "Epoch 114: val_loss did not improve from 4.37463\n",
            "10/10 [==============================] - 8s 759ms/step - loss: 5.1582 - accuracy: 0.1100 - val_loss: 4.5193 - val_accuracy: 0.2000\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0565 - accuracy: 0.1650\n",
            "Epoch 115: val_loss did not improve from 4.37463\n",
            "10/10 [==============================] - 16s 2s/step - loss: 5.0565 - accuracy: 0.1650 - val_loss: 4.6712 - val_accuracy: 0.2750\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0044 - accuracy: 0.1800\n",
            "Epoch 116: val_loss improved from 4.37463 to 4.37224, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 14s 1s/step - loss: 5.0044 - accuracy: 0.1800 - val_loss: 4.3722 - val_accuracy: 0.3000\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0078 - accuracy: 0.1200\n",
            "Epoch 117: val_loss improved from 4.37224 to 4.24901, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.0078 - accuracy: 0.1200 - val_loss: 4.2490 - val_accuracy: 0.2750\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0737 - accuracy: 0.1250\n",
            "Epoch 118: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.0737 - accuracy: 0.1250 - val_loss: 4.4314 - val_accuracy: 0.2750\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9998 - accuracy: 0.1250\n",
            "Epoch 119: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.9998 - accuracy: 0.1250 - val_loss: 4.3144 - val_accuracy: 0.3000\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9943 - accuracy: 0.1550\n",
            "Epoch 120: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 6s 540ms/step - loss: 4.9943 - accuracy: 0.1550 - val_loss: 4.5850 - val_accuracy: 0.1500\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0463 - accuracy: 0.1550\n",
            "Epoch 121: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 14s 1s/step - loss: 5.0463 - accuracy: 0.1550 - val_loss: 4.2616 - val_accuracy: 0.3250\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9979 - accuracy: 0.1000\n",
            "Epoch 122: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.9979 - accuracy: 0.1000 - val_loss: 4.4033 - val_accuracy: 0.2750\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0477 - accuracy: 0.1300\n",
            "Epoch 123: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 9s 898ms/step - loss: 5.0477 - accuracy: 0.1300 - val_loss: 4.3209 - val_accuracy: 0.3750\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9102 - accuracy: 0.1500\n",
            "Epoch 124: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 7s 718ms/step - loss: 4.9102 - accuracy: 0.1500 - val_loss: 4.2607 - val_accuracy: 0.3250\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0261 - accuracy: 0.1450\n",
            "Epoch 125: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 7s 680ms/step - loss: 5.0261 - accuracy: 0.1450 - val_loss: 4.5034 - val_accuracy: 0.2000\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9015 - accuracy: 0.1650\n",
            "Epoch 126: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 8s 829ms/step - loss: 4.9015 - accuracy: 0.1650 - val_loss: 4.3635 - val_accuracy: 0.2500\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0256 - accuracy: 0.1300\n",
            "Epoch 127: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 10s 932ms/step - loss: 5.0256 - accuracy: 0.1300 - val_loss: 4.2747 - val_accuracy: 0.2750\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8060 - accuracy: 0.2100\n",
            "Epoch 128: val_loss did not improve from 4.24901\n",
            "10/10 [==============================] - 18s 2s/step - loss: 4.8060 - accuracy: 0.2100 - val_loss: 4.4580 - val_accuracy: 0.3500\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0133 - accuracy: 0.1150\n",
            "Epoch 129: val_loss improved from 4.24901 to 4.19732, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 14s 1s/step - loss: 5.0133 - accuracy: 0.1150 - val_loss: 4.1973 - val_accuracy: 0.3000\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9140 - accuracy: 0.1850\n",
            "Epoch 130: val_loss did not improve from 4.19732\n",
            "10/10 [==============================] - 15s 1s/step - loss: 4.9140 - accuracy: 0.1850 - val_loss: 4.6729 - val_accuracy: 0.2250\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9997 - accuracy: 0.1400\n",
            "Epoch 131: val_loss did not improve from 4.19732\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.9997 - accuracy: 0.1400 - val_loss: 4.2985 - val_accuracy: 0.2750\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9073 - accuracy: 0.1550\n",
            "Epoch 132: val_loss did not improve from 4.19732\n",
            "10/10 [==============================] - 10s 1s/step - loss: 4.9073 - accuracy: 0.1550 - val_loss: 4.3540 - val_accuracy: 0.2750\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8609 - accuracy: 0.1850\n",
            "Epoch 133: val_loss improved from 4.19732 to 4.18713, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 17s 2s/step - loss: 4.8609 - accuracy: 0.1850 - val_loss: 4.1871 - val_accuracy: 0.4000\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8654 - accuracy: 0.1800\n",
            "Epoch 134: val_loss did not improve from 4.18713\n",
            "10/10 [==============================] - 6s 588ms/step - loss: 4.8654 - accuracy: 0.1800 - val_loss: 4.3133 - val_accuracy: 0.2750\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8860 - accuracy: 0.1500\n",
            "Epoch 135: val_loss improved from 4.18713 to 4.06461, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 17s 1s/step - loss: 4.8860 - accuracy: 0.1500 - val_loss: 4.0646 - val_accuracy: 0.3000\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0041 - accuracy: 0.1250\n",
            "Epoch 136: val_loss improved from 4.06461 to 4.03255, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 5.0041 - accuracy: 0.1250 - val_loss: 4.0326 - val_accuracy: 0.3750\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9476 - accuracy: 0.1550\n",
            "Epoch 137: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 10s 977ms/step - loss: 4.9476 - accuracy: 0.1550 - val_loss: 4.4272 - val_accuracy: 0.2500\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8709 - accuracy: 0.1850\n",
            "Epoch 138: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 9s 848ms/step - loss: 4.8709 - accuracy: 0.1850 - val_loss: 4.4821 - val_accuracy: 0.2000\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8533 - accuracy: 0.1550\n",
            "Epoch 139: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 7s 697ms/step - loss: 4.8533 - accuracy: 0.1550 - val_loss: 4.4259 - val_accuracy: 0.2750\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8561 - accuracy: 0.1550\n",
            "Epoch 140: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.8561 - accuracy: 0.1550 - val_loss: 4.3546 - val_accuracy: 0.3250\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 5.0094 - accuracy: 0.1500\n",
            "Epoch 141: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 14s 1s/step - loss: 5.0094 - accuracy: 0.1500 - val_loss: 4.1715 - val_accuracy: 0.2750\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9125 - accuracy: 0.1250\n",
            "Epoch 142: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.9125 - accuracy: 0.1250 - val_loss: 4.3056 - val_accuracy: 0.3000\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8654 - accuracy: 0.1500\n",
            "Epoch 143: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 9s 845ms/step - loss: 4.8654 - accuracy: 0.1500 - val_loss: 4.1684 - val_accuracy: 0.3250\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7656 - accuracy: 0.1850\n",
            "Epoch 144: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 8s 719ms/step - loss: 4.7656 - accuracy: 0.1850 - val_loss: 4.4667 - val_accuracy: 0.2500\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8897 - accuracy: 0.1500\n",
            "Epoch 145: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 12s 825ms/step - loss: 4.8897 - accuracy: 0.1500 - val_loss: 4.4105 - val_accuracy: 0.2250\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8456 - accuracy: 0.1450\n",
            "Epoch 146: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 9s 954ms/step - loss: 4.8456 - accuracy: 0.1450 - val_loss: 4.5716 - val_accuracy: 0.2000\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7710 - accuracy: 0.2200\n",
            "Epoch 147: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.7710 - accuracy: 0.2200 - val_loss: 4.0550 - val_accuracy: 0.3500\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8260 - accuracy: 0.1650\n",
            "Epoch 148: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 7s 728ms/step - loss: 4.8260 - accuracy: 0.1650 - val_loss: 4.0520 - val_accuracy: 0.4000\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7936 - accuracy: 0.1700\n",
            "Epoch 149: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 10s 986ms/step - loss: 4.7936 - accuracy: 0.1700 - val_loss: 4.0742 - val_accuracy: 0.3250\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.9505 - accuracy: 0.1500\n",
            "Epoch 150: val_loss did not improve from 4.03255\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.9505 - accuracy: 0.1500 - val_loss: 4.0561 - val_accuracy: 0.3750\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7565 - accuracy: 0.1950\n",
            "Epoch 151: val_loss improved from 4.03255 to 4.01122, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.7565 - accuracy: 0.1950 - val_loss: 4.0112 - val_accuracy: 0.4000\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7431 - accuracy: 0.2200\n",
            "Epoch 152: val_loss did not improve from 4.01122\n",
            "10/10 [==============================] - 8s 775ms/step - loss: 4.7431 - accuracy: 0.2200 - val_loss: 4.2698 - val_accuracy: 0.4000\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8473 - accuracy: 0.1550\n",
            "Epoch 153: val_loss did not improve from 4.01122\n",
            "10/10 [==============================] - 7s 639ms/step - loss: 4.8473 - accuracy: 0.1550 - val_loss: 4.1786 - val_accuracy: 0.4750\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7006 - accuracy: 0.2200\n",
            "Epoch 154: val_loss did not improve from 4.01122\n",
            "10/10 [==============================] - 20s 2s/step - loss: 4.7006 - accuracy: 0.2200 - val_loss: 4.4164 - val_accuracy: 0.1750\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5803 - accuracy: 0.2150\n",
            "Epoch 155: val_loss did not improve from 4.01122\n",
            "10/10 [==============================] - 9s 816ms/step - loss: 4.5803 - accuracy: 0.2150 - val_loss: 4.0815 - val_accuracy: 0.3000\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8165 - accuracy: 0.2000\n",
            "Epoch 156: val_loss improved from 4.01122 to 3.99529, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.8165 - accuracy: 0.2000 - val_loss: 3.9953 - val_accuracy: 0.4500\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7096 - accuracy: 0.2250\n",
            "Epoch 157: val_loss did not improve from 3.99529\n",
            "10/10 [==============================] - 8s 792ms/step - loss: 4.7096 - accuracy: 0.2250 - val_loss: 4.1703 - val_accuracy: 0.3250\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7662 - accuracy: 0.1800\n",
            "Epoch 158: val_loss did not improve from 3.99529\n",
            "10/10 [==============================] - 17s 2s/step - loss: 4.7662 - accuracy: 0.1800 - val_loss: 4.2016 - val_accuracy: 0.3000\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6987 - accuracy: 0.2050\n",
            "Epoch 159: val_loss improved from 3.99529 to 3.97078, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 10s 1s/step - loss: 4.6987 - accuracy: 0.2050 - val_loss: 3.9708 - val_accuracy: 0.4250\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7398 - accuracy: 0.1600\n",
            "Epoch 160: val_loss did not improve from 3.97078\n",
            "10/10 [==============================] - 14s 1s/step - loss: 4.7398 - accuracy: 0.1600 - val_loss: 4.1636 - val_accuracy: 0.3500\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8588 - accuracy: 0.1550\n",
            "Epoch 161: val_loss did not improve from 3.97078\n",
            "10/10 [==============================] - 6s 569ms/step - loss: 4.8588 - accuracy: 0.1550 - val_loss: 4.1601 - val_accuracy: 0.4000\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7971 - accuracy: 0.1950\n",
            "Epoch 162: val_loss did not improve from 3.97078\n",
            "10/10 [==============================] - 8s 729ms/step - loss: 4.7971 - accuracy: 0.1950 - val_loss: 4.3554 - val_accuracy: 0.3250\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7505 - accuracy: 0.1700\n",
            "Epoch 163: val_loss did not improve from 3.97078\n",
            "10/10 [==============================] - 14s 1s/step - loss: 4.7505 - accuracy: 0.1700 - val_loss: 4.1562 - val_accuracy: 0.2500\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7792 - accuracy: 0.2050\n",
            "Epoch 164: val_loss did not improve from 3.97078\n",
            "10/10 [==============================] - 6s 530ms/step - loss: 4.7792 - accuracy: 0.2050 - val_loss: 4.1633 - val_accuracy: 0.2750\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7149 - accuracy: 0.2400\n",
            "Epoch 165: val_loss did not improve from 3.97078\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.7149 - accuracy: 0.2400 - val_loss: 4.2363 - val_accuracy: 0.3500\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8232 - accuracy: 0.1750\n",
            "Epoch 166: val_loss did not improve from 3.97078\n",
            "10/10 [==============================] - 7s 661ms/step - loss: 4.8232 - accuracy: 0.1750 - val_loss: 4.2527 - val_accuracy: 0.3000\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6596 - accuracy: 0.1850\n",
            "Epoch 167: val_loss did not improve from 3.97078\n",
            "10/10 [==============================] - 8s 773ms/step - loss: 4.6596 - accuracy: 0.1850 - val_loss: 4.3441 - val_accuracy: 0.2500\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6083 - accuracy: 0.2300\n",
            "Epoch 168: val_loss did not improve from 3.97078\n",
            "10/10 [==============================] - 7s 623ms/step - loss: 4.6083 - accuracy: 0.2300 - val_loss: 4.1678 - val_accuracy: 0.3500\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8993 - accuracy: 0.1550\n",
            "Epoch 169: val_loss improved from 3.97078 to 3.75091, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 14s 1s/step - loss: 4.8993 - accuracy: 0.1550 - val_loss: 3.7509 - val_accuracy: 0.4500\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7808 - accuracy: 0.1800\n",
            "Epoch 170: val_loss did not improve from 3.75091\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.7808 - accuracy: 0.1800 - val_loss: 4.0567 - val_accuracy: 0.3250\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8227 - accuracy: 0.1500\n",
            "Epoch 171: val_loss did not improve from 3.75091\n",
            "10/10 [==============================] - 9s 914ms/step - loss: 4.8227 - accuracy: 0.1500 - val_loss: 3.8377 - val_accuracy: 0.3500\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6555 - accuracy: 0.2050\n",
            "Epoch 172: val_loss did not improve from 3.75091\n",
            "10/10 [==============================] - 6s 531ms/step - loss: 4.6555 - accuracy: 0.2050 - val_loss: 3.9785 - val_accuracy: 0.4500\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.8145 - accuracy: 0.1650\n",
            "Epoch 173: val_loss did not improve from 3.75091\n",
            "10/10 [==============================] - 8s 748ms/step - loss: 4.8145 - accuracy: 0.1650 - val_loss: 4.1206 - val_accuracy: 0.3250\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6766 - accuracy: 0.2250\n",
            "Epoch 174: val_loss did not improve from 3.75091\n",
            "10/10 [==============================] - 6s 551ms/step - loss: 4.6766 - accuracy: 0.2250 - val_loss: 3.7710 - val_accuracy: 0.4250\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6198 - accuracy: 0.2000\n",
            "Epoch 175: val_loss did not improve from 3.75091\n",
            "10/10 [==============================] - 8s 813ms/step - loss: 4.6198 - accuracy: 0.2000 - val_loss: 3.9200 - val_accuracy: 0.4000\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5922 - accuracy: 0.2200\n",
            "Epoch 176: val_loss did not improve from 3.75091\n",
            "10/10 [==============================] - 10s 912ms/step - loss: 4.5922 - accuracy: 0.2200 - val_loss: 3.9041 - val_accuracy: 0.4500\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7407 - accuracy: 0.1700\n",
            "Epoch 177: val_loss improved from 3.75091 to 3.55809, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.7407 - accuracy: 0.1700 - val_loss: 3.5581 - val_accuracy: 0.4500\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6960 - accuracy: 0.2200\n",
            "Epoch 178: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 9s 900ms/step - loss: 4.6960 - accuracy: 0.2200 - val_loss: 3.9447 - val_accuracy: 0.4250\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7064 - accuracy: 0.2150\n",
            "Epoch 179: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.7064 - accuracy: 0.2150 - val_loss: 4.1066 - val_accuracy: 0.3750\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5411 - accuracy: 0.2500\n",
            "Epoch 180: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.5411 - accuracy: 0.2500 - val_loss: 4.0276 - val_accuracy: 0.3000\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6711 - accuracy: 0.2150\n",
            "Epoch 181: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 7s 724ms/step - loss: 4.6711 - accuracy: 0.2150 - val_loss: 4.0950 - val_accuracy: 0.3250\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6428 - accuracy: 0.2150\n",
            "Epoch 182: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 4.6428 - accuracy: 0.2150 - val_loss: 3.6755 - val_accuracy: 0.4000\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6474 - accuracy: 0.2000\n",
            "Epoch 183: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 8s 792ms/step - loss: 4.6474 - accuracy: 0.2000 - val_loss: 4.0041 - val_accuracy: 0.3750\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6614 - accuracy: 0.2150\n",
            "Epoch 184: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 12s 931ms/step - loss: 4.6614 - accuracy: 0.2150 - val_loss: 4.1186 - val_accuracy: 0.2500\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5577 - accuracy: 0.2200\n",
            "Epoch 185: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.5577 - accuracy: 0.2200 - val_loss: 3.6543 - val_accuracy: 0.6250\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6323 - accuracy: 0.2200\n",
            "Epoch 186: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.6323 - accuracy: 0.2200 - val_loss: 4.0401 - val_accuracy: 0.3250\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5799 - accuracy: 0.2100\n",
            "Epoch 187: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 8s 781ms/step - loss: 4.5799 - accuracy: 0.2100 - val_loss: 3.9719 - val_accuracy: 0.4500\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5626 - accuracy: 0.2250\n",
            "Epoch 188: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 10s 937ms/step - loss: 4.5626 - accuracy: 0.2250 - val_loss: 3.9895 - val_accuracy: 0.4500\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7738 - accuracy: 0.1800\n",
            "Epoch 189: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 7s 672ms/step - loss: 4.7738 - accuracy: 0.1800 - val_loss: 3.8813 - val_accuracy: 0.4500\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5228 - accuracy: 0.2300\n",
            "Epoch 190: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.5228 - accuracy: 0.2300 - val_loss: 4.0938 - val_accuracy: 0.3250\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5828 - accuracy: 0.2300\n",
            "Epoch 191: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 8s 722ms/step - loss: 4.5828 - accuracy: 0.2300 - val_loss: 3.7969 - val_accuracy: 0.3750\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4198 - accuracy: 0.2800\n",
            "Epoch 192: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 10s 1s/step - loss: 4.4198 - accuracy: 0.2800 - val_loss: 4.1270 - val_accuracy: 0.3250\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6515 - accuracy: 0.2200\n",
            "Epoch 193: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 6s 567ms/step - loss: 4.6515 - accuracy: 0.2200 - val_loss: 3.6864 - val_accuracy: 0.4750\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.6456 - accuracy: 0.1750\n",
            "Epoch 194: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 8s 764ms/step - loss: 4.6456 - accuracy: 0.1750 - val_loss: 3.8902 - val_accuracy: 0.4250\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5958 - accuracy: 0.2150\n",
            "Epoch 195: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 12s 901ms/step - loss: 4.5958 - accuracy: 0.2150 - val_loss: 3.8160 - val_accuracy: 0.4750\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5636 - accuracy: 0.2000\n",
            "Epoch 196: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 9s 862ms/step - loss: 4.5636 - accuracy: 0.2000 - val_loss: 3.8993 - val_accuracy: 0.4000\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.7103 - accuracy: 0.1600\n",
            "Epoch 197: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 8s 809ms/step - loss: 4.7103 - accuracy: 0.1600 - val_loss: 3.7940 - val_accuracy: 0.4500\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5327 - accuracy: 0.2000\n",
            "Epoch 198: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 8s 759ms/step - loss: 4.5327 - accuracy: 0.2000 - val_loss: 3.8651 - val_accuracy: 0.4000\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5260 - accuracy: 0.2700\n",
            "Epoch 199: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.5260 - accuracy: 0.2700 - val_loss: 4.1187 - val_accuracy: 0.3000\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5483 - accuracy: 0.2400\n",
            "Epoch 200: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 10s 986ms/step - loss: 4.5483 - accuracy: 0.2400 - val_loss: 4.1556 - val_accuracy: 0.3250\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4057 - accuracy: 0.2550\n",
            "Epoch 201: val_loss did not improve from 3.55809\n",
            "10/10 [==============================] - 10s 787ms/step - loss: 4.4057 - accuracy: 0.2550 - val_loss: 3.7853 - val_accuracy: 0.4000\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4825 - accuracy: 0.2400\n",
            "Epoch 202: val_loss improved from 3.55809 to 3.50879, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 17s 2s/step - loss: 4.4825 - accuracy: 0.2400 - val_loss: 3.5088 - val_accuracy: 0.5250\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4486 - accuracy: 0.2000\n",
            "Epoch 203: val_loss did not improve from 3.50879\n",
            "10/10 [==============================] - 7s 575ms/step - loss: 4.4486 - accuracy: 0.2000 - val_loss: 3.6394 - val_accuracy: 0.5250\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5914 - accuracy: 0.1900\n",
            "Epoch 204: val_loss did not improve from 3.50879\n",
            "10/10 [==============================] - 12s 928ms/step - loss: 4.5914 - accuracy: 0.1900 - val_loss: 3.6700 - val_accuracy: 0.4750\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4774 - accuracy: 0.2300\n",
            "Epoch 205: val_loss did not improve from 3.50879\n",
            "10/10 [==============================] - 9s 795ms/step - loss: 4.4774 - accuracy: 0.2300 - val_loss: 3.5455 - val_accuracy: 0.4500\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5864 - accuracy: 0.2100\n",
            "Epoch 206: val_loss did not improve from 3.50879\n",
            "10/10 [==============================] - 10s 879ms/step - loss: 4.5864 - accuracy: 0.2100 - val_loss: 3.8056 - val_accuracy: 0.4250\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4793 - accuracy: 0.2400\n",
            "Epoch 207: val_loss did not improve from 3.50879\n",
            "10/10 [==============================] - 7s 672ms/step - loss: 4.4793 - accuracy: 0.2400 - val_loss: 3.9500 - val_accuracy: 0.3250\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3843 - accuracy: 0.2800\n",
            "Epoch 208: val_loss did not improve from 3.50879\n",
            "10/10 [==============================] - 9s 788ms/step - loss: 4.3843 - accuracy: 0.2800 - val_loss: 3.5103 - val_accuracy: 0.5000\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4704 - accuracy: 0.2500\n",
            "Epoch 209: val_loss did not improve from 3.50879\n",
            "10/10 [==============================] - 9s 906ms/step - loss: 4.4704 - accuracy: 0.2500 - val_loss: 3.8087 - val_accuracy: 0.4250\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3500 - accuracy: 0.2650\n",
            "Epoch 210: val_loss did not improve from 3.50879\n",
            "10/10 [==============================] - 7s 678ms/step - loss: 4.3500 - accuracy: 0.2650 - val_loss: 3.7270 - val_accuracy: 0.3500\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4243 - accuracy: 0.2400\n",
            "Epoch 211: val_loss did not improve from 3.50879\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4243 - accuracy: 0.2400 - val_loss: 3.9203 - val_accuracy: 0.2750\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4092 - accuracy: 0.2800\n",
            "Epoch 212: val_loss improved from 3.50879 to 3.43399, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4092 - accuracy: 0.2800 - val_loss: 3.4340 - val_accuracy: 0.6500\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5624 - accuracy: 0.2250\n",
            "Epoch 213: val_loss did not improve from 3.43399\n",
            "10/10 [==============================] - 7s 697ms/step - loss: 4.5624 - accuracy: 0.2250 - val_loss: 3.6309 - val_accuracy: 0.3500\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3895 - accuracy: 0.2600\n",
            "Epoch 214: val_loss did not improve from 3.43399\n",
            "10/10 [==============================] - 7s 736ms/step - loss: 4.3895 - accuracy: 0.2600 - val_loss: 3.7376 - val_accuracy: 0.4250\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4245 - accuracy: 0.2400\n",
            "Epoch 215: val_loss did not improve from 3.43399\n",
            "10/10 [==============================] - 10s 936ms/step - loss: 4.4245 - accuracy: 0.2400 - val_loss: 3.5293 - val_accuracy: 0.4250\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2657 - accuracy: 0.2750\n",
            "Epoch 216: val_loss did not improve from 3.43399\n",
            "10/10 [==============================] - 5s 475ms/step - loss: 4.2657 - accuracy: 0.2750 - val_loss: 3.6532 - val_accuracy: 0.5000\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4203 - accuracy: 0.2750\n",
            "Epoch 217: val_loss did not improve from 3.43399\n",
            "10/10 [==============================] - 7s 642ms/step - loss: 4.4203 - accuracy: 0.2750 - val_loss: 3.6747 - val_accuracy: 0.4250\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4362 - accuracy: 0.2450\n",
            "Epoch 218: val_loss did not improve from 3.43399\n",
            "10/10 [==============================] - 7s 649ms/step - loss: 4.4362 - accuracy: 0.2450 - val_loss: 3.8161 - val_accuracy: 0.3250\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4070 - accuracy: 0.2150\n",
            "Epoch 219: val_loss did not improve from 3.43399\n",
            "10/10 [==============================] - 7s 624ms/step - loss: 4.4070 - accuracy: 0.2150 - val_loss: 3.6150 - val_accuracy: 0.5750\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4601 - accuracy: 0.2300\n",
            "Epoch 220: val_loss did not improve from 3.43399\n",
            "10/10 [==============================] - 10s 902ms/step - loss: 4.4601 - accuracy: 0.2300 - val_loss: 3.6373 - val_accuracy: 0.5000\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4185 - accuracy: 0.2800\n",
            "Epoch 221: val_loss improved from 3.43399 to 3.39377, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.4185 - accuracy: 0.2800 - val_loss: 3.3938 - val_accuracy: 0.5500\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.5001 - accuracy: 0.1900\n",
            "Epoch 222: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 7s 697ms/step - loss: 4.5001 - accuracy: 0.1900 - val_loss: 3.7843 - val_accuracy: 0.3500\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3211 - accuracy: 0.2750\n",
            "Epoch 223: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 10s 1s/step - loss: 4.3211 - accuracy: 0.2750 - val_loss: 3.6768 - val_accuracy: 0.4500\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3666 - accuracy: 0.2450\n",
            "Epoch 224: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 8s 758ms/step - loss: 4.3666 - accuracy: 0.2450 - val_loss: 3.7508 - val_accuracy: 0.4000\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4213 - accuracy: 0.2200\n",
            "Epoch 225: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 6s 605ms/step - loss: 4.4213 - accuracy: 0.2200 - val_loss: 3.4981 - val_accuracy: 0.5000\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3923 - accuracy: 0.2350\n",
            "Epoch 226: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 10s 953ms/step - loss: 4.3923 - accuracy: 0.2350 - val_loss: 4.0397 - val_accuracy: 0.3500\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3839 - accuracy: 0.2450\n",
            "Epoch 227: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 10s 1s/step - loss: 4.3839 - accuracy: 0.2450 - val_loss: 3.5853 - val_accuracy: 0.4750\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4906 - accuracy: 0.2500\n",
            "Epoch 228: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.4906 - accuracy: 0.2500 - val_loss: 3.7747 - val_accuracy: 0.3250\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2851 - accuracy: 0.2550\n",
            "Epoch 229: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 10s 940ms/step - loss: 4.2851 - accuracy: 0.2550 - val_loss: 3.4254 - val_accuracy: 0.5250\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2967 - accuracy: 0.2400\n",
            "Epoch 230: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 9s 918ms/step - loss: 4.2967 - accuracy: 0.2400 - val_loss: 3.5062 - val_accuracy: 0.4500\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2904 - accuracy: 0.2950\n",
            "Epoch 231: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 14s 1s/step - loss: 4.2904 - accuracy: 0.2950 - val_loss: 3.6760 - val_accuracy: 0.3500\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4437 - accuracy: 0.2600\n",
            "Epoch 232: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 8s 754ms/step - loss: 4.4437 - accuracy: 0.2600 - val_loss: 3.6849 - val_accuracy: 0.4000\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4311 - accuracy: 0.2650\n",
            "Epoch 233: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 9s 622ms/step - loss: 4.4311 - accuracy: 0.2650 - val_loss: 4.0636 - val_accuracy: 0.3250\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2875 - accuracy: 0.2700\n",
            "Epoch 234: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.2875 - accuracy: 0.2700 - val_loss: 3.4741 - val_accuracy: 0.5250\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3665 - accuracy: 0.2450\n",
            "Epoch 235: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 9s 893ms/step - loss: 4.3665 - accuracy: 0.2450 - val_loss: 3.7098 - val_accuracy: 0.4000\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2378 - accuracy: 0.2900\n",
            "Epoch 236: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 10s 1s/step - loss: 4.2378 - accuracy: 0.2900 - val_loss: 3.6737 - val_accuracy: 0.4000\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3576 - accuracy: 0.2350\n",
            "Epoch 237: val_loss did not improve from 3.39377\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.3576 - accuracy: 0.2350 - val_loss: 3.7878 - val_accuracy: 0.4000\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3000 - accuracy: 0.2600\n",
            "Epoch 238: val_loss improved from 3.39377 to 3.26132, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.3000 - accuracy: 0.2600 - val_loss: 3.2613 - val_accuracy: 0.4750\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2697 - accuracy: 0.2750\n",
            "Epoch 239: val_loss did not improve from 3.26132\n",
            "10/10 [==============================] - 8s 804ms/step - loss: 4.2697 - accuracy: 0.2750 - val_loss: 3.6094 - val_accuracy: 0.4250\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3801 - accuracy: 0.2300\n",
            "Epoch 240: val_loss did not improve from 3.26132\n",
            "10/10 [==============================] - 10s 1s/step - loss: 4.3801 - accuracy: 0.2300 - val_loss: 3.4733 - val_accuracy: 0.4000\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2507 - accuracy: 0.2600\n",
            "Epoch 241: val_loss did not improve from 3.26132\n",
            "10/10 [==============================] - 11s 813ms/step - loss: 4.2507 - accuracy: 0.2600 - val_loss: 4.0260 - val_accuracy: 0.3000\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1841 - accuracy: 0.2900\n",
            "Epoch 242: val_loss did not improve from 3.26132\n",
            "10/10 [==============================] - 16s 2s/step - loss: 4.1841 - accuracy: 0.2900 - val_loss: 3.5129 - val_accuracy: 0.5000\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4701 - accuracy: 0.2200\n",
            "Epoch 243: val_loss did not improve from 3.26132\n",
            "10/10 [==============================] - 18s 2s/step - loss: 4.4701 - accuracy: 0.2200 - val_loss: 3.3212 - val_accuracy: 0.5000\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4259 - accuracy: 0.2350\n",
            "Epoch 244: val_loss did not improve from 3.26132\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 4.4259 - accuracy: 0.2350 - val_loss: 3.6060 - val_accuracy: 0.5000\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.4657 - accuracy: 0.1950\n",
            "Epoch 245: val_loss did not improve from 3.26132\n",
            "10/10 [==============================] - 12s 901ms/step - loss: 4.4657 - accuracy: 0.1950 - val_loss: 3.8816 - val_accuracy: 0.3500\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3000 - accuracy: 0.2600\n",
            "Epoch 246: val_loss improved from 3.26132 to 3.12734, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 18s 2s/step - loss: 4.3000 - accuracy: 0.2600 - val_loss: 3.1273 - val_accuracy: 0.6250\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2539 - accuracy: 0.2900\n",
            "Epoch 247: val_loss did not improve from 3.12734\n",
            "10/10 [==============================] - 10s 1s/step - loss: 4.2539 - accuracy: 0.2900 - val_loss: 3.5073 - val_accuracy: 0.3500\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3209 - accuracy: 0.2350\n",
            "Epoch 248: val_loss did not improve from 3.12734\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.3209 - accuracy: 0.2350 - val_loss: 3.2067 - val_accuracy: 0.5750\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1463 - accuracy: 0.2900\n",
            "Epoch 249: val_loss did not improve from 3.12734\n",
            "10/10 [==============================] - 14s 1s/step - loss: 4.1463 - accuracy: 0.2900 - val_loss: 3.3960 - val_accuracy: 0.5500\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3366 - accuracy: 0.2500\n",
            "Epoch 250: val_loss did not improve from 3.12734\n",
            "10/10 [==============================] - 15s 2s/step - loss: 4.3366 - accuracy: 0.2500 - val_loss: 3.4805 - val_accuracy: 0.5250\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1793 - accuracy: 0.2900\n",
            "Epoch 251: val_loss did not improve from 3.12734\n",
            "10/10 [==============================] - 10s 934ms/step - loss: 4.1793 - accuracy: 0.2900 - val_loss: 3.7073 - val_accuracy: 0.4500\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3158 - accuracy: 0.2550\n",
            "Epoch 252: val_loss did not improve from 3.12734\n",
            "10/10 [==============================] - 9s 943ms/step - loss: 4.3158 - accuracy: 0.2550 - val_loss: 3.5466 - val_accuracy: 0.4500\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1921 - accuracy: 0.3150\n",
            "Epoch 253: val_loss did not improve from 3.12734\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.1921 - accuracy: 0.3150 - val_loss: 3.1317 - val_accuracy: 0.6250\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3213 - accuracy: 0.2400\n",
            "Epoch 254: val_loss improved from 3.12734 to 3.12597, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 16s 2s/step - loss: 4.3213 - accuracy: 0.2400 - val_loss: 3.1260 - val_accuracy: 0.6250\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3221 - accuracy: 0.2350\n",
            "Epoch 255: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 9s 884ms/step - loss: 4.3221 - accuracy: 0.2350 - val_loss: 3.6321 - val_accuracy: 0.5500\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1238 - accuracy: 0.3350\n",
            "Epoch 256: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 13s 1s/step - loss: 4.1238 - accuracy: 0.3350 - val_loss: 3.7957 - val_accuracy: 0.3750\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2931 - accuracy: 0.2600\n",
            "Epoch 257: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 9s 846ms/step - loss: 4.2931 - accuracy: 0.2600 - val_loss: 3.1310 - val_accuracy: 0.5500\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1302 - accuracy: 0.3100\n",
            "Epoch 258: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 9s 893ms/step - loss: 4.1302 - accuracy: 0.3100 - val_loss: 3.4196 - val_accuracy: 0.5250\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2493 - accuracy: 0.2550\n",
            "Epoch 259: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 9s 827ms/step - loss: 4.2493 - accuracy: 0.2550 - val_loss: 3.3102 - val_accuracy: 0.5500\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2921 - accuracy: 0.2850\n",
            "Epoch 260: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 8s 711ms/step - loss: 4.2921 - accuracy: 0.2850 - val_loss: 3.5630 - val_accuracy: 0.5000\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2754 - accuracy: 0.2650\n",
            "Epoch 261: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 6s 595ms/step - loss: 4.2754 - accuracy: 0.2650 - val_loss: 3.3983 - val_accuracy: 0.5000\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2846 - accuracy: 0.2700\n",
            "Epoch 262: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.2846 - accuracy: 0.2700 - val_loss: 3.4483 - val_accuracy: 0.4250\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2840 - accuracy: 0.2500\n",
            "Epoch 263: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 8s 699ms/step - loss: 4.2840 - accuracy: 0.2500 - val_loss: 3.7283 - val_accuracy: 0.3000\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0980 - accuracy: 0.3200\n",
            "Epoch 264: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 7s 643ms/step - loss: 4.0980 - accuracy: 0.3200 - val_loss: 3.4266 - val_accuracy: 0.4500\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2687 - accuracy: 0.2450\n",
            "Epoch 265: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 8s 792ms/step - loss: 4.2687 - accuracy: 0.2450 - val_loss: 3.3396 - val_accuracy: 0.4750\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1000 - accuracy: 0.2950\n",
            "Epoch 266: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 10s 904ms/step - loss: 4.1000 - accuracy: 0.2950 - val_loss: 3.4933 - val_accuracy: 0.5250\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2355 - accuracy: 0.2450\n",
            "Epoch 267: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 9s 875ms/step - loss: 4.2355 - accuracy: 0.2450 - val_loss: 3.3926 - val_accuracy: 0.5250\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.3173 - accuracy: 0.2450\n",
            "Epoch 268: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 7s 706ms/step - loss: 4.3173 - accuracy: 0.2450 - val_loss: 3.5817 - val_accuracy: 0.3500\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1840 - accuracy: 0.2850\n",
            "Epoch 269: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 7s 665ms/step - loss: 4.1840 - accuracy: 0.2850 - val_loss: 3.6794 - val_accuracy: 0.3750\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1115 - accuracy: 0.2900\n",
            "Epoch 270: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 6s 456ms/step - loss: 4.1115 - accuracy: 0.2900 - val_loss: 3.2889 - val_accuracy: 0.6000\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0799 - accuracy: 0.3150\n",
            "Epoch 271: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 8s 729ms/step - loss: 4.0799 - accuracy: 0.3150 - val_loss: 3.3814 - val_accuracy: 0.5250\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1234 - accuracy: 0.2950\n",
            "Epoch 272: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 10s 974ms/step - loss: 4.1234 - accuracy: 0.2950 - val_loss: 3.7734 - val_accuracy: 0.3500\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0332 - accuracy: 0.3500\n",
            "Epoch 273: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 9s 903ms/step - loss: 4.0332 - accuracy: 0.3500 - val_loss: 3.1302 - val_accuracy: 0.7000\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1846 - accuracy: 0.2650\n",
            "Epoch 274: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 10s 986ms/step - loss: 4.1846 - accuracy: 0.2650 - val_loss: 3.5796 - val_accuracy: 0.5000\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0438 - accuracy: 0.3000\n",
            "Epoch 275: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 7s 676ms/step - loss: 4.0438 - accuracy: 0.3000 - val_loss: 3.6298 - val_accuracy: 0.4500\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2564 - accuracy: 0.3000\n",
            "Epoch 276: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 10s 856ms/step - loss: 4.2564 - accuracy: 0.3000 - val_loss: 3.3824 - val_accuracy: 0.5500\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2377 - accuracy: 0.2800\n",
            "Epoch 277: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 10s 859ms/step - loss: 4.2377 - accuracy: 0.2800 - val_loss: 3.4730 - val_accuracy: 0.4750\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2693 - accuracy: 0.2850\n",
            "Epoch 278: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 7s 717ms/step - loss: 4.2693 - accuracy: 0.2850 - val_loss: 3.3846 - val_accuracy: 0.6250\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1057 - accuracy: 0.3200\n",
            "Epoch 279: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 7s 662ms/step - loss: 4.1057 - accuracy: 0.3200 - val_loss: 3.4671 - val_accuracy: 0.4250\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0206 - accuracy: 0.3500\n",
            "Epoch 280: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 5s 422ms/step - loss: 4.0206 - accuracy: 0.3500 - val_loss: 3.2814 - val_accuracy: 0.6000\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2073 - accuracy: 0.2850\n",
            "Epoch 281: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 7s 700ms/step - loss: 4.2073 - accuracy: 0.2850 - val_loss: 3.2735 - val_accuracy: 0.6500\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0543 - accuracy: 0.3300\n",
            "Epoch 282: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 9s 941ms/step - loss: 4.0543 - accuracy: 0.3300 - val_loss: 3.5847 - val_accuracy: 0.5000\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1666 - accuracy: 0.2450\n",
            "Epoch 283: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 8s 752ms/step - loss: 4.1666 - accuracy: 0.2450 - val_loss: 3.4899 - val_accuracy: 0.5000\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2155 - accuracy: 0.2750\n",
            "Epoch 284: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 8s 645ms/step - loss: 4.2155 - accuracy: 0.2750 - val_loss: 3.3764 - val_accuracy: 0.5750\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1144 - accuracy: 0.2950\n",
            "Epoch 285: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 9s 921ms/step - loss: 4.1144 - accuracy: 0.2950 - val_loss: 3.2680 - val_accuracy: 0.5500\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1856 - accuracy: 0.2800\n",
            "Epoch 286: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 7s 602ms/step - loss: 4.1856 - accuracy: 0.2800 - val_loss: 3.2860 - val_accuracy: 0.5000\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1681 - accuracy: 0.3100\n",
            "Epoch 287: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 7s 599ms/step - loss: 4.1681 - accuracy: 0.3100 - val_loss: 3.3207 - val_accuracy: 0.5000\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1106 - accuracy: 0.3000\n",
            "Epoch 288: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 8s 708ms/step - loss: 4.1106 - accuracy: 0.3000 - val_loss: 3.4562 - val_accuracy: 0.5250\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0222 - accuracy: 0.3300\n",
            "Epoch 289: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.0222 - accuracy: 0.3300 - val_loss: 3.5373 - val_accuracy: 0.5500\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0184 - accuracy: 0.3050\n",
            "Epoch 290: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.0184 - accuracy: 0.3050 - val_loss: 3.2621 - val_accuracy: 0.5000\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0735 - accuracy: 0.3000\n",
            "Epoch 291: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 6s 574ms/step - loss: 4.0735 - accuracy: 0.3000 - val_loss: 3.3694 - val_accuracy: 0.4750\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1869 - accuracy: 0.2450\n",
            "Epoch 292: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 18s 2s/step - loss: 4.1869 - accuracy: 0.2450 - val_loss: 3.3736 - val_accuracy: 0.5500\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9405 - accuracy: 0.3400\n",
            "Epoch 293: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 9s 956ms/step - loss: 3.9405 - accuracy: 0.3400 - val_loss: 3.3132 - val_accuracy: 0.5250\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9111 - accuracy: 0.3150\n",
            "Epoch 294: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 10s 1s/step - loss: 3.9111 - accuracy: 0.3150 - val_loss: 3.5246 - val_accuracy: 0.5000\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0640 - accuracy: 0.2950\n",
            "Epoch 295: val_loss did not improve from 3.12597\n",
            "10/10 [==============================] - 17s 1s/step - loss: 4.0640 - accuracy: 0.2950 - val_loss: 3.3703 - val_accuracy: 0.4750\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0014 - accuracy: 0.3400\n",
            "Epoch 296: val_loss improved from 3.12597 to 3.09127, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.0014 - accuracy: 0.3400 - val_loss: 3.0913 - val_accuracy: 0.6750\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.1070 - accuracy: 0.3250\n",
            "Epoch 297: val_loss did not improve from 3.09127\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.1070 - accuracy: 0.3250 - val_loss: 3.3327 - val_accuracy: 0.4500\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0036 - accuracy: 0.2750\n",
            "Epoch 298: val_loss did not improve from 3.09127\n",
            "10/10 [==============================] - 8s 787ms/step - loss: 4.0036 - accuracy: 0.2750 - val_loss: 3.3177 - val_accuracy: 0.4500\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8618 - accuracy: 0.3650\n",
            "Epoch 299: val_loss did not improve from 3.09127\n",
            "10/10 [==============================] - 21s 2s/step - loss: 3.8618 - accuracy: 0.3650 - val_loss: 3.3669 - val_accuracy: 0.5750\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9713 - accuracy: 0.3700\n",
            "Epoch 300: val_loss did not improve from 3.09127\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.9713 - accuracy: 0.3700 - val_loss: 3.1501 - val_accuracy: 0.5000\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0110 - accuracy: 0.3050\n",
            "Epoch 301: val_loss did not improve from 3.09127\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.0110 - accuracy: 0.3050 - val_loss: 3.3704 - val_accuracy: 0.5000\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9892 - accuracy: 0.3450\n",
            "Epoch 302: val_loss did not improve from 3.09127\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.9892 - accuracy: 0.3450 - val_loss: 3.4050 - val_accuracy: 0.4750\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7259 - accuracy: 0.4150\n",
            "Epoch 303: val_loss improved from 3.09127 to 2.97095, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 21s 2s/step - loss: 3.7259 - accuracy: 0.4150 - val_loss: 2.9710 - val_accuracy: 0.7750\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0076 - accuracy: 0.3200\n",
            "Epoch 304: val_loss did not improve from 2.97095\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.0076 - accuracy: 0.3200 - val_loss: 3.1238 - val_accuracy: 0.6000\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.2310 - accuracy: 0.2350\n",
            "Epoch 305: val_loss improved from 2.97095 to 2.93712, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 19s 2s/step - loss: 4.2310 - accuracy: 0.2350 - val_loss: 2.9371 - val_accuracy: 0.6750\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9898 - accuracy: 0.3850\n",
            "Epoch 306: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.9898 - accuracy: 0.3850 - val_loss: 3.2128 - val_accuracy: 0.6500\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9438 - accuracy: 0.3450\n",
            "Epoch 307: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.9438 - accuracy: 0.3450 - val_loss: 3.1994 - val_accuracy: 0.6000\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9165 - accuracy: 0.3450\n",
            "Epoch 308: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 16s 2s/step - loss: 3.9165 - accuracy: 0.3450 - val_loss: 3.6343 - val_accuracy: 0.3500\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9884 - accuracy: 0.3050\n",
            "Epoch 309: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 10s 955ms/step - loss: 3.9884 - accuracy: 0.3050 - val_loss: 3.3831 - val_accuracy: 0.5000\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9682 - accuracy: 0.3300\n",
            "Epoch 310: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 6s 576ms/step - loss: 3.9682 - accuracy: 0.3300 - val_loss: 3.1943 - val_accuracy: 0.5750\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0328 - accuracy: 0.2950\n",
            "Epoch 311: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 16s 2s/step - loss: 4.0328 - accuracy: 0.2950 - val_loss: 3.1850 - val_accuracy: 0.5500\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0156 - accuracy: 0.2600\n",
            "Epoch 312: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 15s 2s/step - loss: 4.0156 - accuracy: 0.2600 - val_loss: 3.3318 - val_accuracy: 0.5500\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0595 - accuracy: 0.2750\n",
            "Epoch 313: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 12s 1s/step - loss: 4.0595 - accuracy: 0.2750 - val_loss: 3.4840 - val_accuracy: 0.4000\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8531 - accuracy: 0.3650\n",
            "Epoch 314: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.8531 - accuracy: 0.3650 - val_loss: 3.0718 - val_accuracy: 0.7000\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9284 - accuracy: 0.3650\n",
            "Epoch 315: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 3.9284 - accuracy: 0.3650 - val_loss: 3.0905 - val_accuracy: 0.5750\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9161 - accuracy: 0.2950\n",
            "Epoch 316: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 7s 603ms/step - loss: 3.9161 - accuracy: 0.2950 - val_loss: 3.1977 - val_accuracy: 0.7000\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9558 - accuracy: 0.3550\n",
            "Epoch 317: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 6s 577ms/step - loss: 3.9558 - accuracy: 0.3550 - val_loss: 3.4442 - val_accuracy: 0.5250\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0503 - accuracy: 0.3150\n",
            "Epoch 318: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 8s 812ms/step - loss: 4.0503 - accuracy: 0.3150 - val_loss: 3.1641 - val_accuracy: 0.6500\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9671 - accuracy: 0.3550\n",
            "Epoch 319: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 7s 603ms/step - loss: 3.9671 - accuracy: 0.3550 - val_loss: 3.2556 - val_accuracy: 0.4750\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9123 - accuracy: 0.3200\n",
            "Epoch 320: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 5s 475ms/step - loss: 3.9123 - accuracy: 0.3200 - val_loss: 3.3386 - val_accuracy: 0.5250\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9985 - accuracy: 0.2650\n",
            "Epoch 321: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 5s 436ms/step - loss: 3.9985 - accuracy: 0.2650 - val_loss: 3.1644 - val_accuracy: 0.5500\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8986 - accuracy: 0.3150\n",
            "Epoch 322: val_loss did not improve from 2.93712\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.8986 - accuracy: 0.3150 - val_loss: 3.5198 - val_accuracy: 0.5000\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0544 - accuracy: 0.2950\n",
            "Epoch 323: val_loss improved from 2.93712 to 2.91372, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 4.0544 - accuracy: 0.2950 - val_loss: 2.9137 - val_accuracy: 0.7500\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7469 - accuracy: 0.3350\n",
            "Epoch 324: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.7469 - accuracy: 0.3350 - val_loss: 3.2139 - val_accuracy: 0.6000\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9629 - accuracy: 0.3100\n",
            "Epoch 325: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.9629 - accuracy: 0.3100 - val_loss: 3.2792 - val_accuracy: 0.5500\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0204 - accuracy: 0.3450\n",
            "Epoch 326: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 15s 2s/step - loss: 4.0204 - accuracy: 0.3450 - val_loss: 3.2956 - val_accuracy: 0.5250\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8338 - accuracy: 0.3550\n",
            "Epoch 327: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 8s 827ms/step - loss: 3.8338 - accuracy: 0.3550 - val_loss: 3.0377 - val_accuracy: 0.8000\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8950 - accuracy: 0.3550\n",
            "Epoch 328: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 8s 531ms/step - loss: 3.8950 - accuracy: 0.3550 - val_loss: 3.3388 - val_accuracy: 0.5000\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7277 - accuracy: 0.3800\n",
            "Epoch 329: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 6s 492ms/step - loss: 3.7277 - accuracy: 0.3800 - val_loss: 2.9138 - val_accuracy: 0.7250\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0198 - accuracy: 0.2900\n",
            "Epoch 330: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 6s 567ms/step - loss: 4.0198 - accuracy: 0.2900 - val_loss: 3.1626 - val_accuracy: 0.6250\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7668 - accuracy: 0.3600\n",
            "Epoch 331: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 6s 512ms/step - loss: 3.7668 - accuracy: 0.3600 - val_loss: 3.0593 - val_accuracy: 0.7250\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9878 - accuracy: 0.3250\n",
            "Epoch 332: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.9878 - accuracy: 0.3250 - val_loss: 3.1477 - val_accuracy: 0.6250\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8254 - accuracy: 0.3550\n",
            "Epoch 333: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 8s 700ms/step - loss: 3.8254 - accuracy: 0.3550 - val_loss: 3.3095 - val_accuracy: 0.4750\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9481 - accuracy: 0.3300\n",
            "Epoch 334: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 7s 605ms/step - loss: 3.9481 - accuracy: 0.3300 - val_loss: 3.4747 - val_accuracy: 0.4500\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8996 - accuracy: 0.3450\n",
            "Epoch 335: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 5s 456ms/step - loss: 3.8996 - accuracy: 0.3450 - val_loss: 3.0940 - val_accuracy: 0.6500\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8840 - accuracy: 0.3650\n",
            "Epoch 336: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 6s 549ms/step - loss: 3.8840 - accuracy: 0.3650 - val_loss: 3.4140 - val_accuracy: 0.5500\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9042 - accuracy: 0.3000\n",
            "Epoch 337: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 6s 586ms/step - loss: 3.9042 - accuracy: 0.3000 - val_loss: 3.2141 - val_accuracy: 0.6000\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9448 - accuracy: 0.3200\n",
            "Epoch 338: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 7s 714ms/step - loss: 3.9448 - accuracy: 0.3200 - val_loss: 3.1781 - val_accuracy: 0.6250\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8369 - accuracy: 0.3700\n",
            "Epoch 339: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 10s 951ms/step - loss: 3.8369 - accuracy: 0.3700 - val_loss: 3.3574 - val_accuracy: 0.4250\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8074 - accuracy: 0.3400\n",
            "Epoch 340: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 6s 548ms/step - loss: 3.8074 - accuracy: 0.3400 - val_loss: 3.4386 - val_accuracy: 0.4250\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8267 - accuracy: 0.3550\n",
            "Epoch 341: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 9s 901ms/step - loss: 3.8267 - accuracy: 0.3550 - val_loss: 3.0909 - val_accuracy: 0.6250\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9195 - accuracy: 0.3600\n",
            "Epoch 342: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 5s 473ms/step - loss: 3.9195 - accuracy: 0.3600 - val_loss: 3.3853 - val_accuracy: 0.4000\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7871 - accuracy: 0.3500\n",
            "Epoch 343: val_loss did not improve from 2.91372\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.7871 - accuracy: 0.3500 - val_loss: 2.9828 - val_accuracy: 0.5750\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9467 - accuracy: 0.3200\n",
            "Epoch 344: val_loss improved from 2.91372 to 2.83897, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.9467 - accuracy: 0.3200 - val_loss: 2.8390 - val_accuracy: 0.7250\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 4.0478 - accuracy: 0.3000\n",
            "Epoch 345: val_loss did not improve from 2.83897\n",
            "10/10 [==============================] - 7s 708ms/step - loss: 4.0478 - accuracy: 0.3000 - val_loss: 3.0248 - val_accuracy: 0.5750\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7881 - accuracy: 0.3550\n",
            "Epoch 346: val_loss did not improve from 2.83897\n",
            "10/10 [==============================] - 17s 2s/step - loss: 3.7881 - accuracy: 0.3550 - val_loss: 3.2490 - val_accuracy: 0.5250\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7503 - accuracy: 0.3750\n",
            "Epoch 347: val_loss did not improve from 2.83897\n",
            "10/10 [==============================] - 11s 901ms/step - loss: 3.7503 - accuracy: 0.3750 - val_loss: 2.9344 - val_accuracy: 0.7250\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7886 - accuracy: 0.3500\n",
            "Epoch 348: val_loss did not improve from 2.83897\n",
            "10/10 [==============================] - 8s 567ms/step - loss: 3.7886 - accuracy: 0.3500 - val_loss: 3.2812 - val_accuracy: 0.4750\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7593 - accuracy: 0.3650\n",
            "Epoch 349: val_loss did not improve from 2.83897\n",
            "10/10 [==============================] - 7s 624ms/step - loss: 3.7593 - accuracy: 0.3650 - val_loss: 3.1461 - val_accuracy: 0.5000\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7119 - accuracy: 0.3600\n",
            "Epoch 350: val_loss did not improve from 2.83897\n",
            "10/10 [==============================] - 7s 566ms/step - loss: 3.7119 - accuracy: 0.3600 - val_loss: 3.0017 - val_accuracy: 0.6250\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8899 - accuracy: 0.3800\n",
            "Epoch 351: val_loss did not improve from 2.83897\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.8899 - accuracy: 0.3800 - val_loss: 3.1270 - val_accuracy: 0.5000\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9728 - accuracy: 0.3600\n",
            "Epoch 352: val_loss did not improve from 2.83897\n",
            "10/10 [==============================] - 9s 938ms/step - loss: 3.9728 - accuracy: 0.3600 - val_loss: 3.1924 - val_accuracy: 0.5750\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6941 - accuracy: 0.3600\n",
            "Epoch 353: val_loss improved from 2.83897 to 2.74115, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 14s 2s/step - loss: 3.6941 - accuracy: 0.3600 - val_loss: 2.7411 - val_accuracy: 0.6750\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7547 - accuracy: 0.3700\n",
            "Epoch 354: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.7547 - accuracy: 0.3700 - val_loss: 3.0840 - val_accuracy: 0.5750\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9632 - accuracy: 0.3250\n",
            "Epoch 355: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 9s 864ms/step - loss: 3.9632 - accuracy: 0.3250 - val_loss: 2.9701 - val_accuracy: 0.6000\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8639 - accuracy: 0.3400\n",
            "Epoch 356: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 11s 920ms/step - loss: 3.8639 - accuracy: 0.3400 - val_loss: 3.2340 - val_accuracy: 0.5750\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5961 - accuracy: 0.4550\n",
            "Epoch 357: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 11s 864ms/step - loss: 3.5961 - accuracy: 0.4550 - val_loss: 3.0815 - val_accuracy: 0.6500\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8455 - accuracy: 0.3600\n",
            "Epoch 358: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.8455 - accuracy: 0.3600 - val_loss: 3.0522 - val_accuracy: 0.6000\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7741 - accuracy: 0.3450\n",
            "Epoch 359: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 7s 734ms/step - loss: 3.7741 - accuracy: 0.3450 - val_loss: 3.2198 - val_accuracy: 0.5000\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8615 - accuracy: 0.3750\n",
            "Epoch 360: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 10s 948ms/step - loss: 3.8615 - accuracy: 0.3750 - val_loss: 3.1387 - val_accuracy: 0.5250\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8878 - accuracy: 0.3050\n",
            "Epoch 361: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.8878 - accuracy: 0.3050 - val_loss: 2.7768 - val_accuracy: 0.7000\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6949 - accuracy: 0.3750\n",
            "Epoch 362: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 9s 865ms/step - loss: 3.6949 - accuracy: 0.3750 - val_loss: 3.2494 - val_accuracy: 0.5500\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8536 - accuracy: 0.3300\n",
            "Epoch 363: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 9s 925ms/step - loss: 3.8536 - accuracy: 0.3300 - val_loss: 3.0313 - val_accuracy: 0.5500\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7401 - accuracy: 0.3800\n",
            "Epoch 364: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.7401 - accuracy: 0.3800 - val_loss: 2.8440 - val_accuracy: 0.7750\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7265 - accuracy: 0.3300\n",
            "Epoch 365: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 6s 511ms/step - loss: 3.7265 - accuracy: 0.3300 - val_loss: 3.0802 - val_accuracy: 0.5750\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7677 - accuracy: 0.3600\n",
            "Epoch 366: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 7s 714ms/step - loss: 3.7677 - accuracy: 0.3600 - val_loss: 2.7829 - val_accuracy: 0.7750\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8256 - accuracy: 0.3300\n",
            "Epoch 367: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 21s 2s/step - loss: 3.8256 - accuracy: 0.3300 - val_loss: 2.9088 - val_accuracy: 0.6000\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7062 - accuracy: 0.3500\n",
            "Epoch 368: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.7062 - accuracy: 0.3500 - val_loss: 2.9252 - val_accuracy: 0.6250\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6575 - accuracy: 0.3850\n",
            "Epoch 369: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.6575 - accuracy: 0.3850 - val_loss: 3.3096 - val_accuracy: 0.5000\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.9430 - accuracy: 0.3100\n",
            "Epoch 370: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.9430 - accuracy: 0.3100 - val_loss: 2.8666 - val_accuracy: 0.6500\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6584 - accuracy: 0.4100\n",
            "Epoch 371: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.6584 - accuracy: 0.4100 - val_loss: 3.1048 - val_accuracy: 0.6500\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8271 - accuracy: 0.3350\n",
            "Epoch 372: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.8271 - accuracy: 0.3350 - val_loss: 3.0623 - val_accuracy: 0.5000\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7550 - accuracy: 0.3950\n",
            "Epoch 373: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.7550 - accuracy: 0.3950 - val_loss: 2.9872 - val_accuracy: 0.6250\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8513 - accuracy: 0.3500\n",
            "Epoch 374: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.8513 - accuracy: 0.3500 - val_loss: 2.9430 - val_accuracy: 0.7000\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5921 - accuracy: 0.4250\n",
            "Epoch 375: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.5921 - accuracy: 0.4250 - val_loss: 3.0789 - val_accuracy: 0.5500\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6780 - accuracy: 0.3950\n",
            "Epoch 376: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 13s 978ms/step - loss: 3.6780 - accuracy: 0.3950 - val_loss: 3.0814 - val_accuracy: 0.6000\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5582 - accuracy: 0.4100\n",
            "Epoch 377: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 10s 1s/step - loss: 3.5582 - accuracy: 0.4100 - val_loss: 3.0425 - val_accuracy: 0.6250\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6387 - accuracy: 0.4550\n",
            "Epoch 378: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.6387 - accuracy: 0.4550 - val_loss: 2.9773 - val_accuracy: 0.6750\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6649 - accuracy: 0.4150\n",
            "Epoch 379: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.6649 - accuracy: 0.4150 - val_loss: 3.0345 - val_accuracy: 0.6000\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6723 - accuracy: 0.4050\n",
            "Epoch 380: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.6723 - accuracy: 0.4050 - val_loss: 2.8639 - val_accuracy: 0.6500\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7116 - accuracy: 0.3500\n",
            "Epoch 381: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 11s 734ms/step - loss: 3.7116 - accuracy: 0.3500 - val_loss: 3.2201 - val_accuracy: 0.5000\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6454 - accuracy: 0.3900\n",
            "Epoch 382: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.6454 - accuracy: 0.3900 - val_loss: 2.8164 - val_accuracy: 0.7250\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6928 - accuracy: 0.3650\n",
            "Epoch 383: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 8s 746ms/step - loss: 3.6928 - accuracy: 0.3650 - val_loss: 3.1987 - val_accuracy: 0.5750\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8111 - accuracy: 0.3750\n",
            "Epoch 384: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.8111 - accuracy: 0.3750 - val_loss: 2.8897 - val_accuracy: 0.6000\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5353 - accuracy: 0.4000\n",
            "Epoch 385: val_loss did not improve from 2.74115\n",
            "10/10 [==============================] - 6s 513ms/step - loss: 3.5353 - accuracy: 0.4000 - val_loss: 3.2244 - val_accuracy: 0.5500\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6478 - accuracy: 0.3450\n",
            "Epoch 386: val_loss improved from 2.74115 to 2.67661, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.6478 - accuracy: 0.3450 - val_loss: 2.6766 - val_accuracy: 0.7250\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7939 - accuracy: 0.3250\n",
            "Epoch 387: val_loss did not improve from 2.67661\n",
            "10/10 [==============================] - 8s 811ms/step - loss: 3.7939 - accuracy: 0.3250 - val_loss: 3.0419 - val_accuracy: 0.6250\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7729 - accuracy: 0.3850\n",
            "Epoch 388: val_loss did not improve from 2.67661\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.7729 - accuracy: 0.3850 - val_loss: 3.2468 - val_accuracy: 0.4750\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5984 - accuracy: 0.3650\n",
            "Epoch 389: val_loss did not improve from 2.67661\n",
            "10/10 [==============================] - 21s 2s/step - loss: 3.5984 - accuracy: 0.3650 - val_loss: 3.0544 - val_accuracy: 0.4750\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.8016 - accuracy: 0.3050\n",
            "Epoch 390: val_loss did not improve from 2.67661\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.8016 - accuracy: 0.3050 - val_loss: 3.0140 - val_accuracy: 0.6250\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6372 - accuracy: 0.4000\n",
            "Epoch 391: val_loss did not improve from 2.67661\n",
            "10/10 [==============================] - 8s 789ms/step - loss: 3.6372 - accuracy: 0.4000 - val_loss: 2.7873 - val_accuracy: 0.6750\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6390 - accuracy: 0.3400\n",
            "Epoch 392: val_loss did not improve from 2.67661\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.6390 - accuracy: 0.3400 - val_loss: 3.1452 - val_accuracy: 0.5750\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5800 - accuracy: 0.4350\n",
            "Epoch 393: val_loss improved from 2.67661 to 2.56643, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 9s 869ms/step - loss: 3.5800 - accuracy: 0.4350 - val_loss: 2.5664 - val_accuracy: 0.7000\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7158 - accuracy: 0.3900\n",
            "Epoch 394: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.7158 - accuracy: 0.3900 - val_loss: 3.1126 - val_accuracy: 0.5500\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7016 - accuracy: 0.3250\n",
            "Epoch 395: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 8s 833ms/step - loss: 3.7016 - accuracy: 0.3250 - val_loss: 2.9381 - val_accuracy: 0.6500\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6114 - accuracy: 0.3950\n",
            "Epoch 396: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 10s 920ms/step - loss: 3.6114 - accuracy: 0.3950 - val_loss: 3.0026 - val_accuracy: 0.5750\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6842 - accuracy: 0.3900\n",
            "Epoch 397: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.6842 - accuracy: 0.3900 - val_loss: 3.1033 - val_accuracy: 0.5500\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5683 - accuracy: 0.3950\n",
            "Epoch 398: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.5683 - accuracy: 0.3950 - val_loss: 2.9529 - val_accuracy: 0.6500\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.7484 - accuracy: 0.3400\n",
            "Epoch 399: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 9s 641ms/step - loss: 3.7484 - accuracy: 0.3400 - val_loss: 3.0683 - val_accuracy: 0.4750\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5875 - accuracy: 0.3950\n",
            "Epoch 400: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.5875 - accuracy: 0.3950 - val_loss: 2.8768 - val_accuracy: 0.6750\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6724 - accuracy: 0.3900\n",
            "Epoch 401: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 9s 771ms/step - loss: 3.6724 - accuracy: 0.3900 - val_loss: 2.6501 - val_accuracy: 0.7500\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6185 - accuracy: 0.4300\n",
            "Epoch 402: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.6185 - accuracy: 0.4300 - val_loss: 3.0735 - val_accuracy: 0.5250\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6704 - accuracy: 0.3850\n",
            "Epoch 403: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.6704 - accuracy: 0.3850 - val_loss: 3.0051 - val_accuracy: 0.6000\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6197 - accuracy: 0.3700\n",
            "Epoch 404: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.6197 - accuracy: 0.3700 - val_loss: 3.2248 - val_accuracy: 0.5500\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5972 - accuracy: 0.3650\n",
            "Epoch 405: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.5972 - accuracy: 0.3650 - val_loss: 2.9267 - val_accuracy: 0.6250\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6598 - accuracy: 0.3800\n",
            "Epoch 406: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 12s 993ms/step - loss: 3.6598 - accuracy: 0.3800 - val_loss: 2.9420 - val_accuracy: 0.6000\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6338 - accuracy: 0.3800\n",
            "Epoch 407: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.6338 - accuracy: 0.3800 - val_loss: 2.7055 - val_accuracy: 0.6750\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6801 - accuracy: 0.3500\n",
            "Epoch 408: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 10s 959ms/step - loss: 3.6801 - accuracy: 0.3500 - val_loss: 3.0123 - val_accuracy: 0.6750\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5825 - accuracy: 0.3900\n",
            "Epoch 409: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 17s 2s/step - loss: 3.5825 - accuracy: 0.3900 - val_loss: 2.6430 - val_accuracy: 0.7000\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4036 - accuracy: 0.4800\n",
            "Epoch 410: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.4036 - accuracy: 0.4800 - val_loss: 2.9463 - val_accuracy: 0.5750\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6118 - accuracy: 0.4150\n",
            "Epoch 411: val_loss did not improve from 2.56643\n",
            "10/10 [==============================] - 6s 439ms/step - loss: 3.6118 - accuracy: 0.4150 - val_loss: 2.8584 - val_accuracy: 0.6500\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6252 - accuracy: 0.3850\n",
            "Epoch 412: val_loss improved from 2.56643 to 2.55284, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 14s 2s/step - loss: 3.6252 - accuracy: 0.3850 - val_loss: 2.5528 - val_accuracy: 0.7000\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6883 - accuracy: 0.3650\n",
            "Epoch 413: val_loss did not improve from 2.55284\n",
            "10/10 [==============================] - 6s 639ms/step - loss: 3.6883 - accuracy: 0.3650 - val_loss: 2.8375 - val_accuracy: 0.6500\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5446 - accuracy: 0.4200\n",
            "Epoch 414: val_loss did not improve from 2.55284\n",
            "10/10 [==============================] - 8s 739ms/step - loss: 3.5446 - accuracy: 0.4200 - val_loss: 2.8108 - val_accuracy: 0.6000\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6007 - accuracy: 0.4450\n",
            "Epoch 415: val_loss did not improve from 2.55284\n",
            "10/10 [==============================] - 8s 826ms/step - loss: 3.6007 - accuracy: 0.4450 - val_loss: 2.8602 - val_accuracy: 0.6500\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5537 - accuracy: 0.3950\n",
            "Epoch 416: val_loss did not improve from 2.55284\n",
            "10/10 [==============================] - 9s 962ms/step - loss: 3.5537 - accuracy: 0.3950 - val_loss: 2.7525 - val_accuracy: 0.6750\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6714 - accuracy: 0.3800\n",
            "Epoch 417: val_loss did not improve from 2.55284\n",
            "10/10 [==============================] - 8s 489ms/step - loss: 3.6714 - accuracy: 0.3800 - val_loss: 2.9372 - val_accuracy: 0.5500\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5510 - accuracy: 0.4250\n",
            "Epoch 418: val_loss did not improve from 2.55284\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.5510 - accuracy: 0.4250 - val_loss: 2.7002 - val_accuracy: 0.6250\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5559 - accuracy: 0.3800\n",
            "Epoch 419: val_loss did not improve from 2.55284\n",
            "10/10 [==============================] - 16s 1s/step - loss: 3.5559 - accuracy: 0.3800 - val_loss: 2.9013 - val_accuracy: 0.6750\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5086 - accuracy: 0.3850\n",
            "Epoch 420: val_loss improved from 2.55284 to 2.40207, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.5086 - accuracy: 0.3850 - val_loss: 2.4021 - val_accuracy: 0.8250\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5137 - accuracy: 0.4100\n",
            "Epoch 421: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 16s 2s/step - loss: 3.5137 - accuracy: 0.4100 - val_loss: 2.7127 - val_accuracy: 0.6750\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5899 - accuracy: 0.4150\n",
            "Epoch 422: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 19s 2s/step - loss: 3.5899 - accuracy: 0.4150 - val_loss: 2.8443 - val_accuracy: 0.7000\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6033 - accuracy: 0.3700\n",
            "Epoch 423: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.6033 - accuracy: 0.3700 - val_loss: 2.8039 - val_accuracy: 0.6250\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5204 - accuracy: 0.3850\n",
            "Epoch 424: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.5204 - accuracy: 0.3850 - val_loss: 2.7530 - val_accuracy: 0.6750\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5337 - accuracy: 0.4500\n",
            "Epoch 425: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.5337 - accuracy: 0.4500 - val_loss: 2.8959 - val_accuracy: 0.6750\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4985 - accuracy: 0.4400\n",
            "Epoch 426: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.4985 - accuracy: 0.4400 - val_loss: 2.6065 - val_accuracy: 0.7250\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4346 - accuracy: 0.4650\n",
            "Epoch 427: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.4346 - accuracy: 0.4650 - val_loss: 2.7949 - val_accuracy: 0.5500\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4509 - accuracy: 0.4400\n",
            "Epoch 428: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 17s 1s/step - loss: 3.4509 - accuracy: 0.4400 - val_loss: 2.9907 - val_accuracy: 0.5750\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6023 - accuracy: 0.3900\n",
            "Epoch 429: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 7s 716ms/step - loss: 3.6023 - accuracy: 0.3900 - val_loss: 2.7202 - val_accuracy: 0.6500\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6065 - accuracy: 0.4200\n",
            "Epoch 430: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.6065 - accuracy: 0.4200 - val_loss: 2.8732 - val_accuracy: 0.6500\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5401 - accuracy: 0.3750\n",
            "Epoch 431: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.5401 - accuracy: 0.3750 - val_loss: 2.9243 - val_accuracy: 0.6000\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5971 - accuracy: 0.3800\n",
            "Epoch 432: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.5971 - accuracy: 0.3800 - val_loss: 2.7847 - val_accuracy: 0.6500\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5957 - accuracy: 0.3950\n",
            "Epoch 433: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.5957 - accuracy: 0.3950 - val_loss: 2.5931 - val_accuracy: 0.7250\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5719 - accuracy: 0.3200\n",
            "Epoch 434: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.5719 - accuracy: 0.3200 - val_loss: 2.6877 - val_accuracy: 0.6000\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6404 - accuracy: 0.3200\n",
            "Epoch 435: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.6404 - accuracy: 0.3200 - val_loss: 2.7063 - val_accuracy: 0.6250\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5234 - accuracy: 0.4350\n",
            "Epoch 436: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 10s 1s/step - loss: 3.5234 - accuracy: 0.4350 - val_loss: 2.5933 - val_accuracy: 0.6500\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5238 - accuracy: 0.3950\n",
            "Epoch 437: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 9s 940ms/step - loss: 3.5238 - accuracy: 0.3950 - val_loss: 3.4419 - val_accuracy: 0.3750\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5011 - accuracy: 0.4400\n",
            "Epoch 438: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 9s 858ms/step - loss: 3.5011 - accuracy: 0.4400 - val_loss: 2.9192 - val_accuracy: 0.5500\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5288 - accuracy: 0.4150\n",
            "Epoch 439: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.5288 - accuracy: 0.4150 - val_loss: 2.4681 - val_accuracy: 0.8000\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5431 - accuracy: 0.4000\n",
            "Epoch 440: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.5431 - accuracy: 0.4000 - val_loss: 2.5559 - val_accuracy: 0.7250\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5476 - accuracy: 0.4150\n",
            "Epoch 441: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 10s 1s/step - loss: 3.5476 - accuracy: 0.4150 - val_loss: 2.7927 - val_accuracy: 0.6250\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5593 - accuracy: 0.3400\n",
            "Epoch 442: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 7s 723ms/step - loss: 3.5593 - accuracy: 0.3400 - val_loss: 3.0020 - val_accuracy: 0.5250\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4865 - accuracy: 0.4050\n",
            "Epoch 443: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.4865 - accuracy: 0.4050 - val_loss: 2.6312 - val_accuracy: 0.6750\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6436 - accuracy: 0.3900\n",
            "Epoch 444: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.6436 - accuracy: 0.3900 - val_loss: 2.7957 - val_accuracy: 0.6750\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3630 - accuracy: 0.4500\n",
            "Epoch 445: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.3630 - accuracy: 0.4500 - val_loss: 2.5333 - val_accuracy: 0.7250\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3935 - accuracy: 0.4750\n",
            "Epoch 446: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 8s 662ms/step - loss: 3.3935 - accuracy: 0.4750 - val_loss: 2.8180 - val_accuracy: 0.6250\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5846 - accuracy: 0.3900\n",
            "Epoch 447: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 10s 1s/step - loss: 3.5846 - accuracy: 0.3900 - val_loss: 2.6999 - val_accuracy: 0.6250\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4652 - accuracy: 0.3750\n",
            "Epoch 448: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.4652 - accuracy: 0.3750 - val_loss: 2.5778 - val_accuracy: 0.7250\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6713 - accuracy: 0.3850\n",
            "Epoch 449: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.6713 - accuracy: 0.3850 - val_loss: 2.8518 - val_accuracy: 0.6000\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3441 - accuracy: 0.4700\n",
            "Epoch 450: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 10s 978ms/step - loss: 3.3441 - accuracy: 0.4700 - val_loss: 3.0558 - val_accuracy: 0.5750\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5739 - accuracy: 0.3900\n",
            "Epoch 451: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 9s 928ms/step - loss: 3.5739 - accuracy: 0.3900 - val_loss: 2.7836 - val_accuracy: 0.7500\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4699 - accuracy: 0.4200\n",
            "Epoch 452: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 7s 607ms/step - loss: 3.4699 - accuracy: 0.4200 - val_loss: 2.8124 - val_accuracy: 0.6500\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3907 - accuracy: 0.4650\n",
            "Epoch 453: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.3907 - accuracy: 0.4650 - val_loss: 2.6040 - val_accuracy: 0.7750\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4597 - accuracy: 0.4050\n",
            "Epoch 454: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 10s 995ms/step - loss: 3.4597 - accuracy: 0.4050 - val_loss: 2.6026 - val_accuracy: 0.6750\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4821 - accuracy: 0.4350\n",
            "Epoch 455: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 12s 902ms/step - loss: 3.4821 - accuracy: 0.4350 - val_loss: 2.5086 - val_accuracy: 0.7500\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3488 - accuracy: 0.5050\n",
            "Epoch 456: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 10s 940ms/step - loss: 3.3488 - accuracy: 0.5050 - val_loss: 2.6634 - val_accuracy: 0.6750\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3684 - accuracy: 0.4500\n",
            "Epoch 457: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.3684 - accuracy: 0.4500 - val_loss: 2.7297 - val_accuracy: 0.6750\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3961 - accuracy: 0.4400\n",
            "Epoch 458: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 9s 920ms/step - loss: 3.3961 - accuracy: 0.4400 - val_loss: 2.7802 - val_accuracy: 0.7250\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4744 - accuracy: 0.4150\n",
            "Epoch 459: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.4744 - accuracy: 0.4150 - val_loss: 2.9120 - val_accuracy: 0.6000\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4442 - accuracy: 0.4050\n",
            "Epoch 460: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 9s 846ms/step - loss: 3.4442 - accuracy: 0.4050 - val_loss: 2.6434 - val_accuracy: 0.5750\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4896 - accuracy: 0.4400\n",
            "Epoch 461: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 6s 553ms/step - loss: 3.4896 - accuracy: 0.4400 - val_loss: 2.6570 - val_accuracy: 0.7250\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4468 - accuracy: 0.4300\n",
            "Epoch 462: val_loss did not improve from 2.40207\n",
            "10/10 [==============================] - 8s 718ms/step - loss: 3.4468 - accuracy: 0.4300 - val_loss: 2.7001 - val_accuracy: 0.6500\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5702 - accuracy: 0.4300\n",
            "Epoch 463: val_loss improved from 2.40207 to 2.38703, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.5702 - accuracy: 0.4300 - val_loss: 2.3870 - val_accuracy: 0.7750\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4117 - accuracy: 0.4300\n",
            "Epoch 464: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 17s 2s/step - loss: 3.4117 - accuracy: 0.4300 - val_loss: 2.6933 - val_accuracy: 0.6000\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4131 - accuracy: 0.4250\n",
            "Epoch 465: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.4131 - accuracy: 0.4250 - val_loss: 2.7459 - val_accuracy: 0.6750\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3493 - accuracy: 0.4650\n",
            "Epoch 466: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.3493 - accuracy: 0.4650 - val_loss: 2.6825 - val_accuracy: 0.7250\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4179 - accuracy: 0.4500\n",
            "Epoch 467: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 20s 2s/step - loss: 3.4179 - accuracy: 0.4500 - val_loss: 2.7121 - val_accuracy: 0.6750\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2599 - accuracy: 0.4600\n",
            "Epoch 468: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 20s 2s/step - loss: 3.2599 - accuracy: 0.4600 - val_loss: 2.8569 - val_accuracy: 0.6500\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3602 - accuracy: 0.4550\n",
            "Epoch 469: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 10s 987ms/step - loss: 3.3602 - accuracy: 0.4550 - val_loss: 2.8174 - val_accuracy: 0.6750\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5002 - accuracy: 0.3950\n",
            "Epoch 470: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.5002 - accuracy: 0.3950 - val_loss: 2.5603 - val_accuracy: 0.7000\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2830 - accuracy: 0.4950\n",
            "Epoch 471: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.2830 - accuracy: 0.4950 - val_loss: 2.6366 - val_accuracy: 0.6000\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3917 - accuracy: 0.4400\n",
            "Epoch 472: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.3917 - accuracy: 0.4400 - val_loss: 2.5031 - val_accuracy: 0.7250\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3976 - accuracy: 0.4400\n",
            "Epoch 473: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 20s 2s/step - loss: 3.3976 - accuracy: 0.4400 - val_loss: 2.8958 - val_accuracy: 0.6000\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4708 - accuracy: 0.4150\n",
            "Epoch 474: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.4708 - accuracy: 0.4150 - val_loss: 2.6738 - val_accuracy: 0.7000\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4647 - accuracy: 0.4350\n",
            "Epoch 475: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.4647 - accuracy: 0.4350 - val_loss: 2.5731 - val_accuracy: 0.7750\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4193 - accuracy: 0.4250\n",
            "Epoch 476: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 12s 960ms/step - loss: 3.4193 - accuracy: 0.4250 - val_loss: 2.7008 - val_accuracy: 0.7000\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3577 - accuracy: 0.4600\n",
            "Epoch 477: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 16s 1s/step - loss: 3.3577 - accuracy: 0.4600 - val_loss: 2.8620 - val_accuracy: 0.6000\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3236 - accuracy: 0.4950\n",
            "Epoch 478: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 9s 898ms/step - loss: 3.3236 - accuracy: 0.4950 - val_loss: 2.6943 - val_accuracy: 0.6500\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1390 - accuracy: 0.5250\n",
            "Epoch 479: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.1390 - accuracy: 0.5250 - val_loss: 2.8030 - val_accuracy: 0.5750\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2905 - accuracy: 0.4450\n",
            "Epoch 480: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2905 - accuracy: 0.4450 - val_loss: 2.4789 - val_accuracy: 0.7250\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4834 - accuracy: 0.4250\n",
            "Epoch 481: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 8s 830ms/step - loss: 3.4834 - accuracy: 0.4250 - val_loss: 2.6303 - val_accuracy: 0.6250\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4622 - accuracy: 0.4150\n",
            "Epoch 482: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.4622 - accuracy: 0.4150 - val_loss: 2.5534 - val_accuracy: 0.6500\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4445 - accuracy: 0.4050\n",
            "Epoch 483: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.4445 - accuracy: 0.4050 - val_loss: 2.8997 - val_accuracy: 0.5750\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.5549 - accuracy: 0.3900\n",
            "Epoch 484: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.5549 - accuracy: 0.3900 - val_loss: 2.7766 - val_accuracy: 0.7500\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4731 - accuracy: 0.3750\n",
            "Epoch 485: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 5s 338ms/step - loss: 3.4731 - accuracy: 0.3750 - val_loss: 2.7116 - val_accuracy: 0.6750\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4062 - accuracy: 0.4050\n",
            "Epoch 486: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 10s 1s/step - loss: 3.4062 - accuracy: 0.4050 - val_loss: 2.5962 - val_accuracy: 0.7500\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.6172 - accuracy: 0.3700\n",
            "Epoch 487: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 7s 541ms/step - loss: 3.6172 - accuracy: 0.3700 - val_loss: 2.6576 - val_accuracy: 0.7000\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4127 - accuracy: 0.4150\n",
            "Epoch 488: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 8s 834ms/step - loss: 3.4127 - accuracy: 0.4150 - val_loss: 2.5691 - val_accuracy: 0.7750\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2420 - accuracy: 0.4850\n",
            "Epoch 489: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2420 - accuracy: 0.4850 - val_loss: 2.6980 - val_accuracy: 0.6750\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2758 - accuracy: 0.4300\n",
            "Epoch 490: val_loss did not improve from 2.38703\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2758 - accuracy: 0.4300 - val_loss: 2.4937 - val_accuracy: 0.7500\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4703 - accuracy: 0.4400\n",
            "Epoch 491: val_loss improved from 2.38703 to 2.35783, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 15s 1s/step - loss: 3.4703 - accuracy: 0.4400 - val_loss: 2.3578 - val_accuracy: 0.8500\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2906 - accuracy: 0.4950\n",
            "Epoch 492: val_loss did not improve from 2.35783\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2906 - accuracy: 0.4950 - val_loss: 2.4975 - val_accuracy: 0.7250\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3189 - accuracy: 0.4450\n",
            "Epoch 493: val_loss did not improve from 2.35783\n",
            "10/10 [==============================] - 11s 920ms/step - loss: 3.3189 - accuracy: 0.4450 - val_loss: 2.5027 - val_accuracy: 0.7250\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3494 - accuracy: 0.4650\n",
            "Epoch 494: val_loss did not improve from 2.35783\n",
            "10/10 [==============================] - 6s 532ms/step - loss: 3.3494 - accuracy: 0.4650 - val_loss: 2.5416 - val_accuracy: 0.6750\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2993 - accuracy: 0.4350\n",
            "Epoch 495: val_loss did not improve from 2.35783\n",
            "10/10 [==============================] - 6s 567ms/step - loss: 3.2993 - accuracy: 0.4350 - val_loss: 2.4029 - val_accuracy: 0.7500\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4168 - accuracy: 0.4200\n",
            "Epoch 496: val_loss improved from 2.35783 to 2.31941, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 10s 980ms/step - loss: 3.4168 - accuracy: 0.4200 - val_loss: 2.3194 - val_accuracy: 0.7000\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3740 - accuracy: 0.4100\n",
            "Epoch 497: val_loss did not improve from 2.31941\n",
            "10/10 [==============================] - 6s 573ms/step - loss: 3.3740 - accuracy: 0.4100 - val_loss: 2.3778 - val_accuracy: 0.7250\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2875 - accuracy: 0.4550\n",
            "Epoch 498: val_loss did not improve from 2.31941\n",
            "10/10 [==============================] - 7s 440ms/step - loss: 3.2875 - accuracy: 0.4550 - val_loss: 2.5630 - val_accuracy: 0.7000\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3410 - accuracy: 0.4600\n",
            "Epoch 499: val_loss improved from 2.31941 to 2.23881, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 10s 983ms/step - loss: 3.3410 - accuracy: 0.4600 - val_loss: 2.2388 - val_accuracy: 0.8250\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4132 - accuracy: 0.4500\n",
            "Epoch 500: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 586ms/step - loss: 3.4132 - accuracy: 0.4500 - val_loss: 2.4026 - val_accuracy: 0.7000\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3666 - accuracy: 0.3950\n",
            "Epoch 501: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.3666 - accuracy: 0.3950 - val_loss: 2.4520 - val_accuracy: 0.7750\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1530 - accuracy: 0.4900\n",
            "Epoch 502: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 17s 1s/step - loss: 3.1530 - accuracy: 0.4900 - val_loss: 2.6183 - val_accuracy: 0.6250\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3093 - accuracy: 0.4650\n",
            "Epoch 503: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 533ms/step - loss: 3.3093 - accuracy: 0.4650 - val_loss: 2.5207 - val_accuracy: 0.8000\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4063 - accuracy: 0.4650\n",
            "Epoch 504: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.4063 - accuracy: 0.4650 - val_loss: 2.4193 - val_accuracy: 0.7500\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1835 - accuracy: 0.4750\n",
            "Epoch 505: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 499ms/step - loss: 3.1835 - accuracy: 0.4750 - val_loss: 2.6069 - val_accuracy: 0.6750\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2665 - accuracy: 0.4450\n",
            "Epoch 506: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 534ms/step - loss: 3.2665 - accuracy: 0.4450 - val_loss: 2.5850 - val_accuracy: 0.7000\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4290 - accuracy: 0.4500\n",
            "Epoch 507: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.4290 - accuracy: 0.4500 - val_loss: 2.7414 - val_accuracy: 0.6250\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4347 - accuracy: 0.4100\n",
            "Epoch 508: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 587ms/step - loss: 3.4347 - accuracy: 0.4100 - val_loss: 2.7126 - val_accuracy: 0.6250\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4039 - accuracy: 0.4350\n",
            "Epoch 509: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 530ms/step - loss: 3.4039 - accuracy: 0.4350 - val_loss: 2.7321 - val_accuracy: 0.6000\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3425 - accuracy: 0.4550\n",
            "Epoch 510: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 3.3425 - accuracy: 0.4550 - val_loss: 2.4249 - val_accuracy: 0.7500\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2865 - accuracy: 0.4950\n",
            "Epoch 511: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 7s 620ms/step - loss: 3.2865 - accuracy: 0.4950 - val_loss: 2.9874 - val_accuracy: 0.5500\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3501 - accuracy: 0.4700\n",
            "Epoch 512: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 533ms/step - loss: 3.3501 - accuracy: 0.4700 - val_loss: 2.4938 - val_accuracy: 0.7000\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4059 - accuracy: 0.3950\n",
            "Epoch 513: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 577ms/step - loss: 3.4059 - accuracy: 0.3950 - val_loss: 2.7158 - val_accuracy: 0.6250\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3775 - accuracy: 0.4350\n",
            "Epoch 514: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 5s 511ms/step - loss: 3.3775 - accuracy: 0.4350 - val_loss: 2.5807 - val_accuracy: 0.6250\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2246 - accuracy: 0.4450\n",
            "Epoch 515: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 3.2246 - accuracy: 0.4450 - val_loss: 2.5040 - val_accuracy: 0.7500\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3469 - accuracy: 0.4850\n",
            "Epoch 516: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 8s 719ms/step - loss: 3.3469 - accuracy: 0.4850 - val_loss: 2.7667 - val_accuracy: 0.6000\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4490 - accuracy: 0.4050\n",
            "Epoch 517: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 3.4490 - accuracy: 0.4050 - val_loss: 2.5489 - val_accuracy: 0.7000\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1109 - accuracy: 0.5100\n",
            "Epoch 518: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 476ms/step - loss: 3.1109 - accuracy: 0.5100 - val_loss: 2.6969 - val_accuracy: 0.7000\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3027 - accuracy: 0.4450\n",
            "Epoch 519: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 10s 977ms/step - loss: 3.3027 - accuracy: 0.4450 - val_loss: 2.6290 - val_accuracy: 0.6250\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2127 - accuracy: 0.4550\n",
            "Epoch 520: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.2127 - accuracy: 0.4550 - val_loss: 2.6370 - val_accuracy: 0.6750\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2193 - accuracy: 0.5200\n",
            "Epoch 521: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 10s 993ms/step - loss: 3.2193 - accuracy: 0.5200 - val_loss: 2.4300 - val_accuracy: 0.8250\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3096 - accuracy: 0.4750\n",
            "Epoch 522: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 6s 565ms/step - loss: 3.3096 - accuracy: 0.4750 - val_loss: 2.7617 - val_accuracy: 0.7250\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3124 - accuracy: 0.4250\n",
            "Epoch 523: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 7s 678ms/step - loss: 3.3124 - accuracy: 0.4250 - val_loss: 2.7304 - val_accuracy: 0.6500\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2233 - accuracy: 0.4800\n",
            "Epoch 524: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.2233 - accuracy: 0.4800 - val_loss: 2.6785 - val_accuracy: 0.6500\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1633 - accuracy: 0.5350\n",
            "Epoch 525: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 9s 910ms/step - loss: 3.1633 - accuracy: 0.5350 - val_loss: 2.3993 - val_accuracy: 0.7250\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3084 - accuracy: 0.4250\n",
            "Epoch 526: val_loss did not improve from 2.23881\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.3084 - accuracy: 0.4250 - val_loss: 2.6228 - val_accuracy: 0.6250\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4084 - accuracy: 0.4100\n",
            "Epoch 527: val_loss improved from 2.23881 to 2.22898, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 19s 2s/step - loss: 3.4084 - accuracy: 0.4100 - val_loss: 2.2290 - val_accuracy: 0.8000\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.4939 - accuracy: 0.4050\n",
            "Epoch 528: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.4939 - accuracy: 0.4050 - val_loss: 2.6027 - val_accuracy: 0.6500\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2869 - accuracy: 0.4700\n",
            "Epoch 529: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 12s 947ms/step - loss: 3.2869 - accuracy: 0.4700 - val_loss: 2.7353 - val_accuracy: 0.6000\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1662 - accuracy: 0.4800\n",
            "Epoch 530: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 15s 1s/step - loss: 3.1662 - accuracy: 0.4800 - val_loss: 2.4686 - val_accuracy: 0.7750\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1473 - accuracy: 0.5050\n",
            "Epoch 531: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.1473 - accuracy: 0.5050 - val_loss: 2.2998 - val_accuracy: 0.7500\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1695 - accuracy: 0.4700\n",
            "Epoch 532: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.1695 - accuracy: 0.4700 - val_loss: 2.6153 - val_accuracy: 0.6250\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3635 - accuracy: 0.4300\n",
            "Epoch 533: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.3635 - accuracy: 0.4300 - val_loss: 2.4067 - val_accuracy: 0.7250\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1106 - accuracy: 0.5000\n",
            "Epoch 534: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.1106 - accuracy: 0.5000 - val_loss: 2.4647 - val_accuracy: 0.7000\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0719 - accuracy: 0.5400\n",
            "Epoch 535: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 6s 422ms/step - loss: 3.0719 - accuracy: 0.5400 - val_loss: 2.5679 - val_accuracy: 0.6500\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1642 - accuracy: 0.4550\n",
            "Epoch 536: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.1642 - accuracy: 0.4550 - val_loss: 2.5685 - val_accuracy: 0.7250\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1293 - accuracy: 0.4600\n",
            "Epoch 537: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.1293 - accuracy: 0.4600 - val_loss: 2.5799 - val_accuracy: 0.7000\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2233 - accuracy: 0.4650\n",
            "Epoch 538: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.2233 - accuracy: 0.4650 - val_loss: 2.6304 - val_accuracy: 0.6250\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3972 - accuracy: 0.4350\n",
            "Epoch 539: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.3972 - accuracy: 0.4350 - val_loss: 2.8454 - val_accuracy: 0.5250\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3253 - accuracy: 0.4250\n",
            "Epoch 540: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.3253 - accuracy: 0.4250 - val_loss: 2.3539 - val_accuracy: 0.7500\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3389 - accuracy: 0.4750\n",
            "Epoch 541: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.3389 - accuracy: 0.4750 - val_loss: 2.6179 - val_accuracy: 0.6000\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1691 - accuracy: 0.4900\n",
            "Epoch 542: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 7s 540ms/step - loss: 3.1691 - accuracy: 0.4900 - val_loss: 2.4610 - val_accuracy: 0.7000\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2584 - accuracy: 0.4350\n",
            "Epoch 543: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 12s 970ms/step - loss: 3.2584 - accuracy: 0.4350 - val_loss: 2.7396 - val_accuracy: 0.6000\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2913 - accuracy: 0.4200\n",
            "Epoch 544: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.2913 - accuracy: 0.4200 - val_loss: 2.6557 - val_accuracy: 0.5750\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1529 - accuracy: 0.4950\n",
            "Epoch 545: val_loss did not improve from 2.22898\n",
            "10/10 [==============================] - 10s 1s/step - loss: 3.1529 - accuracy: 0.4950 - val_loss: 2.2976 - val_accuracy: 0.7750\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1827 - accuracy: 0.4850\n",
            "Epoch 546: val_loss improved from 2.22898 to 2.15973, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.1827 - accuracy: 0.4850 - val_loss: 2.1597 - val_accuracy: 0.9000\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2997 - accuracy: 0.4100\n",
            "Epoch 547: val_loss did not improve from 2.15973\n",
            "10/10 [==============================] - 21s 2s/step - loss: 3.2997 - accuracy: 0.4100 - val_loss: 2.4571 - val_accuracy: 0.6750\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2786 - accuracy: 0.4400\n",
            "Epoch 548: val_loss did not improve from 2.15973\n",
            "10/10 [==============================] - 15s 1s/step - loss: 3.2786 - accuracy: 0.4400 - val_loss: 2.7006 - val_accuracy: 0.5750\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2261 - accuracy: 0.4000\n",
            "Epoch 549: val_loss did not improve from 2.15973\n",
            "10/10 [==============================] - 16s 2s/step - loss: 3.2261 - accuracy: 0.4000 - val_loss: 2.3299 - val_accuracy: 0.8000\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3068 - accuracy: 0.4400\n",
            "Epoch 550: val_loss did not improve from 2.15973\n",
            "10/10 [==============================] - 21s 2s/step - loss: 3.3068 - accuracy: 0.4400 - val_loss: 2.5051 - val_accuracy: 0.6500\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2123 - accuracy: 0.4600\n",
            "Epoch 551: val_loss did not improve from 2.15973\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.2123 - accuracy: 0.4600 - val_loss: 2.7847 - val_accuracy: 0.5000\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2042 - accuracy: 0.4700\n",
            "Epoch 552: val_loss did not improve from 2.15973\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.2042 - accuracy: 0.4700 - val_loss: 2.7193 - val_accuracy: 0.6500\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2298 - accuracy: 0.4900\n",
            "Epoch 553: val_loss did not improve from 2.15973\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.2298 - accuracy: 0.4900 - val_loss: 2.6496 - val_accuracy: 0.6250\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1929 - accuracy: 0.5250\n",
            "Epoch 554: val_loss did not improve from 2.15973\n",
            "10/10 [==============================] - 12s 1s/step - loss: 3.1929 - accuracy: 0.5250 - val_loss: 2.4894 - val_accuracy: 0.7000\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1547 - accuracy: 0.4500\n",
            "Epoch 555: val_loss did not improve from 2.15973\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.1547 - accuracy: 0.4500 - val_loss: 2.3843 - val_accuracy: 0.7000\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1920 - accuracy: 0.4650\n",
            "Epoch 556: val_loss improved from 2.15973 to 2.15676, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.1920 - accuracy: 0.4650 - val_loss: 2.1568 - val_accuracy: 0.8000\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1752 - accuracy: 0.4750\n",
            "Epoch 557: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.1752 - accuracy: 0.4750 - val_loss: 2.5824 - val_accuracy: 0.6500\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1054 - accuracy: 0.4800\n",
            "Epoch 558: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 8s 833ms/step - loss: 3.1054 - accuracy: 0.4800 - val_loss: 2.7096 - val_accuracy: 0.6500\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2158 - accuracy: 0.4250\n",
            "Epoch 559: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.2158 - accuracy: 0.4250 - val_loss: 2.4480 - val_accuracy: 0.6750\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1747 - accuracy: 0.4650\n",
            "Epoch 560: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 6s 572ms/step - loss: 3.1747 - accuracy: 0.4650 - val_loss: 2.6175 - val_accuracy: 0.5750\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2686 - accuracy: 0.4250\n",
            "Epoch 561: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 7s 639ms/step - loss: 3.2686 - accuracy: 0.4250 - val_loss: 2.5822 - val_accuracy: 0.5750\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0788 - accuracy: 0.5150\n",
            "Epoch 562: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 8s 734ms/step - loss: 3.0788 - accuracy: 0.5150 - val_loss: 2.6548 - val_accuracy: 0.6000\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1320 - accuracy: 0.4700\n",
            "Epoch 563: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 8s 709ms/step - loss: 3.1320 - accuracy: 0.4700 - val_loss: 2.6271 - val_accuracy: 0.6500\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9515 - accuracy: 0.5500\n",
            "Epoch 564: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 9s 864ms/step - loss: 2.9515 - accuracy: 0.5500 - val_loss: 2.4314 - val_accuracy: 0.7250\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1438 - accuracy: 0.4950\n",
            "Epoch 565: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 8s 739ms/step - loss: 3.1438 - accuracy: 0.4950 - val_loss: 2.9146 - val_accuracy: 0.6000\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2269 - accuracy: 0.4350\n",
            "Epoch 566: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.2269 - accuracy: 0.4350 - val_loss: 2.2796 - val_accuracy: 0.8000\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2504 - accuracy: 0.4800\n",
            "Epoch 567: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 9s 865ms/step - loss: 3.2504 - accuracy: 0.4800 - val_loss: 2.5064 - val_accuracy: 0.6500\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3273 - accuracy: 0.4400\n",
            "Epoch 568: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 8s 753ms/step - loss: 3.3273 - accuracy: 0.4400 - val_loss: 2.5326 - val_accuracy: 0.6750\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1328 - accuracy: 0.4700\n",
            "Epoch 569: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 7s 681ms/step - loss: 3.1328 - accuracy: 0.4700 - val_loss: 2.3633 - val_accuracy: 0.7250\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1961 - accuracy: 0.4900\n",
            "Epoch 570: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 7s 611ms/step - loss: 3.1961 - accuracy: 0.4900 - val_loss: 2.3315 - val_accuracy: 0.7500\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3582 - accuracy: 0.3900\n",
            "Epoch 571: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 7s 664ms/step - loss: 3.3582 - accuracy: 0.3900 - val_loss: 2.4672 - val_accuracy: 0.7250\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1784 - accuracy: 0.4700\n",
            "Epoch 572: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 7s 643ms/step - loss: 3.1784 - accuracy: 0.4700 - val_loss: 2.4245 - val_accuracy: 0.7250\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2794 - accuracy: 0.4750\n",
            "Epoch 573: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 7s 625ms/step - loss: 3.2794 - accuracy: 0.4750 - val_loss: 2.5686 - val_accuracy: 0.6500\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1292 - accuracy: 0.5300\n",
            "Epoch 574: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 8s 756ms/step - loss: 3.1292 - accuracy: 0.5300 - val_loss: 2.5206 - val_accuracy: 0.7250\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.3179 - accuracy: 0.4150\n",
            "Epoch 575: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.3179 - accuracy: 0.4150 - val_loss: 2.2973 - val_accuracy: 0.8000\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2998 - accuracy: 0.4150\n",
            "Epoch 576: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 7s 683ms/step - loss: 3.2998 - accuracy: 0.4150 - val_loss: 2.4798 - val_accuracy: 0.6750\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1327 - accuracy: 0.5050\n",
            "Epoch 577: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 8s 722ms/step - loss: 3.1327 - accuracy: 0.5050 - val_loss: 2.3359 - val_accuracy: 0.6750\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2062 - accuracy: 0.4700\n",
            "Epoch 578: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 9s 792ms/step - loss: 3.2062 - accuracy: 0.4700 - val_loss: 2.2521 - val_accuracy: 0.8000\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2096 - accuracy: 0.4450\n",
            "Epoch 579: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 5s 419ms/step - loss: 3.2096 - accuracy: 0.4450 - val_loss: 2.5977 - val_accuracy: 0.6750\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1537 - accuracy: 0.4850\n",
            "Epoch 580: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.1537 - accuracy: 0.4850 - val_loss: 2.6071 - val_accuracy: 0.6500\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.2651 - accuracy: 0.4050\n",
            "Epoch 581: val_loss did not improve from 2.15676\n",
            "10/10 [==============================] - 8s 765ms/step - loss: 3.2651 - accuracy: 0.4050 - val_loss: 2.3802 - val_accuracy: 0.7250\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1469 - accuracy: 0.4700\n",
            "Epoch 582: val_loss improved from 2.15676 to 2.15383, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.1469 - accuracy: 0.4700 - val_loss: 2.1538 - val_accuracy: 0.8500\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1081 - accuracy: 0.5000\n",
            "Epoch 583: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 15s 1s/step - loss: 3.1081 - accuracy: 0.5000 - val_loss: 2.4432 - val_accuracy: 0.7500\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1351 - accuracy: 0.5000\n",
            "Epoch 584: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.1351 - accuracy: 0.5000 - val_loss: 2.5115 - val_accuracy: 0.7250\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1317 - accuracy: 0.4650\n",
            "Epoch 585: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 7s 580ms/step - loss: 3.1317 - accuracy: 0.4650 - val_loss: 2.4847 - val_accuracy: 0.6500\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1805 - accuracy: 0.4750\n",
            "Epoch 586: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 9s 864ms/step - loss: 3.1805 - accuracy: 0.4750 - val_loss: 2.2849 - val_accuracy: 0.7500\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0159 - accuracy: 0.5050\n",
            "Epoch 587: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 10s 863ms/step - loss: 3.0159 - accuracy: 0.5050 - val_loss: 2.2986 - val_accuracy: 0.7750\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0898 - accuracy: 0.4800\n",
            "Epoch 588: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 3.0898 - accuracy: 0.4800 - val_loss: 2.3505 - val_accuracy: 0.6750\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1139 - accuracy: 0.5200\n",
            "Epoch 589: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 9s 866ms/step - loss: 3.1139 - accuracy: 0.5200 - val_loss: 2.4535 - val_accuracy: 0.7000\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0240 - accuracy: 0.5250\n",
            "Epoch 590: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 9s 872ms/step - loss: 3.0240 - accuracy: 0.5250 - val_loss: 2.2863 - val_accuracy: 0.7250\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0319 - accuracy: 0.5500\n",
            "Epoch 591: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.0319 - accuracy: 0.5500 - val_loss: 2.4216 - val_accuracy: 0.6750\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1522 - accuracy: 0.4850\n",
            "Epoch 592: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 9s 809ms/step - loss: 3.1522 - accuracy: 0.4850 - val_loss: 2.3807 - val_accuracy: 0.7750\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1279 - accuracy: 0.4800\n",
            "Epoch 593: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 9s 885ms/step - loss: 3.1279 - accuracy: 0.4800 - val_loss: 2.3948 - val_accuracy: 0.7000\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.8921 - accuracy: 0.5400\n",
            "Epoch 594: val_loss did not improve from 2.15383\n",
            "10/10 [==============================] - 10s 958ms/step - loss: 2.8921 - accuracy: 0.5400 - val_loss: 2.4034 - val_accuracy: 0.7250\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0891 - accuracy: 0.4950\n",
            "Epoch 595: val_loss improved from 2.15383 to 2.03681, saving model to Vgg16_ASL.h5\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.0891 - accuracy: 0.4950 - val_loss: 2.0368 - val_accuracy: 0.8750\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1936 - accuracy: 0.4850\n",
            "Epoch 596: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.1936 - accuracy: 0.4850 - val_loss: 2.5464 - val_accuracy: 0.6000\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0992 - accuracy: 0.4950\n",
            "Epoch 597: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 10s 1s/step - loss: 3.0992 - accuracy: 0.4950 - val_loss: 2.3310 - val_accuracy: 0.7500\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0069 - accuracy: 0.5100\n",
            "Epoch 598: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.0069 - accuracy: 0.5100 - val_loss: 2.2030 - val_accuracy: 0.8500\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1275 - accuracy: 0.4800\n",
            "Epoch 599: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 16s 2s/step - loss: 3.1275 - accuracy: 0.4800 - val_loss: 2.3812 - val_accuracy: 0.6500\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9809 - accuracy: 0.5050\n",
            "Epoch 600: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 12s 1s/step - loss: 2.9809 - accuracy: 0.5050 - val_loss: 2.1292 - val_accuracy: 0.7500\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1246 - accuracy: 0.4450\n",
            "Epoch 601: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.1246 - accuracy: 0.4450 - val_loss: 2.0442 - val_accuracy: 0.8250\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1098 - accuracy: 0.5000\n",
            "Epoch 602: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 8s 588ms/step - loss: 3.1098 - accuracy: 0.5000 - val_loss: 2.2376 - val_accuracy: 0.7750\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0571 - accuracy: 0.5050\n",
            "Epoch 603: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 8s 775ms/step - loss: 3.0571 - accuracy: 0.5050 - val_loss: 2.3219 - val_accuracy: 0.7000\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1506 - accuracy: 0.4850\n",
            "Epoch 604: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.1506 - accuracy: 0.4850 - val_loss: 2.4090 - val_accuracy: 0.6000\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0561 - accuracy: 0.4850\n",
            "Epoch 605: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 9s 918ms/step - loss: 3.0561 - accuracy: 0.4850 - val_loss: 2.2280 - val_accuracy: 0.8000\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1932 - accuracy: 0.4750\n",
            "Epoch 606: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 18s 2s/step - loss: 3.1932 - accuracy: 0.4750 - val_loss: 2.5298 - val_accuracy: 0.6750\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9207 - accuracy: 0.5750\n",
            "Epoch 607: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.9207 - accuracy: 0.5750 - val_loss: 2.2976 - val_accuracy: 0.8250\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1768 - accuracy: 0.4400\n",
            "Epoch 608: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 7s 642ms/step - loss: 3.1768 - accuracy: 0.4400 - val_loss: 2.1889 - val_accuracy: 0.8250\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1462 - accuracy: 0.4950\n",
            "Epoch 609: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 16s 2s/step - loss: 3.1462 - accuracy: 0.4950 - val_loss: 2.1221 - val_accuracy: 0.8500\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1586 - accuracy: 0.5200\n",
            "Epoch 610: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 9s 868ms/step - loss: 3.1586 - accuracy: 0.5200 - val_loss: 2.2850 - val_accuracy: 0.8000\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0275 - accuracy: 0.5100\n",
            "Epoch 611: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 17s 2s/step - loss: 3.0275 - accuracy: 0.5100 - val_loss: 2.1830 - val_accuracy: 0.7500\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0564 - accuracy: 0.5150\n",
            "Epoch 612: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.0564 - accuracy: 0.5150 - val_loss: 2.1743 - val_accuracy: 0.8500\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0352 - accuracy: 0.4750\n",
            "Epoch 613: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 13s 994ms/step - loss: 3.0352 - accuracy: 0.4750 - val_loss: 2.3944 - val_accuracy: 0.6000\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1483 - accuracy: 0.5250\n",
            "Epoch 614: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 8s 720ms/step - loss: 3.1483 - accuracy: 0.5250 - val_loss: 2.3884 - val_accuracy: 0.7500\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0726 - accuracy: 0.4850\n",
            "Epoch 615: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 15s 1s/step - loss: 3.0726 - accuracy: 0.4850 - val_loss: 2.2997 - val_accuracy: 0.7750\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1538 - accuracy: 0.4900\n",
            "Epoch 616: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 13s 995ms/step - loss: 3.1538 - accuracy: 0.4900 - val_loss: 2.6220 - val_accuracy: 0.6750\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0071 - accuracy: 0.5400\n",
            "Epoch 617: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 8s 827ms/step - loss: 3.0071 - accuracy: 0.5400 - val_loss: 2.3239 - val_accuracy: 0.7500\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9686 - accuracy: 0.4850\n",
            "Epoch 618: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 8s 680ms/step - loss: 2.9686 - accuracy: 0.4850 - val_loss: 2.4269 - val_accuracy: 0.6500\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1491 - accuracy: 0.4650\n",
            "Epoch 619: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 9s 917ms/step - loss: 3.1491 - accuracy: 0.4650 - val_loss: 2.3522 - val_accuracy: 0.7500\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9892 - accuracy: 0.5350\n",
            "Epoch 620: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.9892 - accuracy: 0.5350 - val_loss: 2.3300 - val_accuracy: 0.8000\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.1065 - accuracy: 0.5350\n",
            "Epoch 621: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 10s 961ms/step - loss: 3.1065 - accuracy: 0.5350 - val_loss: 2.4498 - val_accuracy: 0.6750\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.8953 - accuracy: 0.5450\n",
            "Epoch 622: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 12s 1s/step - loss: 2.8953 - accuracy: 0.5450 - val_loss: 2.6095 - val_accuracy: 0.6500\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0435 - accuracy: 0.5100\n",
            "Epoch 623: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 14s 1s/step - loss: 3.0435 - accuracy: 0.5100 - val_loss: 2.2590 - val_accuracy: 0.8000\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0939 - accuracy: 0.4700\n",
            "Epoch 624: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 15s 1s/step - loss: 3.0939 - accuracy: 0.4700 - val_loss: 2.5613 - val_accuracy: 0.6500\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9840 - accuracy: 0.4900\n",
            "Epoch 625: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 21s 2s/step - loss: 2.9840 - accuracy: 0.4900 - val_loss: 2.2813 - val_accuracy: 0.7000\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0109 - accuracy: 0.5250\n",
            "Epoch 626: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 17s 2s/step - loss: 3.0109 - accuracy: 0.5250 - val_loss: 2.1831 - val_accuracy: 0.7500\n",
            "Epoch 627/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0637 - accuracy: 0.4500\n",
            "Epoch 627: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 13s 1s/step - loss: 3.0637 - accuracy: 0.4500 - val_loss: 2.4779 - val_accuracy: 0.6750\n",
            "Epoch 628/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0311 - accuracy: 0.4800\n",
            "Epoch 628: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 11s 1s/step - loss: 3.0311 - accuracy: 0.4800 - val_loss: 2.1456 - val_accuracy: 0.8500\n",
            "Epoch 629/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0880 - accuracy: 0.4800\n",
            "Epoch 629: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 21s 2s/step - loss: 3.0880 - accuracy: 0.4800 - val_loss: 2.6823 - val_accuracy: 0.5250\n",
            "Epoch 630/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9943 - accuracy: 0.5150\n",
            "Epoch 630: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 13s 1s/step - loss: 2.9943 - accuracy: 0.5150 - val_loss: 2.3931 - val_accuracy: 0.7000\n",
            "Epoch 631/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0345 - accuracy: 0.4500\n",
            "Epoch 631: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 7s 678ms/step - loss: 3.0345 - accuracy: 0.4500 - val_loss: 2.6286 - val_accuracy: 0.6000\n",
            "Epoch 632/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9851 - accuracy: 0.5300\n",
            "Epoch 632: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 8s 788ms/step - loss: 2.9851 - accuracy: 0.5300 - val_loss: 2.3956 - val_accuracy: 0.7250\n",
            "Epoch 633/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0702 - accuracy: 0.5200\n",
            "Epoch 633: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 6s 511ms/step - loss: 3.0702 - accuracy: 0.5200 - val_loss: 2.4225 - val_accuracy: 0.7250\n",
            "Epoch 634/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0913 - accuracy: 0.4750\n",
            "Epoch 634: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 17s 2s/step - loss: 3.0913 - accuracy: 0.4750 - val_loss: 2.1610 - val_accuracy: 0.8000\n",
            "Epoch 635/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0480 - accuracy: 0.5050\n",
            "Epoch 635: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 15s 2s/step - loss: 3.0480 - accuracy: 0.5050 - val_loss: 2.1550 - val_accuracy: 0.8250\n",
            "Epoch 636/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9534 - accuracy: 0.5200\n",
            "Epoch 636: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 10s 1s/step - loss: 2.9534 - accuracy: 0.5200 - val_loss: 2.3143 - val_accuracy: 0.7500\n",
            "Epoch 637/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.8770 - accuracy: 0.5400\n",
            "Epoch 637: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 6s 613ms/step - loss: 2.8770 - accuracy: 0.5400 - val_loss: 2.2381 - val_accuracy: 0.7750\n",
            "Epoch 638/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0026 - accuracy: 0.5250\n",
            "Epoch 638: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 7s 693ms/step - loss: 3.0026 - accuracy: 0.5250 - val_loss: 2.1802 - val_accuracy: 0.8250\n",
            "Epoch 639/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9368 - accuracy: 0.5400\n",
            "Epoch 639: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 12s 1s/step - loss: 2.9368 - accuracy: 0.5400 - val_loss: 2.1222 - val_accuracy: 0.8750\n",
            "Epoch 640/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0276 - accuracy: 0.5100\n",
            "Epoch 640: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 10s 656ms/step - loss: 3.0276 - accuracy: 0.5100 - val_loss: 2.2289 - val_accuracy: 0.7750\n",
            "Epoch 641/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0367 - accuracy: 0.4950\n",
            "Epoch 641: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 9s 914ms/step - loss: 3.0367 - accuracy: 0.4950 - val_loss: 2.3148 - val_accuracy: 0.7000\n",
            "Epoch 642/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9453 - accuracy: 0.5150\n",
            "Epoch 642: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 9s 890ms/step - loss: 2.9453 - accuracy: 0.5150 - val_loss: 2.1768 - val_accuracy: 0.7750\n",
            "Epoch 643/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9331 - accuracy: 0.5100\n",
            "Epoch 643: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 16s 2s/step - loss: 2.9331 - accuracy: 0.5100 - val_loss: 2.4052 - val_accuracy: 0.7250\n",
            "Epoch 644/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.9400 - accuracy: 0.5550\n",
            "Epoch 644: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 9s 865ms/step - loss: 2.9400 - accuracy: 0.5550 - val_loss: 2.2001 - val_accuracy: 0.7000\n",
            "Epoch 645/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 2.8643 - accuracy: 0.5600\n",
            "Epoch 645: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 14s 1s/step - loss: 2.8643 - accuracy: 0.5600 - val_loss: 2.2610 - val_accuracy: 0.7000\n",
            "Epoch 646/1000\n",
            "10/10 [==============================] - ETA: 0s - loss: 3.0808 - accuracy: 0.4600\n",
            "Epoch 646: val_loss did not improve from 2.03681\n",
            "10/10 [==============================] - 9s 864ms/step - loss: 3.0808 - accuracy: 0.4600 - val_loss: 2.3138 - val_accuracy: 0.7750\n",
            "Epoch 646: early stopping\n"
          ]
        }
      ],
      "source": [
        "hist = model.fit_generator(generator= train_generator, steps_per_epoch= 10, epochs= 1000, validation_data= validation_generator, validation_steps=2, callbacks=[checkpoint,early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4Jzz5XROr_w0"
      },
      "outputs": [],
      "source": [
        "model.save(\"Vgg16_ASL.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def load_model_fn():\n",
        "    model = load_model('Vgg16_ASL.h5')\n",
        "\n",
        "    return model\n",
        "saved_model = load_model_fn()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               12845568  \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 29)                14877     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27,577,181\n",
            "Trainable params: 12,861,469\n",
            "Non-trainable params: 14,715,712\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "saved_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [],
      "source": [
        "def category(output):\n",
        "    max = np.argmax(output)\n",
        "    category = ''\n",
        "    if max == 0:\n",
        "      category = \"A\"\n",
        "    elif max == 1:\n",
        "      category = \"B\"\n",
        "    elif max == 2:\n",
        "      category = \"C\"\n",
        "    elif max == 3:\n",
        "      category = \"D\"\n",
        "    elif max == 4:\n",
        "      category = \"del\"\n",
        "    elif max == 5:\n",
        "      category = \"E\"\n",
        "    elif max == 6:\n",
        "      category = \"F\"\n",
        "    elif max == 7:\n",
        "      category = \"G\"\n",
        "    elif max == 8:\n",
        "      category = \"H\" \n",
        "    elif max == 9:\n",
        "      category = \"I\"\n",
        "    elif max == 10:\n",
        "      category = \"J\"\n",
        "    elif max == 11:\n",
        "      category = \"K\"\n",
        "    elif max == 12:\n",
        "      category = \"L\"\n",
        "    elif max == 13:\n",
        "      category = \"M\"\n",
        "    elif max == 14:\n",
        "      category = \"N\"\n",
        "    elif max == 15:\n",
        "      category = \"nothing\"\n",
        "    elif max == 16:\n",
        "      category = \"O\"\n",
        "    elif max == 17:\n",
        "      category = \"P\" \n",
        "    elif max == 18:\n",
        "      category = \"Q\"\n",
        "    elif max == 19:\n",
        "      category = \"R\"\n",
        "    elif max == 20:\n",
        "      category = \"S\"\n",
        "    elif max == 21:\n",
        "      category = \"space\"\n",
        "    elif max == 22:\n",
        "      category = \"T\"\n",
        "    elif max == 23:\n",
        "      category = \"U\"\n",
        "    elif max == 24:\n",
        "      category = \"V\"\n",
        "    elif max == 25:\n",
        "      category = \"W\"\n",
        "    elif max == 26:\n",
        "      category = \"X\" \n",
        "    elif max == 27:\n",
        "      category = \"Y\"\n",
        "    elif max == 28:\n",
        "      category = \"Z\"\n",
        "\n",
        "    #print(\"Its a \" + str(category))\n",
        "\n",
        "    return category\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "JlxAzMuX55Id"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "10_test.jpg,I\n",
            "11_test.jpg,space\n",
            "12_test.jpg,K\n",
            "13_test.jpg,M\n",
            "14_test.jpg,M\n",
            "15_test.jpg,Y\n",
            "16_test.jpg,N\n",
            "17_test.jpg,nothing\n",
            "18_test.jpg,X\n",
            "19_test.jpg,P\n",
            "1_test.jpg,Q\n",
            "20_test.jpg,Q\n",
            "21_test.jpg,Z\n",
            "22_test.jpg,R\n",
            "23_test.jpg,S\n",
            "24_test.jpg,space\n",
            "25_test.jpg,T\n",
            "26_test.jpg,Q\n",
            "27_test.jpg,G\n",
            "28_test.jpg,G\n",
            "29_test.jpg,X\n",
            "2_test.jpg,B\n",
            "3_test.jpg,C\n",
            "4_test.jpg,D\n",
            "5_test.jpg,Q\n",
            "6_test.jpg,E\n",
            "7_test.jpg,G\n",
            "8_test.jpg,G\n",
            "9_test.jpg,H\n",
            "\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'to_csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [162], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;66;03m#print((line))\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m((line))\n\u001b[1;32m---> 30\u001b[0m \u001b[43mwritetocsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn [157], line 3\u001b[0m, in \u001b[0;36mwritetocsv\u001b[1;34m(line)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwritetocsv\u001b[39m(line):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mInsaid\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_csv'"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGhCAYAAADbf0s2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOz9WaxsS3IXDv8i16qqvc+5Q7vbQ3eLiwHrz8dokMA0CDAWYBAgSxYWQsADkxiEbQm3kKARk3lp3vADBl6QeTAWg8QfJJD80JYwAmGBjJCFEP5wC2HAX7d7uOecPdWwMuN7iCEjc62qXfvcc7v73t557z5VtVbOQ/wiIiMjiZkZj+ExPIbH8Bgew1dgSF/uCjyGx/AYHsNjeAzHwiNIPYbH8Bgew2P4ig2PIPUYHsNjeAyP4Ss2PILUY3gMj+ExPIav2PAIUo/hMTyGx/AYvmLDI0g9hsfwGB7DY/iKDY8g9Rgew2N4DI/hKzY8gtRjeAyP4TE8hq/Y8AhSj+ExPIbH8Bi+YsMjSD2Gx/AYHsNj+IoNXzaQ+sEf/EH8ol/0i3BxcYGPfexj+I//8T9+uaryGB7DY3gMj+ErNHxZQOqf/JN/go9//OP463/9r+M//+f/jF/za34Nfvfv/t34+Z//+S9HdR7DY3gMj+ExfIUG+nI4mP3Yxz6Gb/mWb8Hf+Tt/BwBQSsFbb72F7/3e78Vf+kt/6d70pRT83M/9HF5//XUQ0btd3cfwGB7DY3gMrzgwM66urvDRj34UKR2Xl8YvYZ0AAPv9Hj/5kz+JT3ziE/4spYTf+Tt/J/7Df/gPi2l2ux12u53//r//9//iV/yKX/Gu1/UxPIbH8Bgew7sb/vf//t/4Bb/gFxx9/yUHqc9//vPIOeMbvuEbmuff8A3fgP/+3//7YppPfvKT+P7v//75C/ofADPG1TOk4RYlvw0CYSBrFmEYCblk5HIAQCAiQW1m+QPAIAADCg8AomTGICpIVACw53lfkGzZPy1dojY944QQ26SVTyICKKYjAKmpUpSLiQBwqDvHCLTQFgLIOJridSASrqdpSxo8u1imfa8SLsP6nZp2yV+sBbO+Dg9rfl09a4lNvJRC/4Y+JO8CLZcZpcmKcEwoT7WG3g/92JJW2vIoWmlibUwCuFDoKxvTWAfuPu1FAXFq3qQQkyH9nQAwBhlGWH8y6vATUACyPgPAZLUnLy5Z/a0lzCizuWptHXROSoEU6giu/ZS0rX0fM6PLu/ahzwVvZV8DyyT2howXeytjry3VP+QVxlTqLb1gczfO0zZemz8hYdB11I93buqTdJ1QO6VRx27e730zyOvftJqbiTVrM4PCGM/pBhG38ZkBlLa9Hs/oQNGxIERKmnIGlYIJa4AuQZsPYrsHcrkG8Fvw+uuvn2zilxykXiZ84hOfwMc//nH//eLFC7z11ltIGDGOO/w//58VPvjBDdJ4APMBXCaJqDRXxovCZJOFx0UGAkhgHsBMYO5AKjESGERCrFMguCnJJLO/OGFgZSkwWVokiTuk5Ks2eWVj0TL4hWWCMDMoEVgJpD1TUouotS1FloIAMnm2cSFKfEIluNbiBCMKg9NKIXRcWIia5iuoS8g5AD4rDDCDS0EpxduWFJQ4IBvps5qWvS0OCFBC2oN8qHgkxvG3fTpIkRIe5oaQGZG1nopgVAGuAncpxcvjIm0NZAxFQZCKPCuk8bScBoYMUbweBVK7pF2n39nyKUAuHt+AJgFgEiIBljowWCaRja3NVQ3F+k2BSKG2Mgo2X+pXqbdVPAIKS12d+DnXwVCaB0Kc5PK6sKUgZzKIdGwcfFsoSHG56Jplr38CE3s+VpUY4roEwhiHiDEOhZqzr012IJZ5pmtSGuXzj0uFGm7WSZz91h8xP+v5+4O0UxhS9iHVAWx5gJZ+hDIIAKUktCqR0zcASESgRBiGASklDENCGmS8EoT5KplBNABISFRX10ATEhVM5TVk/hrs+TV85ud3eHH1HHc74L4tmy85SH3t134thmHAZz/72eb5Zz/7WXz4wx9eTLPZbLDZbGbPEw4Y0h0++DU7fOSjO4ybKxTeImdTDQrIUEqgNGhnMIinSvhNeuIBhamZzAwgESMZgVW6nKgCDRGBUkIKLDEpIZe/wIlpXCLCahhAlAAdZCKAw2CRErISJz4ZSMU/mQoCBlpvriA16KQz+HSQQJ2sscFssxiMwRYmMbhoXG3DMIygJPXPUwALrdc0TSilIOfsfTpQcYJo5SaUJq3Q1NzkZW0x8G/id98NALl7VkGqLh7rO2t76SQBZgGjysyzdRJKyTVeLii5bUfRPyoyZkU5TR/LGfGhQOg0L0re/0k546zgzzOQUgkiKaNVpKyiIMUsACZrIYCU9XFhZC03OUiRg0tlF2KNqdZBvxFXKYyM4eDKIEV6ZLmVCAxJ5ToixZ6qgaj5Kn9kv5yJCCClhaZEM2LvcBPWJnxoK1GvzGeUpi0eVHIwIOL6qUwLG/gHkIIzYgrQqPPealnCHJ6ha+i3OBwC6gpSlj/TIshZXUsxGlKcMUmUQMlAavDxSilhSAOGccQwJIzjIECVlMlmEqUNjaA0IlH28lbDASkV5PIBHMqEu+lNPH9+i7vbZ7ib1W4evuQgtV6v8et+3a/Dj/3Yj+E7v/M7AQiB/bEf+zF8z/d8z4PyGjGhTM8wrp5jfXmFtPn/gvkOA28ByGQeBjhITXmSASkZxLYOR520CSUsMJt8CQVJZ3ACMFDgvoJeohHLdcIbYaRkk6YgpQEpEUoSkDKOJXJ1fZD6axybz4aJqunoCbYJdlXqq9y2QVbkqGpao0iMkaqk6KAyrpDGAZvNBsNqhWFcqXCv6UnyvH5xhWmacDgckHNGyRng7AQRXISPjN1mIEDF1QveB03nREnQ0hZfzw4CLkFEGSd855Z0MaqUYf0iEpPxuyQAVwBGrv1mBE7LrYufwVnGXkCrCIHgSLSsTirVQomaS5oQsNA8JwXN5GpZka50dqDAOPkwtsbwEHQGRJAqXl7OQTUcGCsnlqlVLce+PmZ/1YxFF8e4+Ki8iGuh9sCxhWEAY5Etf2U1jDlL9Z0smYSqhdW0QW1VK1iBKkUG0jQm8VOBhpkVZJP3W6PlcOlLGIjCBZxlLmUwSi7IJcM1B5EpMwY1F+9O0QaptkEXfTFoWhwT8vgcQMp7KDDAAq6SR54YB1MWaL+RKoMGZmWI1xiGS6Rhg2GorEtebTEMGUgfRuaPiBaAMwi3y+PahS+Luu/jH/84/ugf/aP49b/+1+M3/IbfgB/4gR/Azc0N/vgf/+MPyocF+kEDkMYCTgcAE8gHh1GMc+YCkGpiKfsETzZ5qVROGTZvCxKKpmdl2qqewUgXE4cyQ6CqMVHyg8JJt4mSc4yRY6tJK2glkb/hag0FIQAKXpgTCQWwStypLmoiJbckUlJY3BwqPbj0aRwzI6UkqhbKIgmCQKmSfmkH4+JyxDQBw8g4HGTxlawLQydpJf7an2xjUJRocK16M+6m4oxPjbCTc7DHaFvNp/1edLFVEC9IBqpWD8sbod+EHa4gxVwJwMCBRhcnAHOQMiJpIKega8Nhz3JROhGJZpVUEuD1kqoXq7qrqsMElllJUqdhae+VJKOiUvSsDwlh/ixw7ZoeaU4zXdL1KUogypgP3PGBJJ3Xbfl11dXU8V3y/vbC2ZSdIavCxunJ+uvrRLabp8yL4R15zfR3YIUiQ4giIKVMWeICTqrhIcu/0qOiIFWmjFJEOodKadZSosqoLDMO5LGLTCwULoFxCwwacpjjDCZu1MMiSAYVcRGtSSqqWdB2l3wDSgdw2mAqF9gd3kbOIwj7hfrNw5cFpP7gH/yD+NznPoe/9tf+Gj7zmc/g1/7aX4sf/dEfnRlT3BcmAKthQFol0EgoVFRkFc7euQHlKodBOBw3DiACYajyBWUHLSGEBcRFJpaOYgEHwh/UUWj4+rpuurjMGShARm5W7RJ3Rsohi/69AlRMk1Sd1wdKCnK6BwZWlWSKC6vdMPf6ysYThsHqT3W5a76258QpozC19U4Jl08vUHLBfr/HfpcwTQPyXtRkmVg3V+B9F1WQ1i9VYq3VY4N8j9cSJM/jBGGrAlBtu0kTRpOsZlzYKK3XlfrRLkU5Wsl7ACnQEFDCvhfTjHAsqSxLIZRCoU8M9Eh23ntmSr80kp1WJhLFQTn80JkS3I6on8WmNoUQxVJVONIZ5AzYse72OlGnSnemT1/rJpPkV7z+dVhPABUl74RaBseB7taIFxQEsA6c2dLr/C9RytZ1WsjXR5U8+zXafrYV5xaktA5JmcOoPTBJi0tBGQhTFmbGmllCvIQKNMuBdZ6GZsa3bKAYGRopOxdlWHy/2fpYFPdcRhQigCZv8w7XYGyRMWLKa2wPn8f+8AaQjtWvDV82w4nv+Z7vebB67zE8hsfwGB7DV1d4T1j3nQoiTqvxwpAgZp3yjpwTVe4nqfjMYRPedp1NaHfunmreCNxG4P5iHWKQtIGb6iutaiNj0HspatbGhtNb5r6jgQEAUVUk4cpNKmMQShHOk4hAHCQkD7ZrZZwYIyij3KDDamRCZuw3+RTFk5SdkCihUIJYSZIaiQROWcfCes5VJo2VlnQa6/Oq7jM9HCQP4hl3GINwu7TwsEqwLqElkaZMeWNSS58UiZzzdnXNTMKoUmh8VkVKyUO0SwTdbvB9IVE/Sj9FCb5OTVILvXafJ5Su36oIa/MwSrPzmklF6j6RLxnPd3GDnlHT2PcgJbOqe5lqnqx1Kq5SbWsyX0v2brHBC2r4GK+ma/YA5y2BjasUaWMc2hg1AV1aaqRIyauZBz4sdd5QM7JShu9Hm6Ym9JutOiLVUiJYbjZtN1pj6yfWwRoEXYvte7Eutn1x3XezOWm0NNS3HbNoHWnvvsIlqVcSEgEDMF6ucPHaBTC+BtAeFNQFxfcKdDKWgml368OVeIQNDqPUjXzVNycj2pal/fneguyjROs+GatiyjRUhU9q1ElGGE6ZYBZNL3sh0TpMc0y6C9EBFxF0Y5M9nlmN275EQt2ktholGhTEEsYCEA0ADzBdBo1iTJKTbKqa6sO0SIkG3csaQCS1Jwwow4QDEabpoH1NappbTbmhAJYQrYpib0h/Mydpb/OuqnhdpcXz/bpm3yqEpIswpWqRafHZdqmpmvu3xM+MWvRMUTRaWAh9fZZUgLNnusflxhelbnYXszIr1Vqv5GC8wXEOyV7CUp2qgU+do2b+ndtDZUGNJX1c7JiC032pgxH3UqrRTORLnCHQCTvA9j0KqsFqZBxC8DXHzqQYcCRdlzAetAHo2Lc6j61/Ynlh8gXD+qpF1JMupVPLAWZKX89Z1Trr2vR62xplDP6dkJIMQqzpwLq/Pkg7zepRGCs7mWSqZlTT94X5D4gK13blLZ5ZvArJtNHv2oBA/5hBuYALoWRCQVIGoxoHDT5OA6ZCKEUM2I6xBH14T4NUwYQJGauLDS6fMmj9OsA7gA8AdAEX2YhlJt2fKhgHUiLDfkhSFvcAZiF2Mv4MUo7CFn0uBbAN/lJkINk4H9vb0rwtHz33Ihxrw4u2CzUESWd8kyxC0RWzm0oLtnbcsTFPum7dnDhOPuO0LB8rk4TY2BkHVksyO9/DxKAkBhNpKEjDAZQGrNZrN2terSaM44hxXMOakJBAaQQPK4CBkoQAMRFsy9Z5NmOlvR9D1/hisz2/DgSMuBoRsP20wOWSs+5STli+YjHnxDf0aqrUt3Kykdip3SRnqPDVAEQfmCPBU0CqB3VQjwGEoPSiIIAmDEDquT/f9E41bwMx74S+27SeOU+ViPnk1f5j67cQiGFzPtlBT7bndn6GtU9knfDCPgSZmRjJGiVlKk04NX58ce+1r1L/jY/MldD2yDT6HPL8ubZrMdjMjWPGTTc3UGPA6bQgjL29VmnIpRoNRQ10ZFhSMLbUw/U6LdTqHkhqih8ziVUZWOfbANOamMFOcY7D1mCQwKzdDGVAACRhUAsTChNyqUc9JGkCYcCQRozjCikNpzq1Ce9pkGJMyDxhffEUT95YY1i/CeYtuMg5KenACYbi+/1eCO9mDRtRbv4y7IwRoFZ1hYCSkLOa6h4OKJzVY0H2iaannbxmJp7LwOcqg3ecSctlBZst9m1r2OSxUxBFCYHQkSrB1dwNgAUIEhEy1KaJ48onN8WXX0J8k+ctJeYpuwm1mgwCtHe14fpi4+3YbDZYrVbYrDkc+BtkUg4MMGFKGUiMwlnMgRvgFoOVCiCVINvCMAm0n+RVRWh5kf821VJ9loQ7rh0OSkCJkl3MxrhD585L6HWVmf2cHTWmxm2IakqEzWcb72Xu0jxXGMdbcZv97JRIcUW56ApSOeeFetQgRhHGCGkd3FJdgCaFtmkrYLpCNs6HrZ7CRZtarBp0LNchJeXmlUAzs6g7dSjKkXSNpaIFZxI8kvavScF9HiEvr3/IrI3dF+XqzJbPVFWtsHp+1kzSsNMFeVZa1ZjRgFmbDRQAVq8OyY01ks5nWaNUG4VZoOajSlCw+ZIrc1MWRG4yVWNlAg56zCLRgFySA2xc10wJCQNSGrFabVTbspD/QnhPg5ScOhuw2lxgc0Gg9RNRM2GtERilHOSQbklYXVyCuSCVohxDca6UuaA0ICV7NgJShGmSwdsoQSiFUfLBBzRClJXNrv7Rw7WsEhwzuNTJUAlfIHuR+VECiSlLfZ3lC26AwqQubm6v0ohOKnaCrenV8iwKLATxvEEEOeNTCg77jMNhwuEwYcpZD+kW59xWq7UvjIuLDdbrNd5882uwXq9xcbHGer3CMAzIXOTgbwZyYWSGW8UBlWAP5nUASjxDn1o9l4LXmxfUQ058FGTdJU3NW54qXHDtFLOKIxrqIqfKgdu5KHfPw0DSRToDB0ZYvLYAGXI2TPvDTHfdeCoyJNbWmkcqBTwMARR1vzGClPZv9CIg7TB1HGOaDv7dhE0zhzeNRCRvrOPDYOQsYBCBzKQLhhxBQPMuMhUVgM2qsRQW61Lr3ziETX9Gna8CpTICzADnDGMCajjCCFCVTAAFT5fCa/rZfmZbA/mXohQTy0761BgTiR+ZjvpJodGBgahPZF4yu6r5lHDSs7J1U1wsAgUHB6clHF2qeSYtyhED6zQ6Y5QLIRfCarXxtGkAiAq2hw2GvEFJF6A0gJf0zgvhPQ5SwsGO4xqrVQJWa/EQEc4OFU4y8UtStQeDVGXHXPScgXAQxZ4B8FVRCChAGopzG8WAKo8BpMLyVcIiQCQEnWCSmVD2kifdX6iHUEHz5cO66CRSUlCt74dU41nFiUvwrFAB0IlCI5m0+ckelMozRYjL7rDHbnfAfrfHdJiQc1bQluxXq5UTvmm/x3q9xiqtMW0O4DyBywVWqxEYAtFSTi0Hs2YOJq0m1TUqB+i+Ya19Nx/MM4Duw6SWRMRma090zKYSeC3JjUB0npG5mCLU7RCIqquIRUooSAldILCmISxhf8Pc+7BKLGIWX/SJElrlvn2vLAKds/OtajGClBleGEhF/4bk5Yr0Yq6s5ISAMVFiEt+AlKbJarqfqLrsaoKNN6qxQTNgXv3qIkji6W4sw1VMM2yIoGdTRaU7U9PNhIHFDX6dJRzn1JF9sK7usZ3zKIuo6iDFHieyLQjMRpPMpR6vr01uChFOSMw+u+3gsv5LNROd33XedzJXC1LKDKYE8Vji/v4IoMHLSYNsouUygDFgxaPsh58A1Bje2yDFKwAjLi+e4vXX1sD6DQBbMFeP6QI+JIdo1fCASoGpsohFvVRKRuZSVQusBnLO2ArITVF1Ern85syAgphKXzmLWjC6iSkqkUTvAk3TVL2Yc9ZzE0k3n9t4S0cNChWXDKPYbdKeC2J+CFWBgQhjWiHnjMPhgEPZY3/Y4/nVc9ze3uHu9g7TVtwdlZyRKCHRgM2mSlK7ccRqGDHt9qL2u9zgtdef4vLyEm984HWpSwJyydjnPba7nddPtn5F8kwEDMk2oI1YKhOhjMQwVE8XdmZsGAbv09Uo9Upkow0AGXYuTmGq5gE4ARvUeEOELlIus8ZseNo01KsGTBWnwJBYOOfs+5gM26y3esuYqCSVAJjKUTkQc81ELnW0nD2VcEhYxduoxq29VDn4pTAOg0pNxQ+O1hDcDWj5pSvXcu6ljz6uhahWMqmcda+COSlwG2gtg4W9N+YxSpPGPPqE1x6I/eita9piMSPOxDNuc8Mbq78DZ8TzwBiauYvZoQjPQUF6ieuxli2pWjCrhlm2MxhUx0fAirSsOQC38Zf6W5pWx4II6oUHQGLwIYOGEYCo+Z3hTAKMhIQxj6BhjTE4ALgvvLdBCqSrWnSylAYAao1mr5GQWKxObAGLA1o7rKsnGYssDgoiaGIgse7bkOwzpABSYpoMyKQoPtEMpHisxFXyrhOoTKUDqZYLdVcpucAOIOdcfdr5ultQR4ghcgEVVTkEaYQAcNa4g5p7BM5SbPeE17/bbrHbbrHb79y9kfkHq8RV2urEc8rIhbHf3qFMBzBnjDphLy5X6u9PzayJogFRXSbGXXJU90UVXCTU9XvUp0sfTV4v23ephyTNG1tYcIG2iMWigqTSZ9t/iBIXPL4t4rDwgnWWx06tix2tfM0H1ROIGyV4H5H+T10xFKZecZCqfVQ/RUJti1cWHWaWLBaWNh9tbkauWghmIlMHth1hHH8LUnVsLNjY2t6HAU6tr+yh5ri2vMsMmCpAJPNVF9rOLm7E9GlGpFPYP7Oi3F+fAnB1ZAuEYjSKEH+brkzRgJt9TqeQrjGoIDh4xHlZ34eWcDRRLzCjKPN6YfnU8euDaW/adTSXcgNDER5V6VylwsAc+GqNqjzWemoK82LVb5AcC+9tkHKOx/YMku5JjfU1yYQZnCtTDxRQHbsyH9bR1A1q4upOpjBjiCDlacyKMDwv3EpazMot62DmClJ1kYZJoY5ZS5aFxkRIAaSMELnzyoZDFTUjcQp5VomtQPbiBrIN2Gq+zUV9bTBwu91ie3eH3W6HaTr4npEDDKsnuKBTEaeqCfvtHfI4opQKUrsnGwyrEWml3r2VYLtnGiUQtiFcgTuAbFhJjWUjy+RvJFOOACULs6ow2gVCJKayyf5SAg1ipi9Xu3TaGzehkvqmWpyuQF3Rpp6jKDUlz4yLqWY1v8YPXfK6sbLeds4sjlltSwKbu6lA8BtfgGSmyyGUoIJiDnsxJr3OCR0zi3WmqtXMrVfvpss+s/rNbEEKDgBLez9GfKecW+MJrmpGKmL+zNQ3ykDK+jK2QdmBLo3Pfy3LrRNdcmql2Ga9lhaYZLunSkBWrjGskd/iMH8JaPwytvXzVRCe2SGVoBdQgFoWPsOWBuKEba876dcHN+/gfC/BFPAmSSt0+d45kDjSNz1icB4+AXivgxRVSzCCmRAngKoZOaWI3KGrOQnXWOoVHf2RROMojaTZBrCrTGzWsFm+xf2sak3mU043mcFKnLi4uxlTe7RcoHKqyp2KsYJJbVputj21uGBE4iklu1fpUrLvA6X9AVwYo/s5N4LJOOz3OBTG3W6Pm7s7bG/vMO0OKAbOzK4mJJUyrXeljQWFCIeSMQ2E6XAH8ITpsMO4ImwuNrh8+gSUgHEYsbnYNIteVDQMM/N36RdVGnWHmN5kctWmHTiU9w1qqP48qeTRuShiRi5ZzvtAAJQydSs9xK84ox6izbs4+bUGlJL0kTJKSeeUMFKaoxEzA0I2AsCoXtBTuHJF+9qJHMEOqZHnI/PMgCqZPzgoExa9+sKT++qws2txXvVAZfM1cSS4wizGOPY3DnaeL5QUHfFyINY+uFLjMadZ+bF+7MxINcU3YJ6IEBk0z7oLDgBUV8Q41j3naMXfAKvV26VmE7sb1ipUewgVMLJuUligKTY2M6Fbaxfmfm2AjrON2bJM5gfpwzG0GucIeMSVFHJCAXAAQJTUeGcF5hEZe4/NGIWRJ6pnygjdOcfj4b0NUgZQbsHGzfg1A0yAUQH5agR2AOmeVTIuAJVjN5ASDYzxKwZS8E9iMfKGPhKQktF3T8tOcCGApdaF2QGrOz+jWZmtRIp6fZ3Mvk8VOVSVpCpIyR6QGELIXhIXxqBObp3jK0XO3JDcETVNYvRRsh1QRQWlWKaxVdCFwWZ6KyLAdNhjPxD2ux0omcn64PfTeL2NOwYDqob1fURbcOFZXFCk40uUVCKAE0wuyqwwO6dfJ0W/7OTfChTxXWki+hwpBUU91ROReMpntboyjp2kb3qP2kY8Ew+V7zEu1dJouka67Nse6uMPpMOUoQiGNE2b58YMpvqy+i1KDovgdRyk3EAkzlM1QY6ahF7dB9QDxU2IIGrTsaQqDZlBh9/BFqT9gppjmMKs0kxS6SQl8vHgGNc6bAZWWlvFkeKJIkgB1YrGAFTGi9zSuLaW+m/KYDVgIuiK4s6wUenVLGqV9uqB4gpmy+rRvu9tlWjbHaADw9tXD6zKpbizdqKMEN7jIGU2+wxKRSWrjKUzEUyVG7cDroQBSCPASdSBxkWG0Gw0BxVOy6MUJTLtIlalQo0ZK2RWfszKjbLvOdXCK0fm9/44FdNyVPpqCYgYFxSVQBhyvxOrd2LzNCFdxb5HxcwYhhV2d3tMClLTpLZoLN3qV3nY/TnWlrBPxGApg5TT2h/AAG5ub1DAWG0usE6EYRgxDmuvd1ELSwd4HeO4n4KgLuklIecmrP+VSJgE6sQQEK/sjZUdYUgDTF8fSKnzu5bWaI/VcCoFPGWMBlKDnAExY4perdSYgOth8cGs20O7UhIiaxJay+XqvHLaZZW2+UZKL7ktsxSwX4qJuVRFsd9b8Oj3lHojDvOKbyECTr2ptp3jPTDNyyFRDyP6MIB7hc+mfVgQjzwvcKMu5BzGVNOaZO0GFKXI+UBht0BD4H6TgadqOLh6qvHlUYo6cI2SofVb7HcZW5nPgW4F7yVL7bJ4HIadLG3U3C1KKxTmb2cEZOnqx9FgHi1klct6rx5kppDRBILcjp6LWAdHB8r3hfc8SNm9RAI+jPbqCeVGVZphMuMIGxbyQ5ISuduTskv6rDSGcuJxggGgQQlriKcLwItyOq6csLkgBmBiSkvMdCNTNygT1MorlCNpl2aULC27D6aAkcbJr8kYktyhxYciAJUrMRp4wIv0QkzNpywEPpuQWs1ymSoxLwFSrE31N/ke1zQV7A8Z2/0etB6BEViloQJcIT9f5Jn4RObar5Z/z9mTyrkOdBIlDXYVgsQrzKCUMTSDW6q6lRiuQiNVwxkDs7CwXMoh0vGS9uTGsEYmgwGchUSD+DUsU2iuxLMDj349inLeAmuB+lC4mFHLq1JVxW6/92hpnlk9m243ILTzZ23bU+r6X2tm6rvWo4cyhv1h2bBWLJ+6H+Y1tBxqyiLusUh8QbWdGvrRznElLwByMaSmqP1m7oSKehdn8KrUyyt9r7ACEXHwRO6qeADKqLg2ZHHO1BqYZGSKxmYO59DH2rD46ecMieDuwtSYQju87e8wVly7xPnvaN3cEj+Vu+wfZj+ErZNLxgyTKmaqui/RBLlhoiBxARVWTUPLvB0L722QYhto1bdTPcSqEQDIsBeK5saqN1bLQMRF4InDHpODkf5D0cafDO/aetUVJo985ffytxINnquiXF2oBMJuu7S9jcjy2/Q2JaGpQopKVGkcReoqjDGNICRkmoDMDnSiAhwx0IDpkDFl1qvh1RQ/TEYT8St5qBy4lV+lGoFNOV81YX+YsM4FI8NvTBbVTO1xVoajjvM8VKagA+5gph5NsZ0AlgJOst/jAkaeUA57wOeJbDr5WStmwK5XOQFUpgyW+VXVhpVLb5MPJNtIcejd4CHp/V3MYYqJlDkEYKkj0BLreN2D3fOVwl5aBJAqQHXAEkJ/ELivr/KMNvvqe1MHRdcLNXEtKUhU/tl3tYGlGjKReFHGPKLOCzsHGetq16f40pV/S1ATWp2KX01R8zXzfM5FDTfkVmP1iiW5mfTLUSswq6HU0UmC1obExVavXqXuE8y2/R5z1HItv7oAbP4bI6FvW/KlUyiqW/252TuaV+Jm3UluxTjaIEkRpgpQrPYDgPtFvS+cuXX1GB7DY3gMj+ExfOnDe1uSujcI9ziXKk0Uhou1yrt1EpCqTRqpiZaYNkndPPfMm2e8xBpWJUtXdwSmyMpm+OEirUvkhAhUr16AcJpEwMDiP4upqPkyuerSOWsSaZQZyFN2bszUo2IM0VXTGLeoc4+vwg87q5SnSVV3tn8jkTLL/lxJYtgarb0id+tFqwQo3VJCL5L/1X6AS2yaOBgjAIQBqYzwOUB6liYKvioVLUpS4VscyVhfomjh5g/b7gzZM9tZoqhQLfFeWSmJ4uHdOn9d8NcfROJ2iGqj3ZAjqvyaOqNKZHEcoM/t89T+AkE1CQtSmKfl+t1Ui/V6liAYWLmqYRBH3nVNxn6ph3briNj5nqZvtJZJ17vtXcpRtVblLzFNZtX9wqIlJHH1JZKUGnGAj/ZNVaLE+qn1abG+DfXWPjFJ0rYh49yWrjTPs5Zvm0fMx2KYMQ+r4RSx7eeFPmc7LiIdX8e9jqsbm3TPSPsqsRkBHZMu5+F9BVKl0nUJtjB8oobVaZ9kREFVRH4nhy5+wNMTxFHrLCxpJXySxDjtgBowsvn50r0tSWwTzeVvINkeWoLvfPihF11YFAgzA5QG37QG1DUOElAYKevkgVnBMfiQkDPh5mYHLiaSa/mm+3fdSeiqQORsWdgCKFyAnHGYMrDbi5uUN0Xt+OTiaQUpkjHkUbyDZM7uHJW7HWRmu47C2lXP4RQmyE2hpKoUdn0aMYNKBrKqr3Q8EjOwWqnVZSX0DFM3zDf1Z4Yb9hn6ikO/qJ5GPYZbWlF8lGAub9sMJSuQFmNG2OvRA6XNCqeYPkAtqBBVcAEBw0BIpCb0CkZ223MyM3p9Vk44qnUC2JQIX39paMG4riugHqkACHYYucZ2jxtRdTlYdwuILxldiCHQ/LoW9HnZe4ZfY2LnFxPPQaY9q8fNoXxjLHJZrlNfl96i1w+fO1C3PdrnZfWpPhjnlhZNbA6m7uG9gdRg9TWjCGcOBaQaDjS8r+004BIjJABqUFRQWI4HjTlhSAvn9Y6E9zZIUVICLz6hCCNAvYWcbJuaeYVwOZCFAAYr7NjyqvOPJP/gTIiNzsxECUtBS4+rCW1v5cXK03FI2yBsjwfk3J8x9R7brKdCfFZrqwoiJI4dFc1pkIODoyq285QxTXfuTJaYMFByZ7hmNm+r2S9NDHsNxrXXPlUzeTDSdAARcNgPSCCsxxWeXjxxK7icZHFOo5jTTlycMLrUZG3jYPnErGb3XMdHFyKD1SOGpBXLoow0VaJsYyFbTu0idWaAIW223g1ceMlZHKgqgFMRrpq56tNJmRShqcFfofWai7N6rgmtkZYcNidhnBwEu36OuUb8tLnNc5AqKkFkZLihg/1DxvQIFzxQqubztgcX+tDOK7GmT8YBWpkUDm6j9h+rVODrg6rHDODY4dZIaMkBuHmP1rw9Bl8PHUhBrzgRiWDhfFZfC67l2Nype1sLZxiNuJt1rO0H+5gpwxasDK09MqUZ0bOKTg5x/VV0fRtjpsuSY+FM4tsT5H1cb8ep/clpQNzLawzMQn4ys/RAMaOCVHBOSzQhEWMYRhQMGMcR4lB5keWfhfc2SEGQmRWs1JMUmhnuEo1xeuoMluribkAnzHTZwCTEQ6GnwD+qthZqin6QOfzbECp/L5wJyFQgcWu8A7V+/nS1FaCKVmGMlMRIIiWZBqUcMOWCaRIDB5OyCk9AMS4tcrkVnLwdKrhWtZqACUEOLZecMB0mJCKshhGXmwu9WwbIA6MQsB+BDLHIKpP6H+yIhXGsFuT6FBsDXTpmphx8J+Ys0pm5hHJwZQAj+RUXIpkZ0CiXrdK1MDP1BmieJgBTVbkF4DPJ1iwLpe451JttcAIFq+rJKJnVtqFz2hhAr/FYIe2zYxXczV9iAOrRJKMlfj23TUTYrNdqph8kETdAMSLWzWFqqxtr3bFsOp2qwVAt29oWkTe3aYHO5N/GyTj7+aKsAGuEVcZZ5nnXjjPAqhrHHAcpC+2llP3cJgyliNFTbqU2gPXcohnhcJWQoYfUdZ4Qd1sdZuxgbpQiLhbdHPCuo8bk327qrbMKLjnb0JlTBFkr1QOPXGLKSMOAhEFU/Oq67ZzwHgcp8l5mMtWEApW+hrmcQUJ1M2oOPqNljUkj98ugFRfIOco2QtB72wS0/aPIuTXlYgZSbopqUiD5OpK3MlPmqn4DZq7WWJOqLoYUps6QGtPakhnXV7e4vd1it9tjVYp4kMhZTX5ZRXQOfYa2fGcQAptWivDVDOGwOGNICeNqxGo9OuE7UEEmYBilbYkAHlXtUIT7i0JCT/BEBZirdpfFujHnyRkTAU0B4SaYitAksCzqXzNJZuZ6KaARPW3rOKwwjLU2iVEvJjTpTqVB8bw/OmNESsxiJybyEfY2uPsuVUHHA8EmqYqUpoyEShrCRlQTMC4ZptOyGmcri0awgXDoVyPmmaq62w8q57B/o5ku3f+UxmE2WfwwNBmYkzotNump7k/1wc6XUaoqsv4IB1huu11c0tKwNg1XZmQOHNx8dzVYULXJGJQ61piDUPUlaSBj3j1aMK7ntUIdLP+8mjmntn0sk2YA03qEddIBaUwrTqkD28yMAcfpIZsWyF8TQElUekgoU2UMEvYgMNKwAvIKBwygYVrMdym8t0GKzdY/nEdB6LfwwwUqHzHpYNNUVa56YVB0oTjv7YADtAXUcp2Zc0Gj5w4XkQXtmgmOTUgmX8NNIwAkLSzkWJ7+40dKGAAlJ+yAqPu22y0Oh4Oa1So31sqaTX4M1qa1hflmuFffTBngRJ4ISEO9OoJ0VyUFT692up9T0rGmWV28scwoel2pLlOkwrr3FJ5xAYahRTkmmU+2J6VnqxjFpatk/uc6T+ZpQHA2rEOh+ZBxmwp8/b6B+bRrgwAgqed+4khM9ZwUeUyXuGxD3UBKNG2MxoiXCCiNYjDMYwPUcKGjD2DLWceV4qDJdWrNiLxdqhiHTD8TAWYwMOgB8GRnv5plExiBWHcK64mC4QoBJkrMV7Wu/+aNMpdEILsA0rKNTphtTKE0gaDSSd0j5RSBaqnNaljB1cAiGqCYNGgGFBKE2eOkPj+5qgulXuQ+HmU+nAapEsqKEqC1sen5Zji1rlLD4K9RHfWCdC9Q8+cE8VeZMCD5+uYFmrUU3tsgpYNB6hqpsFzBURcwC7EgwA48RimEyA5Jaoi0gkMeVC1SCIYHCzOvr1vkAo9xhKd0hDCXbmqp1qBvkNDA/SxCa0eorm5g6iUDDSEeWaWK/X6HqxcvsNtuAcj+jeyfFOmnFE6VBYukhvAEblB+i4Wh7O5pueEA9jCmqqZRUB4TIRP8SvrYqqJrZ0nxWvG75V7bhSfgMvR7FQw9R2bdyR7fTSfYLnzMYAQjAvOUj5pWKbWXYWfUuONip+mAnKcaTwHcQV6Bst6/oxxvdF6s19sTBykKLd9Sp1n1OKGFNtyMSHq5xW+tW+nUZmY9ZyozQlWXulSiTEwkkE2/Ayo5CYsyab0DNmJ01WoHkkR+dsu+J8nMPZg7QxBQN9Y3hl7tb+7Wls6GNanCOMP2sBmNpHNMXSgH7ufxzOBjxv0G6Swaa7g3mLBPe6xMS5+5lfRKKcKgHhknU69bfxdVZ46jHIufcgZY+j6t1FIWAPEIB1hKGLLchDBnfZfDexykgEqsMVM9iTm2qfQoAE6dovGvDwU8m8h+i+2piRsIdVTvnUqzDGCWmflTiyhq7UBkh7XixgWFjU9rpUktqpfOJWM6HAAA+/0eN7c32O126j6JnZMl/3QzEm2mAZVNdOsk4+apSlRcYDepmjqPVE0LiM80IWoJKXLdWrhs9pLXaVYDrn0WrcOSOl3Vhmtcu64gAk3dm6jjUTlKtx7UyyxrEQYoYXiMAJjazwCqVCCC9r87WoWCqBF4sxCMgKWA58SX4Zd12j6IDYQTMCMWRAJQ4e6rSKRkfQxy8D3uc2jbozU2hTzJZoN1VTTh1laZZ36ff8JKixSWqkTTGhKIAcUBuRlp+IgYUMociYBVDWLg7bAH4oia69yjMG/CohKGjmo7G4bSQCWU1Qo9bo7dsXENI5dSuKoHkUbMD37bMxnegpSKC8XS1wq+xigcCQ5SnQUfM6unjKJzimdpYl/4/W7KP6U8ABA3cxTdIpUBzKIKTFzHaZm5n4f3AUiFYJyP/hTarcTZiJyuJKLW8so5N6dZLUCZBcypfu1NU2eGEI0oP39+PE9bYP5U38X62MyXr1U9WeEkLkLf71DuCQAOhwN22y2m/cFXBxmQoBqcVIkycn/sxYPhAOdQw/XP66AUykCKihkMKJeMVrht9hCo38Ttu6jvmUCoIxhYYJX3lqzBTIXE7AszGhYYIeq9W1j/SnEKUHa1iv47NiohkZp6gGNNDwUyzlVFy4CAFovVol9ln9u7tbztgdkRTUMPZgzo2TohRqq2KVmPQHiHqRBfJRJWUDHVq6nACCpJ+rEK7VZOzqwYI8MoUUiHaUNkfYYxKa0sLXUtPla2bsyfYlQS52TxIVI9kXriqHu/Pq5WN6puhJZCu7eltIYW9otDXN9vtvXjy5wgknoLUhaNYSBRAU/AWIm/HXE8JUkBfsVOBCkDn8L9/V/2vTIBth70ikyRYlkuPfTKAuL4twwAJyTuPfrfH97bIKUcpt0B1JuhEtgtXeT3vFuMKKqQ1T73b3CiSgUvFfzwXCSe94CVc2ixLhSdNcVzXOGix5LDAq7TwZZqSmrEkIHpcMDNzQ0A4Ob6Bs+++Azbuy0Icr+SafhNzZcss5B7FNtddaAExZz5IunldTJgopoZxNLHPKGT+k6kZKezVCVJdfwMqJYWoRC8+bmY2O/kbVKHmFzTiuStRKNUYwa/dVfL5D5/3R9Izo2jSj5mvs9a367OhfV+HQ57V0Y0cgAsrlaOjdk35DyVSGkFrXVZ3Rcz7iFezinc9CTOdq2uJkFQnXMSL8tN06UdfN+rMSDU9ShgoTcLcEFxYxE7+QqQGtO4T0QGiOpxh8rVwDo0lB36H5XwMsuapyzq/cxDUz+L7/1tY0/GPCnIJcI4ruCGKi26Wi6NBOZ/ME/3lTlEKCsyTPY5W/tqpdxI586VkkuasuwNNAAw6fGHZXVfLCuVubQ0DMMyE4c692L8Ugoyy+F8LgS5cFaphZHOcgDxgAlPMeanKMMTrMbbs90dvbdBKiykZs+oFSo8RC7JM7BvpJJBkFYiSFGYbwvM0b2hJy7nht5iqbahtq5pI0fgsrcUGl//CHJH1Xa7AwDstjvsdzuUnPVMDKt6Tcm60wyGqWJmFbB62OOq15OlSfADojPVTOCC2XNQ+VcHtonftdsBbEGKbbok9GdFXKo/wySKkltsXaMGTkm40oaLV7Yg6byCAZX1Tq1BggGVAr7FGeS7eIYfKoBRqBcDxYw8spo9Mxopqjky0BGlnA9i0szV7L74OtDySwHljJIyijk81e4yel3UE7iY3ZtVqfaDGx0E024DOB8CzddOkKSgzOUFScqBF41xSgU2YUDqhXzUpOVYJpP7aHQ1ZgF4mnwqNEdIgn/QKnFGkEpuJSlxKjn2fa5AV5hD3k0xJ4gNR9CO66aun4ahCKEHxwhGPncpzQAMaNeVqByTq8mF/1F1X2h/SqM2ckTGqOek+u2L4+G9DVKwCWJcEBAJTyRB/mAxE8BE+hjUCDZwMAZmy1ktcS5LKr5jasEljqqPby1ann7wyWpqASh0c4J6NZBFZF11OEy4vhJJ6vb6BtvtFiVnjMNQpSZOSMrZGkAhqIx8vw91gTTQmhJoqJM+mUdVu4Yi7FPEs0Y1b8PXFqAarjBFQrDUv6F+QZKovSqg4qomyyPVQ+AAqhdoP2QSiBRqOp+PPFSQqrkgJAZg9hotEXacsHpy5XBn+wWFfS9oTlxqybGfmBllOri7KgcptJvzpRRM0xQOLYegKknzDELcEnRWCS2CigGlG66FWwSSXsRZwR4A58X1FvtKbpQ2qa3eYF1yJbBNEj+OEoi7QQcB4ILDbu/vA9/iz5KloX4/jNVDvPVVCmvc+Dby72jyrurTiPN9MKMFS+H1tO1GKbYyNie7r64H0Wqo2hno0vIsHbNcnsrQvi4mSdX4CWsQExKvwLTCqujN6UdAtA/veZCKYZF71k8mBPGyLgKjJXZleSQjHtNmqHJfaWZ1Bn/nP92FUJhldn6hrXTzrtkH8zzrnsjRWdu02Tg58tikDo7kVmdVjzLjsNvj6sULAMD29k4s/ZgxpIRU1GdcUjdMEZy4KbCpN4FQkF19MluNBLmM0aSrpg9j11RAip4K4udszONvPwBruVTwmHWljrFuE7Sn8anz8EFw7w3WF2lI/RSo+bJCrR3Gblzd2LNQQMSzBFBzP5pephjKMSmmByQPQRUXVeLMDBr0LjUG7DCvnxUDEG+PnvltBLtZ/TRNDpbkacn3y+z8VoneNsyDeI4gxlVC83VTz3nVIZPKCG4XsFr3yoF38rFovdhbMmW0msZw+y8JY2X0oWG6+u6FaSfrgWuesq50owGWWEoYAmh5q6yOVBmfqGZs9pM1nl/jQsklaAStEhQ029DmU5nKClY825MKqUNzRM2qYDwOOlb2Z/Nukv3HvAanFda8wTiOC9a7y+F9BVLAsoQjz9s3kft1+tnFqhx9nRQzNZCGaqhgEyVSwQh9yzWskzVyLyE+W5nsC/BYS489sdyCYIicM/Y7Ufft93v1Vxa4R1kxovoDYLfuNiDeFXuvNKiLKM72eZpATLEAVN6GbhysLFOLLfUHRYmpTesF2XMKXU3hQdscK65rwdIvTUyVLDT5t0Ou3HS1xrPyPTdmV7d4lZf6VdOk8J65+JUSEm1o1ISRwxafj/VWYY+j+xpDuFTTqsqsB42L3Xcl0prnmcWkX+5oKZWz9g6p+2dNX7IBFPtwsB4YZ3CQ0MKh5yiNNhyH9VHw2WmZDtVHZg9UPchxyEuAXmXjEI3DaNsBagr9Gc2EHKSi9iB8r5KbHpKgbg1RzWeRJsQ8XaLUeha5lDDGWwrUlEkgGkA0IvrtAwCUESgFg/43joMeF3i8quMxPIbH8Bgew3s8vHKQ+uQnP4lv+ZZvweuvv46v//qvx3d+53fip3/6p5s43/Zt39ZuNBLhz/7ZP/sOS2b739k4E397Ht25EFSpYiZpAcbGOgNdufd56PecFs9shO+nQowPtBzXLO7RB9R+hroYd8ZF1DS73Q673Q77/d4PYyqvKqoAV0P0HcDhT590ElHTZsK8LxZUCou+LM/TDISylmSoh4UlYVX6xv7rW4+5ZBZfUPz0Ckud3b6//lEyiUvS+O/UxpG9iCRXIegnqWl/GpRrHZLu/6Vahu4NWjxK+kcJlAZJO4wYhhHDOGqckO8wIo3yN4wrDOMK42qNcbXyv9V67X9j+FttNlhtLrDeXGC92WB9cYnNxSXWFxfyt9lgvdlgtdlg1LxjWWkYkYZRji+kBKTBv9Mw+B/SIJJo3+ahtp1C2yXPATQk7UvrE8tH/8IzCs9J06Q0glLXZ9q3MqYJTPXWavZPlWxdBSuaUvlj5FyQc8FhklsFDoesvjbl+ZQzcinyx1y/l4JczJ+lWJVKmazbisGbO+Dtmv8ZHZHpm1K7WOucb+khqXNiIuuHc1aghFeu7vvxH/9xfPd3fze+5Vu+BdM04S//5b+M3/W7fhf+23/7b3j69KnH+1N/6k/hb/7Nv+m/nzx58g5KrSSVbBMZmKlk7Euj6gvRUiBsTWf78/B0oZdb0X3+7L4NTItfzyuoRaDru5cUXHPcYLdUIndMYG1mhjh0LUDOB+z3B1xfXwMApv2kp8ptI77q7r2flFi6f7CFyWYGD72KT/RNlZCaF4y+z2YqsPmg3NuHYHXXElRWTQGnMqP6dmlMPXWo10ytFDKr/UCoauCQf1JV0ILKsJbWqmSCvi9YprV1XjQSiWUwg8xQgQkZ2dePtW8INSqdyTKhzus0ljBfgnatMOo+JsNvHWCWm2zN0MFvGNF1UsyzfcG0n9wVlaU1glrKJPtixCA3sJC5SdAkLL7e+2BbNX61fIGmY58AyVSDzbBWt1Fm6FONY1j2EXXu+XEGaZmcb+Owblx1KTUxegPLjWO8oGbVVqaUVXWWFYcJGKolcb2Mps6LqCr0hts6pwoqDf30fKrT3MoPNg4GKzNq5RVhCJK4l8XICtZnrudXDlI/+qM/2vz+h//wH+Lrv/7r8ZM/+ZP41m/9Vn/+5MkTfPjDH36HpfHsp0tS9ttOmAOVaw2Bur/4/Mw+nIVeEooWfL2+/5z05sxTJqeCQ0csKuGx5nMlnmTnfyS2WeDt9ntst1vsdE9qOkxibkysV36Lm6lQMa8HKTdocWvx1QTe9M62iO1uq5QSxkFMUVvdeGvQIi1MzVgYUM6ggPxtfXCkj+Oe2rG+j79lL7pFyqa8mISVYBkTxKnGYfuooMP+WwDs+LQIFpRNoXVPgcMvN3sWJIFZI5okbf0gdbJ5pmAWTIM5VSIHY5hghBmVYKUqDlg9fWsWrPMQiB7g2a5GKfHgar2iwjbvx7W8jF6zBaAySp6QG8/18L4EMziLxZ+bO3O7BqubqizNVifApIf5FbfkvjEPYvebupGIVoTMXA8V2mF1qIENM+pFhxwAQBkBc0TMQGXj6rEMxy0ZIHBRiUw9jtCgY+pz0LOQskz69jxZzkQSISnz6FK5AY0CDxc7rpB9naZh8Glm9KfZFzUpDOoFnVdKk88Tp951w4nnz58DAD74wQ82z//RP/pH+OEf/mF8+MMfxnd8x3fgr/7Vv3pUmjJ1lIUXao3WBF0MXGe7E4YzBBgPDiZYYuAj93EibQClVxFcy2bN4rasWjvInr8xRgQgToaOkOdpwqR/8js74XLOzQvVtrOBSMgztNWWXlR3xnfO4TmABXWoLVZbXFpfHwsTVmg+vZeYj4eG5TE7PuaL8Ql+wFUkuoCoDaY5WYL5ZZQxW5qsJhHFuRlnZ2QSwjvLtCK+crhGNLkOGKDSBreOP+MZnyiGOa5y/dT6V+nNGChx8itlBRP4ZCBlBhvaHwoehcVFzzAaYxZAyuKUKXipdx/tOo8ZPNnZseps1Q5f18PPBaUkdV3FaujB9QZk7g7VsnvTjA8bAAQ4dl1ISt5XBhDxvqyUi9ztVajpS0OByo7U8TEGNGv5lGUcIvPh0QmgEqQkVV+PSdRxydXHhGGoc1wcJiR1bKsXQpqwRKK+E1sRRaowr1qDdFvvpz14xPCuglQpBX/+z/95/Obf/Jvxq37Vr/Lnf/gP/2F84zd+Iz760Y/ip37qp/AX/+JfxE//9E/jn//zf76Yzyc/+Ul8//d//8MrYAtp8Z0s3o5+vCuh36/qLafuSwPjbsOEqa+4xoG2IwFmhScuhwicGaZjYBAKM3a7HQ6Hgy/uXPTeJVUTmBQU7+5xQNE9FAbpYlS5R706Ry7M6mnAlFJyTxPVq4V6UCbCkMgttJoTV0TdUYJ5OGfim2PhedrzuZlj6l5Zfg8J1UTYVLt+B5MSyaKcLhjuiTxC/3G5cB5aSPOKS3mUhEin2uveTge7QDRDWmMxTDVteNtLoAjXhpB9dZdPtZZ+5UUEiDBo9XrzGi+rRGZ5marNv2vecpFmcG3F6ojW3FDZvoylL3qvmdbQ74oKFxNmVz/KsQ3RGCwQH0tjgMoskob2ZRr0DHfnqRzoDhRbG1WVWErBsBrrcBgQLmht6qWI7ICTS7g6g6V/01DvTDPm0s7DoWSMo+x1xvNgidYgjBjHytAQ70E4YOI9Cg5yLUjwxH9feFdB6ru/+7vxX//rf8W/+3f/rnn+p//0n/bvv/pX/2p85CMfwe/4Hb8Dn/70p/FN3/RNs3w+8YlP4OMf/7j/fvHiBd56660mjnGQURcKoOppPWJdDMwcXP7PJZNGjeXz7TgJeiiRu0/iaoh8x7fhRL1RjJ64OCVtJuGWEyUwic++aZoaj8dDShgIfvUCtI5uiM+1L5hM8lkifQpagchZHYekm/lUQQsAJuNQFRwTkZ7rUuLsIiWOkGV2SQ+I48Xt5wKS9CqKvh1Omx2IAgFyIaJKXRFEQP3coIVvln8oNUo/3Fbbz3GFmeF7gfaEY6tp4WqEWenyv/nrs3YC9fbWpo90jNufcEm4KebIPDfwI6q+IG0dc3DpRKHRgJy7gRB4mBRW4lk1VnWjXblSyytDvNnWzMTj7/A9gpTWz/bKHAQUpOyToHt5VEHNi9c9KZtHLQ2og2/uuSIxt+UcKYHtfYmj3MjS6VqIE8nLUGmnMIiK391lhZgUikQNDZRxErYJlJD1apu6RgrA4lh2CmcUE01IyDiUCVnBKrNvDNwb3jWQ+p7v+R78q3/1r/Bv/+2/xS/4Bb/gZNyPfexjAICf+ZmfWQSpzWaDzWZzNL3rgnUA02Asmq/yZaBi1vMKEhq7fVeVyODLmAcZ1qM9XAZbSnMS4JxwwBdKSy+C8sfnGuvZCTIKKn2jwFAyuSRVPWozxmHAANbbaNknvm/C+3pKLe2hWi4phSSYn0DtvyKHTsdB9qYEEEViY4iqQ858CNEqkJtCOaheZlJDQzNJjQjIwYpCvTyaK00C0Yt93IwJhTZJx9dLFAwcUFV8/cIzTLT+mwVuUsk+SPS3F8FJJNyo0m4IFkId2OaHEkMHs/vna0rqHaQpv5UKrG9ijgyOP9AkoXblxPM1UCbIOrmZUgYWaNWQEkctFs1gY6hSmhYCkxii5qLZF47zoosDPWjMpYgfRQ1yILkSdXlWJSmbI6KKLM0Fm5xEZTagnZN14eqH7deFPs/mgLm5DVnSJkABreZjYzMzYArtk6vk7Rybtb31xuFJmeXuNE4oVHRvyoxI5OqPPB3AZcIw1DUzpgMSTTjkA6ayx75sccilBcAT4ZWDFDPje7/3e/H//r//L/7Nv/k3+MW/+Bffm+a//Jf/AgD4yEc+8sDSHg4Qsxy4bvSL0UAdeLnMqQB6JfNswL9kIYDjrPj76lM5lmjViIKZJCWclXLiJh1xAqhUUkp1A9qtaWMdlMARcXX6W3l7EIkqZBwGcb2kJtIAkJCQmOE2SRTy5yqZMZvj2YWuqN53O108tYTxAcxGq65Fp8aKhS+kXcinl6o6vxvqlJeUsHNfVWnLUlmM4GKqMjNWj5kgtJRHlPC7PlyyWjVCONMKENDceUU064vI9By9/45IiW/XT5GfUDPoZOAT0orlGnyyEEyFuMDHc22LSWOD7l8hSAwurpGZHrAaXKjUpITegWuqAJdN1RivYSnGMNX7zHKeUFhMzr16JWsdq4GNqCY1n2S2mPbSuITArCNIXMHIJV4DEw1AYlca08BgNSTRP70qPhVGGhO4tFd1pCTS2kAblLIGDiPYpbD7wysHqe/+7u/Gj/zIj+Bf/st/iddffx2f+cxnAABvvvkmLi8v8elPfxo/8iM/gt/7e38vPvShD+Gnfuqn8H3f93341m/9VnzzN3/zg8s73cyWM2vSOSdaf7fqNcDlaxvKSislbccdPxTAYvrTRDKWSy2HirZOfTDpY8nicJqmqmPWjEy1F8uKeRnhcrXXTHCoYBKtliwH0ndDsPyraamCElULP2ojeV1mXcZo6tv2URxfs5Oald7GX5g5tc29QcWJ8Tv65kjox3PWTp43vnvkVn6zl7Qw16I0ETn0M0BtoXpH63hPZovrVNPNVzL3A3ykXVJD2/djkDsDXirfXTLaP64JmC8yk1FhWge2u73MKEPBaqhph2IgBZhHelYTQlMhgoGUxadiiiDF0UO8zNHo9d6ZaLb7vEL9rL7k5iDixNfaadKoEZvaCdAhiKsjqBHFvJ/BctVOGcRbBTvVREqjXmw5IuVR7p06wmgthVcOUn/v7/09AMC3fdu3Nc9/6Id+CH/sj/0xrNdrfOpTn8IP/MAP4ObmBm+99Ra+67u+C3/lr/yVhxdmPXeEMDXxlkJQAxwHCduE1X9SVSm8aiu+c0LLRZ87zDWtpJIzKHd3N9jv7sB6MywxMKh2E1x0k7769ZpNq4Wm2x5UdL9DOkbC9Ynn5GEcMI5jgysOjYHrrkBFdRyF3V1oYDvU/bUirs6J/wa1GXFdXD1ALQFW0w3ncg1o51pkdHlhZNNSuUTqdqcGO+O3NDes7gUnlkKsT/gex+bUTPcxi33cik5z7I1MHrW1b0Kie/s0llPjagt6+nDEGTF1n8xc1XJRQtM4dWzqnm4KElo1c4/Mj13dYl7AWY9RmZotGigU8GSlVmMSo1tiLFLvKYNKiNkcBZdQnmQh0hVqPlZPmMSu+TRXpVi/Nv1jtIExCubIgWPWCw7z1ssdSfel+AlSvkQeLpHGNZDOg593Rd13Krz11lv48R//8VdcXlhOFAnKvC7H6rcIODNucC45RZB7GalqWQXU5xHOWQGBGAXJ4Ei7TDoxE19TDYjPvj2mw9SJ3dQsAtmcNoJYvU8Y17nYVgKIqt7c/goBwzBgXK0wDHIavyFehmZqFRWJf5Wc5gYnUSru1VDHQgVBjc/13Ah147zYxqXMXiJIk4LvPQfJ5bJdHce1nfa8qUaofiS+M0CFTvMY2U3ow/MlyehYexYCd/WbM4fLMEgWtyl7iUGxiXKaceznztG4Gs/X26x/40qkuh6jd3yV9mIRDGGE7EJIWbvQcS+OFaRqc04VpPwlWD1GMEa3MGQHGLv80iS5eEZMijCJzRgK25+r6kr5i2ckI29YzewFoIvjH9MA5gGlVP99THJtPCgBepCXib58e1Jf0hDHr6FmC1HP5cQWQzRMmIPSOZZ695ZwLqiFbw1vd6R5pEQn6plzKch5wn63Qz4c3ILIt8GZ1VGobNwaWFXGyr5YikBUFWhsP6peFS1x0zBgXK8wrka/7LDuLRlxNGCzx9E66zRD4GezjPDWgXNqQUuThCEWYwbO70RCbvWzszrOCm76dik7aj6B7moSVLCz+vd7QCDzucCzps/La410IuHv29Fv/i/F44W49/VvHW+ejffpus/zva/cRUbR59G8L61/fSm4bFEBK+njpVYG+1A4yDEA6B4WM1JR45UFCi2yUAUTq3fJud7+zMHNUZSYSo1Pob3xkkxT98WD115uiCfgV8BZJbcEMEaRpFLyhV2QIC6kCGLcNIBBaHM/Ht7bIBUUN7YZeq6PNwtu6vpAohQJ2ZdC7bfER5JO7sC8yXPjdhbySID4+9rtsb29wX638zMbBQBoBLiAuMBO7VcOTmLZYoSqLfza3liOAU14RilhHEes12s50zEOCJoQ5cSWl7V/M3ZugXDNCGj70hcNz9+GSlagro8XzP/vI5rnzocoSLDOxa40Mz5xZQFzrJ6r2VqJFLUNZPuHlXtv2kGpyyvkE8F9gVjPVXbztlmePdj0BiSLgQhirBPHhMI1LGdIdyc9eSzFs8j1nFA7H/pq+4qAMQF+L1pXbq2z9SmDq1WWaCBKQW9pCstb12J182TWdaSeKhBApNOwGKjF35aLApwbfhSG35isZTOj2QMzWkAsBh25MIARzANyHl0SGzCA+IBcLsH5EgNtgGHo9AXHw3scpNpgE6UnzxQW2lI4BjA+X6lOwroeT0tP5xhE3BusLLS0zJ7V9i4YKPBsfbiUwTljmiY5yDtNrg6IG8GOBbFUhoMWh1o0/SIFde3Q/ksiAYzj6BJWdCY7n7QdOHXtXwre3zNUoYZ4L5mKx/r2OviZUcVLj+ucWgozvjySgk19m1qjDTsAbGM+myveJ8sSgRE5S1S7ab5uesbsvr54mW6a9xDNx8SbdE4BNq/vKZcrLtdy7EHHhS3Wr34nHY+luC24kv4v8hdRcc3DQg3lr0RVZDhmkcwpbZWAZ9ahR5iEXgqDS0oW18CJG5ASWiPxhyKSFDDIBYha8KC+Ew/TGpjWGLEW/6FfjSAFSMecwZ89KD8n1fzO83toiACF8P0MxrDhsBPkorUhJRz2e2xv7nB3cyt3SZm1EISzk4lvvgINhGxjFtWyR7m/JR7AzPnd0IOAFa0wjCPWFxukcYTdettWermHI5k5pnqCtnf5XXXD8hD10bsTevbaPskljig5RZNoGaPqf08jQNoX9tPCmDR5xeIDaDXC0AO65ty+rELZ8X1cj9tVoT+jRUBjTLG4P3kEXJfCUppZve7plyj3mLrZQeueELNOlJDG4z5VZJ8qeoCJS4ZcuQGgPQIQ0nsIVoJJz6Fyo86LgGbupWqaepbPDEcI4BXAA5h33qpEFyA+YL/7AA7TG0i717Fab5CGL5PhxPs9LDJHX4IQwWlpvVSzYZ7F66Ub9vR9LsurUIXJ5vW5/eDceM+dnlVyF/p9lnlhJzj/uRQwq+dS/RYqdnJf5tx4zasFLn3hUUO44z5Kx/7X+JG4h1wM5L1eoduCyuA+o56euLd9uNS+OTgdmws1hcnsPSgvpz3LKMKzOQOcQv2XeaKK7v56gWk7tlpqeXMpcTlerO/i5Gwq0UvODZ1YkISPrxHyNeTvWJja+g6A3ozcWk3qb5d+oXX7KpWkAMzGjp2jCaLvsf6ZUf9XWbGvpMALc/y4FLOYNrLdxr51kZ0g2SQ90p9Ljx/I1H/JwlKdlqbNfFfw5drDCx2xlPdMVzV7/5IVWKrTOeWdLPdYD33pwpIkZxae3cMv7US0JfVys+XEr/PDYt/0zxSUGqJKzuPM84SJfQqEZ9bu/QNSVL0btCHKv3MO5EEY9GWjmtR8M04z8sMcJgo7AWi5L/vb3W1xe32D7XaLMk0Y/SoB2Y71e3bolKfi6mqd7AIrTXuMgV2NK2w2G1w+eYL1ZoNhvUIZhpalB6m5qrSQVL+4yN/2BfWSVtjwolQ3lZfVTCfUQfcspx5EGsGRukV9LISpyVpm075U3zWbHWTSVCgc5Bv37nNP81xyzW1Wzp6PelgBEAwUbFy7ijbMdidudw2MsERQC7XzFdft7yVJM2zyn20tG6U6X1FKbGMepd0xZSkw1IP1//NpSyMZn/ViVvsmspoxBNVhH72VjKipPrvmzo3JYpVYPeS7is8qGs+aiuk5l0HP7cnzkRgJK4AuQNMFCl0AKWE6Z+jxfgGprvNr4Nk6YvuXw6SkmPaINHEu1/gOw1Edf6fOi7AVD58e492j/4dpmrDfH9zbRHKQMjrDIZ3k4dL6kS6oXTjvQ4dP9X4+rlbuDolV9Hddd0Pt2y9Nrkf2pSLNaLg2tnacs4cSKeAZYz4j1velX5hj5h8y0N8KVJE1MSC3SByE2I6ZCcLrKQVCvw/VmG8E36O9CmdmoLSQd//WAbMBvOXYtsd2JKsjj+pgPFjdxwvzyQj2wjxrWt8OT1uno1NoztE1ar2lPo7ryysxH10Z/9NWqb1K1gzMnJ4001gNNQzMuMKh92MaAUoo6sTa0idSf6DDiMSjems/j4kA3gcgRSTcfzrZaF78flY3RY7qSwRUMQitXappe2p+cb+jIS56bmFi7LZy2eF0kKsN7LwSASC9kZdLRnPjpr134kJw6bXRacsrPwycEjKL1+Q0JKRxxGqzRhrlmu4Y5CprDo5CH9bXfs7rHYTlaXQa2I7ujRDQUPkT5emptCOYKH0N0tw4Xv/XxGrK7o0r7tsDMEGoGocJ0TrGnFQDFi9k1k9xzybwPgD0KpIjXdPviZzsf69PrfvLWF+6wcOxQrp3S/s3i0Y9zvhVKRlsfEkHQsFVWEzhv0l8XLqYpAZNcitZl/YeWhXcXNZPAsz8/ZTlMhe7lBFKBxJoGJAnsTakIFWnQS45HHgDHi6wpgu17jsvvOdBCqgSALFetGcvmABk5doiq1i5dvUr7APUn7OiED+G4vxmdcbZq9esjLNa4MxcBBv26VknDHV/qKJ6rXD8AYZY9ZE6ddzvDtjd7UEsXtGHIeSTDyBi0KCclZ2LYl9eOgn1aChV6z0Azf03zg2nhDQMGMYRq/UaF5cXSOMABjA198qoNWDXvNqUXp1rRKLtS9uY9R7ldqm3y74S0bomS+hb+Uvel8DSkDqhitVrB8WxpkvpH9Vysg2t0cCRvC1/lyJaIhPn46wIDgBJRuxbz+9LXHsvgXk5ni72eGt6LXMjOn1yZOz62OrVSxxWXu2FRXN4mh96r33UxVVJ1qdEyKN90KZxYLKK2dz1seJumMhVwu7VHtAbAMTCT2Zgy1iUOPj+OOm42yRv50Nb15ZLaJgazbOXLpeMf9xEntqzanbmjhLpPXaahvTgbkogTn4BJvMSqzUP722QCmhtUEOckBpyhDr5AFC/qpr381loE6+VxSIAtXwPIAO7BE7HubuYT/1qC9HuFG0PdHZUPIKiHpwhhns2IUrKJStIbfeQHktI7nyzSAJi8XBuTictz6a2BlINSVcVjfaB1imlhDQO4q9vNWK92fj1HDlwmCC7zTeCNTX/OjhamkDM5stPvhthZv8NzPdPeuofiHEkFRGRmzZTpbNLh9SwBFCeQW3f7N3CPusRvieeeGoJ0OmkxtnffxC+669InOM3qu+5cMNxt9EoZFkWLRWtVTN6a6MZkHRR8jLXD7HaRxpasTjOpHs6xdej/TZ0YkPOht9p0uhh4egVAkRAcva3qV09xxjLD2OQa82XNEv9QW7rV2eye1E3VtbiwTQkWsOOVoEINCRdy/Ko2I29ur6rt5QjE7kL722QWth4pe67mEPes/EdvvV7Oz1TNEtrRLau2UXx/x0Fn+RGZs/JM7RK3SKVzJj2E7bbLXbbnW+YN8Uk6PmJAoId2NO31MTsYCSUHDlaAijJtdRpHETNN6zARI3UZfkVKHB1XPf9oSVQUQUaz3y0C7ElAucumlcZjLmqMnMMqYu59F2fKDAYyJ+1lzbLoDJ1fSUdZJyWtmMTjTeMu7b9Tun/h1Slbd/SqaGZirmpTQ2FDCuqeJT0Cvu59VqHjw+oc9xLnKXtqtUAtn2nuCd0KpygRqcI1UJUByivRqvWTscc8TZqXnGUS8Qo5v+SyQFNMiKoOxmARVX5VbUntRQqbxWACkHysQkVBtTj9PTrDCswU43cpw+/z43PrB2nVIbKrVNHV5p5GhhXaVvBNB1w2B9wOOw9pvvw4hC5cJO2FQMofB53bkKASlMyKZNeGZ+G1CzG+lkBX9L0/XB+mLmEWcqg46ipkxZjO2qc+ZSIC9Klu7PXYCX2jMjo1ErMpMp78o7FL+3lHDN44Hb2LNSwo622kJwp6SQgqszbLFeOMfXfsHBnWrtZrc55WS3MeqOPNmE3//3RQ5A15BfTNuWGGoSorYTUxq+Cu+0N2haFhXmf+y+a76VSX5+uo6nNoO3ahd9Ws0ot5jSDzdtL/OvLOhHelyD1UoFN6qq/sbS4jiU/Ikm9irA8hQEKT49N2+S8OqFME+5ubnBzfY2b6xtwiafGpREDJTAyGCVct0GzcmwdLrXSjSnsMxGQEtbrNVbrNcbVSq+7jgAF8EzipdmvHjzuk3D7nNoFVXdoEnRDOuah1JIWefmjBb+kPLbUmp5qHpNdY/EVapam4GLtqpDRClNNZ99veFHzM8CpCsgU+roP89kVn854ifDqlLiiakJGx1VUMJxtSdl6IDpR27a4xThEIjEsRaBOirC9v7gH2BTT1aRZj1aASSyBUdZXxw5OLx6iZtaLXrkBk1nbYlu8vVZmBKJYrtEDb9jZ4asApGxGHlFloHZgE78Vs1QqeLU1ewiYVSaf/U8dGC0CVGORqJJMzhl3d3fY3d3isNuKBR8XRJmGNAlTAqH4uafK7VIghOx5962QzVEBqJQSUhqw3myw3qyxUpDimnFoqP1zeoHYeMxhLBC3fm/jBOMsiMmxo1H3TLqYS8T/HvyYSxptXdq87gej+8NcxcYs5upNr8w2FZalL1dCOHesZkdNVGrjoyOSzVxd3nXsv4as5mFhqfYREp2S9TV5R8RtXZqq8mi6henrJffA2L+3ddR99vWQ1LHX+vlXd4SZWzwW/9DHaczSHp7vC9ZIR1oY6qP/JUANqSqT6u3nhEJza+BzwlcJSJ3iiSoRX3xVKu8NVMJ8ionTiOcV/UC2W4hCQaup76WGYLhBOoWIkHPB9u4O+/0Oh8MedulabJ8RDCLIHUtke8+qbz7STT2XRkSyuZoSEsmelFzRscI4jvJOEXGB2TzRAf3XBY5wKRn1nHqXoas8j4iH9w1UM5Y9OFJl4B3vjlBYXn58XghgatO032RpEQo9kp3jh6/P7vhwmYl54KpD2eeQqtP51zjtt5giNWrceXdHCbEl0KdAipcyi6XOJvH9rX0Q8abZlzaPI/W+byviZB10vsxJHzkNKIHpp1DHCFytdub+8B4HKbsjIoXvx4MT2cZMmpxQ9vpYRlGp9TiaGOFZ0nnTEVbrbLVJF3pllPydVkUpBIOYMe32uH7+AvvdHpyLaOECTRYz1gpaidCeSrf8AvcHLANUGvQsBxFoEIOJJ0+e4uLyEsNqFI/NkI1tl2wI1crSs+TKaTrXGDi0vksAH9+OVDVSU3Cd27UhLYy0m0jO3njBDCwZKzQEMgLIYjjSria/Y6JaqMtS/Raj3if+LTbpnjAnzs7EPAQAzy1LmQ925s3mh82BuSXZkvVetNSNxJWafm2ZuWMG1LMxWnD0GuP1klMLGDEvYzIisNbzRpRqPkwAq5Pnc0IPyDMJrGcmj4ylWO0OkOtVLM4AMawYkIp4Pxcr3fPU6O9xkALOR2TDfO4fKWPbE1zlqmbEZw4xSyJs1caf4jNP150jsY11bWMtPu3PgnBhTNNBXCHlDDALSAHhtl12nYH3k/VLX+IR/Xbdj6r9apLUer3CSk+b+5IPnFlUbjBOed6I0mwPAhGgqHkTCVLdrI51h1gmdXPEwOzYeueuHn28Vs3YZ7IAbDh9hGGZ8ByTAPu8l0Ik0P08fliYG5WEfM4kmOdJc9H4wL4nf2f/tjOgl5zQPruneo16bCGfxbi9+uwdhLkmrhurTh1e3kG5S+fNgE5BFOe9MQs2Fk16XX229fDAWfU+AKmXCAo+vqNjqoeOc5C+Zicukdb0BONVGkosVvmlU0m9p2nCbrfD9fULlOkgZ6NINrTdFJ0Y4OzXUC9tGd2n5mwA29V+CUMacPnkCTabjYKBXgewkO1SWNat9wB9Sjm0AOSzfQDqJJ9lYnR00xl1Cr1qmeHLEZw5eqeNIQA9+Gt4+ayXxjmC08OJ4fFwXIvyKsf5fFWcMU3L89LmtZmQ3wf6trba/NuynUkOWhQxNGmZAQeoXrPEBFACp4cvkK9OkMJx5mbJ7NvBybnC5cTvJlC9mxB4XI19/jJ8OX36uxOO1oXb9/NrCvxbfT9/OSsn5sPHo790eLcZoKNltsL0O8wQvoDirIp5HzMcOJ3pQv0eiBynmI4Hl/3A0JfdzKXFfaK5ivaUYcR9xlnH3vfHaZb24JyJORKqNNk/P55mKbz/QSqoG+7RwAOYcw4vG5Ym+akJeUaOlov/5vC9fRfKDGX1Z4fiO8vGxfloQMDt/k3UtM02aqM6DipZ6XO7Ur5vUXPa/kg43k9Lot1Cf8RoR+jPolFb7ZBZfY4SslNE8xWw3ufPm4cUVvuMlhbNUjibh2kjnpXkPhG7GZMHLtb72nQs2T1ahFn29wDdfTTpmNq3j/PQNOeEpfk9s0R0xqMq6k+fmmy/n2uk8z4EqdBppB59KQ7cmRP6qJXXlz60ZJ2BsBk8h5wurRKzw2GP7d0drq+ukA8HELOotkpBznrTJzOSAtmczpaw+NrymrjqTDaGYUhYjQOePn2Ki81GQI/5/Isaju3BLD6r+xKpf9WF3nNcv7irwZ/1u9XnGBMyVz423xfq0GzsfwXoCMn/lJAEF0/93iMbgjf1fhWNqGqnVj5dLsaYoOPhTHQ5UveFkT5eL0vDLcVpSj1RhaTuy2rZuYsxB6Xl2hnNOyFFHXlfzcgXEjlPyrDbet3USF21MRGIBlhDxCO6WCRzesCZQw3vP5CiOIW4chYUo8w5bCEivdrmvhMWaDmL7vnRKkZp4oTpZ2PEUSuPuRS1JDEQ7GAdAci5YJomHPYH5JLBXPwcBauTV4IBlEldFMqbtym22xckk1B3rlZdpFLUOI7ucd09QeN4L8+I4mwzt/0pWBK8HzT28gsg0Eh21o9mKRa9iNQ4xzaul3G09s2peST92ILfO1Xxketiaun3ca4i9dYjC33ll+vUzuV+n8/3OpgE8O4pX/KRfJsm3JtuaQ0wzOipPzO3SJzBi9LP8m5am0W/ppvSjgoXPcBrlTv6dV9YisP2fNbn0qFs5RzJ3oybltLGUhnh3ioGmKs05Xtk2t8pyVkpO0J5bnj/gdQDwgysFtQ05whUpwCqX7CvKtyrSGA7YCdn/cUd0h777Rb5MCFnPcRbCrhkr2vyywyllON0jQNhjQSGA1DJ85TEJdJaz0hpRF2ErX861oUTueMlFeGpnrEFaEAIYNEMuLWGSqE8wB2eIh5cFunzIePYg8+xOsxUKSfC0lxiLC38dn/j1F5PNXZBA1Jz45Im1Syf/miC/R2TJL3uXa5M7fo8VzXUS7L3usY6MzTkuVFpL88HH89j+R2ZQ+cC0731JQEMy1WfotY5vu/SNvWpT83IrMYxBlRiFAa4DLpiqp5EJKkE5oQhEcavepDijh4lrsQqMA5xY9hIEDhMagLEFcwDij7TiiYu+ng24pRJqVWKje7axChqmWNiNAGcXQCSL6Xg+vkVbq5vsN/LIV4iBnIG7CwYZOKQqbaMGwzEQupbvK8sXXREmRKBEpAGcdkPLkgpYRxHjCuRpGKr4r0yWToi3Mzb9cGxhR373QkIw13TdJK0L7iYRwS0QFENLGWB2jmc40B1zMuFSRPHWJ7e+8BSiFJff6NUU443YtmLwb2GCRT+Co7XRws+NqdnsfvmL+GHoXqQXhrJziO37axkE8ae6LfT93m1NWwli0gLomwU9Rjel7rg6h7m8X7rCm29siug13zuOcQSk0oVvO3J2xNAKkzzmQTa9ROjdjOxnUONa03lNTJmjsR5LCe9Ky7MPUrIWYZ2GJTWnIlU7zuQmglEYXwiD7DkcqTNiNuvOj4KKS8lFd2nyjm5IR4WZd9GWzRxCgkXR0L0C4s7pO0W0zSB9aI0Ufnp3pRkVGl8z9rGH0G8rNyy/Za/lOwZI5HcZzUMA1K407oncvVslv0dB6rGAKQhkt4paBdU36Se6M2+NPOlXrZUO2dJbRtDfV07p/cDuHSA8pik01iedu1vP6XMcyWzPnCc34FJmeVj73pJuqec5NC+iNE251oS2EacrWlnMo9laAxXS4hPX6HTq+lCf/fpohROwcPeAwDK8aNT+8V8jjE1bb2bHHUdSbubOWzFOXB3fcMhH+MpwrN2K8QayhD2sqp1Qe18jQwHgEd1X0tPaenpg4LTY+Z7r7w+nc87cDp7isYYKnRxSQlimQr22wNePLvCzdU19rstKE+gkmXDM/qqS+JktXL91bnlURURAVXCgIOOfJc4yS491Cvj4fxtmLxG8JTrguXdtzWW3dflSD1Ra1e5x46Q2L7Q8RDZgIcR/XktwpNOeurnyTmqv6VPY6ReGqj41aidrC52R1ITFhgh279oBMUzrOROPeuB56F9YfXq77CKhLgF2Hce2NTlD0rTMVb27GTHh/JOxlmgpf61SqsC2F1a1UQkfrk+et+BFNBzQNDOrNNoJm3dG7j7fkpPr7HO1aGfAK6Gy4YxXNRx0R45CAOVo+bCyDlje3uL3W6HUgoG1ZX7QTyKqq22pcy1Hq5jp7CpHQCpq70/JCIM+pdIt6EJclNwbUnNQgFqZiixEHqC3hOhKM2Qx6+c4lJ92wZxY3+xvBlt8XuA6eqp0l1UK9l+3hITEyWayMP27V4imjXvtl9O7THdx0TNVW76D1EjXbvKwZkYy2CW4/F1SKG17ZRHJ9ACx/Z+4vyMlT7qKVqNAbq1S6DGmeyx/par2IJIWKfQvFrxR/e+2XLgth+WZ0OM3o3pvZQuLvi2XKAf81A69TXQvtN13XY7hbT6+QBe730JUn2wzpPA/vTVcYovH84BqbkKpHvSqLZIwUf+y6UgHw64urrG3e0dOKtT2TAfU+gb0TAYKFW5IdalWWRHG8aAupscCBgSYaAkenI2Yh/a6QTNrIaklKjxWAL+k8xA59HcbvVtCb/l46W1+VO/0JdZHFOxzZ918TyPGIlmQMWtnKl1DMzErKzW2MEoTeSkiQjlHn9u577jHpSEute2NUSsy7OhewvlMStzzmHCRWKKhsmgIHX1rhLNortnhhbnjU42N/QIL6iraw9UvldrTEBEodkcsu4JhkmW95K6tEl4gl4Ua3+/Qvv5OxdO403fvDzFZ49axkES1WUQgUqlq8A8E8713PdVAlISquqqHb5j7M4Ci/NlDTasNrEj2CJYizAAMU5ITMiHjP12j9vraxy2Owwg+SOx5JvJMixWbaz3zjOzEDaNk5LeUEX1NMZRyFJVZPL9KDGmqHf9htKZARpAoKOOOx8UHjx8X56xjtVspByQE6Qlo4RzpaAeWO67fiKGU5Zmp/bOZtauXd8+mDl0Au4ZyE/DRgOsRQE3RO7qebJIINwcTf3Rv5kUZZKyTHnpA0vd31NGRCB9L9qOMM4mlb5UOIt9fBeCMRP31Jtern7vU5CKnKo94vZ3937R2msxHOvkl5tYS2eP5nGM247P5DP10U2K0jT5MGG/O2C33SIfDkig6i9eOd+wjQHnO3neAzbFHKDo9JTzuCkpQEnJdpmgFdqoKChIUC8RmjNxPVGzfRpU/xn9qDcJzl1PSx21+G5BsqB2ivZqksaIIb454xlRbWWv+ntwoO7TOi8wTJFzjg2fK4VC8HW5zFVU6zI03ebFcz3PRp1qdrH+J1/VOsznRmdJ55JUaIZrAdi/h9QhbX3iQMUGdLNJcG+IjM1c3XeEMjkYhv7nnu4tW2221WuZ3FPr1iHqgYv74cd/7wl/42/8Decy7O+X/bJf5u+32y2++7u/Gx/60Ifw2muv4bu+67vw2c9+9pWUHa3M5PcJLv9LHKrKJT7l8LeYCn4FCRO4kHNdHoNTENUl/ogBA0TPcXt1gxdf/CJuXogkNTpIsegH2M4zMHy32ghHKUDxG2IgphgB5I60084S2d+wGjGsV0Aiv3SvQK7piKUz6dUduog5SBOSd0vc+7/ZYdFwgwsTg4lRUMBtqaHt56weG5P+L1bmvtCXrX++SXh8TpghSr/GjgOPMTii7nwwQIUp2ICE9/WSNLVUg3pdyrG/4+Wyltf+MRXVj7DHKfpfRkGmAuasv5b/a2Zg9JbafH9Yl1n1pS+oChAEkchUw+BmbnH8XhKgANdWdmuir1hgwGbro18Xxf+Yc/jdjgMhWAh7yxcmTVuRE+/a8K5IUr/yV/5KfOpTn6qFjLWY7/u+78O//tf/Gv/sn/0zvPnmm/ie7/ke/P7f//vx7//9v39l5UftgHESruxjQ3yZhA9VPVAs4Iw61N89J3VGWc6lLSegpW+ke21M4Fywvdvi9voOeX9AyQWDEYqGDrIVJBKUfsI5NOmnyDW6d3gXp8JiYTjhNFXfOMoVHaAak0MjZ6TOm7QsabZGMSFqg2PL/WbzoT2fYkW1nORSWJa6yX+134FGfbOUpUefxzulbmvqcKYV4FK0sy3e+r5+AEvczP+j/RoIrWbvDD8tSDeyS99UhzWBDW0R8WCxptEKdRZOCDRHVatWJcwP8s62mcikvzqffRyWVCZnhvkBlbauRr8iqMhakOe9NW8rHdtve1cpLBkCs51Ny6FsBeFG9XI+3X1XQGocR3z4wx+ePX/+/Dn+wT/4B/iRH/kR/Pbf/tsBAD/0Qz+EX/7Lfzl+4id+Ar/xN/7Gxfx2ux12u53/fvHixctXjmXiEo5PtpcLvUVL9/Ykt3teqBuyCTYVpGSfQpC3A5gLSim4u7nD1dU1psMBnLPcGMrcSlC2EIKKj0tZpNeNesXX+MJBZFQOfxxHvzI+goxnfw+xeClOtt/HWIwUyi4xci20rqmWEZhVM+xfSPIjlPEdhFc7X19F3mT/e1g6PHxOGQ5QXZ0amt1hvyGJaK1YJPWOyL/sENxv6bhQV1g7TiDcQ8q9R/V3+rhCXVNRxjGANOOaCKat9eeShByByjLvyqT2etGmKi71H23SYnjl6j4A+B//43/gox/9KH7JL/kl+CN/5I/gZ3/2ZwEAP/mTP4nD4YDf+Tt/p8f9Zb/sl+EX/sJfiP/wH/7D0fw++clP4s033/S/t956692o9mN4DI/hMTyGr7DwykHqYx/7GP7hP/yH+NEf/VH8vb/39/A//+f/xG/9rb8VV1dX+MxnPoP1eo0PfOADTZpv+IZvwGc+85mjeX7iE5/A8+fP/e9//+///Q5qWCUHOzB3clvoXQzM5sh16Rm7xs0kbDvdv1hX4yaDGoYLo5TsN/EeS2flaQVcouJZfUwdWMtAl65XId67Z0KxWuoSJqgAvVnc1kXe8+xZ7EevYu2RWee1afv3y50dtaFBK9pFmn15BeHlJ6v1X8wFQNf/76xWwMtLe7O1GNdE/9dLVrHclyj+XV/+vWoWYdkEnUL8fl62yxJL/M3NfwvtNG2Ci1kxVkjZ9HkYDfJMYAZVS3V5JzqAV67u+z2/5/f492/+5m/Gxz72MXzjN34j/uk//ae4vLx8qTw3mw02m82ZsaviIJ4Sb0MVgOc63JcL70wTY+dZjmQSAKr640IFpLByzb6m5IJpe8D1i2u8eP4c5ZBBXIA0gPRGzQYBpBZo4KGZrzJZs+rc0yCNppRAJaoluMEqIsJ6tcJms0EakmwWs21Zc3tg1zeVITUp2qJomVl1E/cOW/F5UP/1Os6ecpg6/Zy5b3DbvJtnL21O/IoCAzPVT6cJfcezv2v+0h7XS5dxLKGNlS0Hhu8xLjl2bY17w6jTsR2ydxa8eveo6ySUQKfCJD9r75vqHjItzcMYmQEkMQk21R3UD6SesXJNX+nzkBdLVWpMqaKVSGBUHcRU3fcQevmum6B/4AMfwC/9pb8UP/MzP4Nv//Zvx36/x7Nnzxpp6rOf/eziHtb5IXSmI3v3XIlPZHDjnkol721YIGVA0AW/jEHEsTA3D2anxe15Ljs0xxCn5dwcaERicC7q+XyHvXqaSFzUOIfrQmY0BIV8YgXpw/DK9qMIQCE9c1NQKHk6M0iQw41J1dRiPGGcViQh7cFJO2Ad2Hu28rkSpaP9HPbU/B9pTHOjWLAAaeZDMwEY0c1As+d0gtutezFL1oLmgHRuetGmnafT6r5kMKMh7taH1nm+i7CcSz8/Z4kI/V5cazCxVLNZDsu9u0B/m33HYAgwE7QCH2prqSnjhKHCsf3HRpsQ8llYpUd7l2Bj3TI0S+6XjgXfR6qFzusVS/TOYe8HM5qww96yr7cwd23lxvnr1wHJJx8be7J7ph5OJ9+VPakYrq+v8elPfxof+chH8Ot+3a/DarXCj/3Yj/n7n/7pn8bP/uzP4jf9pt/00mUQEKxVGiE1RIrXBVRuxYkybFyUYwBmU+voWbVAV5fJS/0e/4B5mlaFVXNgFICE3ZkNcpCImBlQNd/hsMd+t8V+twXnDC4FJWdw4dbimmsesS51vsuB3sIFuWRRIZbihFh4Mfmzg8Cs75kAGhLS6K6PIZ4f9CxXUAfa+S4qhFTgEh8x+/ekHF9i7Y7uT0Q0ie99Ygsv9NfM2hZGrBWFnSvvpYLIasY/ed6rD9l7qGWkrAhGVetGbrWZf/3EWfqLISTmxnxbW0nt332hVbHaEqLZn7SJ5u+1QseqfaoptRL31dLmEkK7DJISEsmfmeGnc9pPx7s6qs8i+yJ0hORoCAhJj4i0fyFuM9/Sve3sxy1+Nl1+YrKQ0RD/JHUITV0+oSyNz7UiINT+bObcrBsF1FKX/7nhlUtSf+Ev/AV8x3d8B77xG78RP/dzP4e//tf/OoZhwB/6Q38Ib775Jv7kn/yT+PjHP44PfvCDeOONN/C93/u9+E2/6Tcdtew7HZSQQDkKFnJJKlbIkBTtEbMsE06vMmFRxXU6+GRMpGuijkpPxmrtICouf8lNnJqm4+A8FkmbAFA49m6EbRCvsJ2kX5DzAXfbW1xfv8D19QsMYwJlRpkygAxCQSpCYGlWrpVRQShOctvfKEWINaVw/QaJOmEqkyy8BGwu1nhyedGVI54r8r6KgHIZI+BXbDR9VUCJhNgMMtEb8NCzWUMyItnu6zHZmKlqolsl0S2Mj4iqOE0CTMnvIEXh7PUjk9QM5FViIUDP4mgZuqALV9Cs1k4Lq/YhohO3WTgvZtkscV1nhEgQT4KIRJYizN9oqH9XvfPLD99OH3tsV10VGFpm1N4tZeXgOi98saTjddb/ynIWDFL/zZUeAbbe2qKPrU2pagUoZha/zDBaQiFrK0A+CtnZRwMXQr3glEA0yO+uoQyo9wBSwNe05u2GABrsxmvC3EcVXKp8wMx+9SD1f/7P/8Ef+kN/CF/4whfwdV/3dfgtv+W34Cd+4ifwdV/3dQCAv/23/zZSSviu7/ou7HY7/O7f/bvxd//u332HpdpQ9P7OesIfHUU+pJvuCTbrF2o139yMeyzUcN19tWiW8dLS0gYZ0dNoXFTdNx2Qp6lyL5FwcAfQVJ+bGWpzHolrXMlGvJ8zV/C0w4qWrt4ltZInRYGjMAoDU6kLc8pFQYqqFOd8QEFKBE4JVAZlPgPhobAIXLkOX6gkSAXVQKIul9q1UXr1TyrSKLKzN1QTNGOjuXLd5SSiVvvlBAGzMDsMqf19LmFfms2LZ8mOpQ+geTQcfVVfSNut8tQB7UIG4ZqY5WjtODlhbSu/mLcTYszdO9HCj7YP7+u77gW3z8lupr6PzFjVI1eBFoD6Zz2DxdY+X+OhKrFvuvUfH1fkttWRZoyzFJ00fqhLdARqPR3WimlIEOr4EIbllYPUP/7H//jk+4uLC/zgD/4gfvAHf/AVlioc/X1eA+I8Mk6+Q4VZIPNQqQAoDOLSBYXLjK8R/NOqhdMz2c4hyWWFBNPSEoQrj9oc0fYVTHnCfr9FzhMKsqTQOiSPH/TMWg3HoTBhe0ITH8uWUV0IzHJ+y/ahVqsVNpsLbDYXojIsB2z3GYcMTIVxs9979kXLjUYdsc9SIgxDwmq1QhoS0ljfjatRjDhCj9o6Sb54CGL5oWAVmlVyQT5MKAE0TS9jY5cH+S4unkL/lBJGkBygALmVlJnViIOsId6nyetcL5OsDQ4ECjG0u0g9MXmIhRgQmZX53G5/t2dczvUBeJoiLXN49xGxFmR6lSw1X5tD28Biee80UCjHR+fEIXxSnsnAxVYiU2UMzwEof9czbDhijKXvls412lx3f51HylK9AjgpbTLFR9tCfxIltjCVzg7vK999J7d/fRUej7XIpUF0qYD5cK0GB022TflHxCpXA7Xxjq71jiDY4UY3CSWqnowsiYJlKQX7/R6lZJh7ZCI24zpJUWwjNFTS6ulSRZSsoAvKmDJR+aWSwsJKClSqmtMKlZyxu91iYsbtbsJUgENmXN3eet6liFPPkrOvuUTVBdAwCPCt1wJSw2rwblqv1xjGAeNaDw3bny+Y8FcYPGUcsuydAUDJGYf9XvbsTO1C0FuGB1E1jmoAMo4YVqNv6DZjYlJYVuLDEcK1Tc7ynkHoTSoNAoepFi2Ubq40MLUwT2dzvMOJY/79pPeMtdONdw6lxabMJIvFprXMUEjbcOOdkCnvyR/MrfnIP2SZcNu+eVU81ezdUr11/Nou53mcptfr+0bQse8GELMiW3Hav9IStalgxbPrMtra1Wwikalf423bS8GY25Ozl7oa2pVAkYCeEd7jINU11Aj3qQ44gSPzxeva5Zp9jNoYGszLXZScuI4d87zIU8E8ThQI8Q5ZttINAZkFpHLOMKs8F8I4LtrTk4U5Lpw4yciJueikK8FgBij8x0X2wna3d9hnxu1uj0NmTLng6sUL9zadGSiFMe33Wg5hSEO92Xcc3XvFMCSs1iOMEuWLC4yrERsGhnHAMIxIK9WO+9XXSi4KI08Fh8NBzpAByNNBrCCngxh8MIMSgQbCMA5IQ8K4WgGrEQMEuOyWYTd8SEqwS3W7VVVuyj10xC0S+ZNWXOEzSr9qQKzzail9y1QtcuJYJmhzbrvjZyouzyraYSYivDXPVSU6r9OREA5J9YrAtsg6V2nh+ay8U9LYUvwFGOoihBfH19is3zn2FAdhrAfBMD7N3i0t53sq9GT0zMTtnnDNa8ZTkDJWpMNHVufzgOo9DlISaDZhzwsyf3SAlYj0JrZufQOb8MapnO7gJVF9Oc2CEcCx+kamxyUrViIzeJ3LoWB/2OPq5hr7wx65FAya/iGT1wwkqvWZSjYeQ1VZpebLWZx8YoBrYPd3W9xeXeP2doftfsKL2zvc7fbYHQ74/BefobC4YJqK7EmtxsGlp3EYMZBe9TGIN3UBqQGbzRoGUk+ePsVms8Gbb76JzcUF1psNNsNazmaBUAqDi0iYh8OEu9strq+usN9tAQAlT5j2W3DJzgwYSI3DiDQkrDZrrFZrbDYbHC7Xzm2O4yjAOIrKMYH8LMqg06fo4DSSR5wFQR1sn356phfFTETQ73RE8nGLvMAMNUCDIIW9JKHqanXGs/vzfXjJbVpnKcmkqNpANXGp8bt1erYK84wavNNAREdB9VUH39d6YKqZ3EfheMsrCu8LkAKwxLq1v3p4BxoO5P5BCv7+lCPmSL2bvKvkZdJYQ2l8kxrOeRwjCuQRQ/5hM9SUSZGrKiVjmiZsd1tk8zaB+Bc75NT3WIuWy6xPOlUKc5WslDofdnvc3d4h8xbb/QFXNze42e6x2x9w9fx5kKTkc63OaAWkBlEb6nUfiRLG1YhhGHDYbGDEiHPBdLHHahic0FxsLkAYZV9oysi54LCfsNvvcXt1g+sXL7C9uwMAlHxA3m9V/ae9mghpEN+DaRgU/NYo0wQukwIgsBpXWK1XwEb3mIYRgO5bJds/7M8qteO7tFG+NFL1XWVdPe3C/GVUxsrLC9xOL2nci03ddG/m7hHNgM+VZo+Gjys9nDk8XnYbvTaCuG1DVS1VOa7Fe2qW1TGGcalvZ1LkmSE2u1lHaOvOaNt2NL+gVrtXQXJygI6Hk68jB9R0cJXtKMxCBrBgPHg0vH9AysOSGBl77sjSn81eKP0tzQQ1AwTba2BPWjMgBbzUSHjBw3BXjINfX3y0Agxz1SScYntC3crcHw64297h6upK9qW4YGj6RQ1MUgeyHZBVtZ61jYBwutzbHLJmmAlAAjOQp4IXz6+w303YThPutjs8v77G7e6A/WHCdlcNJ0j3frahRmO8XkKJ4DAMGIahejAh4PU33sDmYoPd3R1ef+MNvPbaa9gMA1arDVarNXZ3O+x3e1xdXeH29g7Pnj3H8+dvO0jl6YDD9latAE1slL/1eo1xHPHk6VNcXF7i8ukTXF5eYhhEel2vRbp68tprGNcjxvUaaRxA44CElRBCZpVkOwMa8o6eGSxwnANYniPN0HXBDFDmVqKn050sI4DB0pKZ1cH+Wl6mgtBCBhQm+ykDkZgXmTGRNTVZhYtiIzfxvXbNbYYy9pLHcWlgDlittNY8P5pH/N6eo7sP9I/jT+ricfd+CWhpMe5ZgRa++xbJnHk3C9yeXb4vvA9BKgYdfE5qNYOTJqFxqkRAKC75zHX3RPOhr/FaC5uonvMholhajWPfrV5oFmssSw7UmepJ1FkH7Pd77HZbOXsUwLFCa1EvRKFNQhVdLbjkNKH6+Ks5Fq7cupALUwMW7LZbPH/2DNfDDe4Oe5WkbrHbT5hyQQ5FDApStocGQLxZdIs2pYQhDUCevFsSGPvdGoeDetnY3mEzrkT6WV/g9uYW29stvvDFL+L29hbPn73Azc21q/u4ZOT9zhvNpR5Slr2wAdubG2wuLnDx5BKbzcZBanNxgYvLJ3hjtxM149MnGDcrjOs11hd62SOhWnT5BiGqUUrkoKvFCkxStXnmllsL1xdTJwHEWePEqBvPbvrdG+JUoeZJLHWhoEUSeewpvM3nxK37Q0IEzdrNCSGjMgJ9MaEDrH/jHL8PqFyT0SNu5S8XSu2busyILKXlhezqdkJbr1OgdDIs7qUvcUE942/f51JhrMHS11PhfQ5SFuoEOsoxdB0mk5vFSsaeucFBAJ0TJVrMaC11TM/spXQTTf+HHSqN3LXsRRHiIV8CcJgm7Pd77Pc7FM71PJFKYG4K4YQvtjkSNiB6uPB52lj+QU9UWJ3EfJCZkXMWsNwfwAzcHQ7YHQ64vdtiP2WUwqDVygssw6hEIteF5xZ6gSunhJwSuEzap0Aixn6/cuOHvN/j6eUTXO4ucXF5wPWLG9ze3OKLn/t53N7e4cXzK+y2d5img5RTMjhP4t+QC3IpKMyYclbDiUGAb7PB5maD1bruSV08eYLLJ0/BpeDi8gKZCzb5AlwY42ojEvWgjEnoQx8zqtKxjbuPcSB2LjAzST3DmHWTqX4Yh0/Nq5q2C8e5dKv2kkQSIs0kpnbeSFsegIr3BAdvAyh0ZvI4vU7NUq9N8BL1JfLBsHl6qs4xGMydG2Zw6x3cMrvy6rS84oxz0L0el6pCOWT0oW9LBam+VktM/jnhqwSkjoclzuRoILf4PjKp+sHh8MZ8ZS1NgGPcpHLNqtqriy1ORjn5bed7cs64vb3G7e0NdrsdSsmyh8+A+w1S4Olv2G0Ws6/dIPWhltOYQEfJXs9yXV9fB0lQlm2BWB0CBUNS0/7ozmray11fenBWSh4EhBuuUk7IHw4HrzdtC9J+wPZuwGF3h7vrFxgIuLy4xJMnr+Hq6ho317f4zGc+g+12i7vbW5ScvT1iOLHHkEj6y0xwCShTAfKEPU+YdlvcXSfQUBmD1XqD9XqD26sXuHzyBK9/zQfx2puv47U33sDm4gnGJEYVpYh6NoezWFJOtNRkUNGZokwRMYvpo/HtVI9FAD2doJCX5shAw1+dnO+Elv5FUDoRVAoH4Aea2/edpPEOg0k9gLW/JbRtzB42j0tkUldALu87DdmW00ytRr1TgdNBDs5aqET+eMldCyJj8NJdfLrcRl9rrj9OlNVsT9haLi9nBvJVCVK+XGadfN8IU008YwmOLMKGSTs1RMdAj3zx172h+SRxbkjVbMydvzrHA5Oi5hzxsgmuXXIYOK0YjxeZz+ZQrPOWldl0ztckOvP554TYq8BNIQ3HFywcvbzCyNMB0zRgOhxwGEYcDntRAx72OBz2mA57OeSci1e45IJSsnPWpgo1MYfRAXTJYQ5I+w77PcZxhelwQJ5yMFpRDh/AbDPHHXRWRsaAxdXCC9OmmX7cc+d17M6ecc5Q1N/slgQnMmkSIbT31YZ7V2YDVG265d2lhWe2NJSZPCVBxVNK/t0+GHPycCRUdeHCuyUDjsic2LrF8TNLRw2yFulXjLDwukfG+0aloX2x7NPJ+vCuO5h9DI/hMTyGx/AYXja8rySp46elWkhnLg2cn+SYlEVx1dWrZhHZPs4VhCsHY/vtDHbjjilnbLdbbLc77A9iCJCSavrQqiGqts24X91bCuJ83feSZ9U7RHSGSd4vURJgQDyQO4tJ6qTcNNdiB1n7QCQbv+IeIrAwFalrVHYEBt+56JSQ0ojdTszwr148x363w7Q/4PrmBre3t7i7u8Z+t8d+t1NpU8suVZJiBOONgmqRx+r9IrXqxzIVlKng6sUVdvsD9jljPx1wmCa89vqbAIBxtQIBGNT7aiN8FFOJqNRW1Flykf7gXJDVO0YpRbxgpKByStVE369ESVU1nKMy6b5pdmR+m7Jxts/wDhfEKZ+Bxzxf9KG3ivRPOrZe55oDP2/4KmTARtpZlojOy+YUXZq/M6H3ne75nSM8x/0lX0Nk/5gFq42t9gPVrnlIHd9XIIWu0+zhknXoMf97M1m004BV1yVVDXOsw7mvzNGBWVbp9PX1StTKyEIscM8JRY0V9oedGgUUNZs3pVurfiKKJ24Q1ID1vhlrY7+oa7uAnnqZIUD2M1qiMmSYZ/AkZyWaPIOPJ1ndYM6+8BKo+hS3hRS6lCghDVJeKQU3N9eYpgNKLri5ucF2e4fd7g6Hvaj+pCjTT8o/pSQB5GJ9Uf0uMlcXTQ1hzAUlF9ze3GB/OOBQinpdJ9zd3GAcBzx5+kTcNKnbqGpARnqQXH8V+c6FAc235Ox+Bd1Z8FBBKg0DhjSIgccK6g6qusanuABmKpiqbuqnoI+5u9pBM1dEc8lLWR+fztRCtKl9jxsTLSqxFr962bYnZhWmBaNxbuOfqvQp4Fo0UIjaP5m8i7U39b0xmn2M8+k4dZ/LdbTwIND0sW37POYY97JjUbXc0J6lq4buCe9xkOo6v5SmA6umdq4/Pd/1h6Qs6lbBiEMb58iJ/8Wpt1CGxz72PnCHYIAG4feZsBoGFC44BMOJ66sr3N3cYpomjCzufVAK7B6ZISlAmTl8Y2JfwSir9wWROKz24gehQKz3uJRqZAAxI08pgXP1f2cymt85BcBNhBdAym5LZRj4MoZhQGbJSazqqOnRnDOAAmTxzE6UcHN7i+1uh5ubW9ze3WG32+Lu7hY5y51YEWxMtisoWrACQANIYhTi1xRo+SkNSMOEfJ0xjCO2uy2221s8f/E2Chgf+tqvxebyApdPnmC9XuF2P7nkC3VWa+eGODPy/oB8mHDY7WRfbbtTwC7Yb3eoc1oW/jCOGIYR42rAar3GMI5ifTgMoEGMPCglmbuBoDDE6KVyt3Dwbw6I+hC1EkIfuPuMb+ysUk/wmY6ZSh9hGkMJlrbAJOAeLc9b48ueJtr5ZUzEYkVjzexIgekPHCBpHj9oDND0sdErWqYrrgHoGddaB2vXPO0xKhMYgnswzK9MWSRuXb/BeGkKR3mkkHPB8j0OUjW0jExYYGCAqbnW4UFQTpUza8bhJST4Y77BGhXFkXTxQG39TzjuUgqmwwEMYNofsNtusd/vAa7msM7TUMwFiIQjgm08qyRrzlR47PUxq8MUgcb/iwW3MNxLbH0MkzSqk0s7X9b1Y7Ou5NC1SSp2XizngjwypmlCzkXvcqpGJU3Nu/73PvO4BYkJSNwcODVcwyQLkfVOrQLG1YsX2Gw22N7dYbUSt0qpm39mxVdKkfG72+Kw22N3d4dpf8D+7g7QiyT3WznXRUHdJ26ZRoyrEeuLDcbViJKLu2oa1itQYiR1nUWpro/K695vj9aoXJvI3drq+pHDMw6XQp3CksoD2Fgvr9loNTsjzotSVqzqfN7Pi4vtiRme7q25VLrwI1pONhJZC05zdWYEshN1eJC6tOZ3llsjZW5mvUD31OpYuhPhPQ5SDWk7O/YDpU0nuHaT6hLnN58Q99WHmwVziquoi09vwrRrRhjIecJhf8DdzS0AYL/f4+rqGnd3d8omqzQA8Uwx2G/f0GolKSwsXDvb5V4uFATsbyaRVMqHStM4tJlUkoqcG7vfr2mq1wQMSX0Sol6HUUSXGCQ/qOl6wjolkSzzhCkT0jBgZHa/gFacS3FHGATjgovFNlCG7StV4lEIGEAo0wTKBTmLmflhynj+9jMMw4AXb7+t15ZsMNLg/ZKZIEaGjGl/wO31Da6fP8f27hbbqxscdjvs7m6RDwdwzjjsdu6/0MKoADWuVrh4conVeo2L1/ZYrddYb9ZY5Q3SOGLASqWr2s6USJm4yjcYpvjZN9+Tw/3TGh1RRZfpS2o07ivrYfs+ywS+XdcPq1c8b+TeQqhZAg/KK34ee997I5mbwi+0cVFqBCLHtxRnuS6h/CaHI4H7Qy/nhfc2SKkDKL/W4YwkD94YpW6aMTmhnZt4d24Azl43bRmtKC/g5FxOMbNp2XNIIHBm7Hc7AMBhf8D25haH7U5uBtV70hOX4JGgdhp1ew4uyXjZxpWSdHCW9MwilRSTYJw7Trp3GleoEoWiBhDcdU3d8dWfgxJFPcdP4sR2HPV+qrVICxcXF95/aRCP6MxmnAHsdnIOasoF+0PGtD8IIGjfRe5b2p0hXKwRZqu+AqwxOKU0t/SiFOSyBxW9mpyLAm7B7c0V1psVnr/9Np4+fYrXnjzFuNl4W/OhgHPBfj/h7vYWz774Np5/8Yu4vbnBzbMXmHY77LZ3KIcJnDO4ZAxpwLgafeaM44CkXjE2T55gtV7h8uYp1hcX2Fxs8PSN1wW4+Al4JVIXIBLVMK7rNfY+60w1UyXWhCrx9eH4nmztQ2ZlMpbiEtW7D13iqgCXGvJnc6R+t3zdZB7dvO1qFQqWKapr3PfHrA4LXj1iHYn0qEBfhkkmoQ619lT5Qgr1DmnDKtWWd5IpYtr47Dzq1oPQgmB3FOC8Dr6vys29Zx43SKAE9UJDL0F/8V4HKbRMe/xcirkg/zygBGf4F9+Bi1sSVY8QZxbRifc9p9dzTlXnLYuLS8FhtwcD4hJpt0PeT7rnpBZ9pRJakwyshJ5r8l/K/bp1H/exOPwnYQicc9zjgKkPg7hPqHItmXQHcnWY/KwS5zCMWK837vH8tdde84U5rtZgEKaJMRVGLgy6FiOJabdFLoxDLnrwmHSBt2NW4aob89BecJWuImFlZjnInBI4i3UdEbDbbrG9u3MXTHk6yJ1XylJxycjThP12i7ubG1w9f4bnz57h9uoa18+eY9rtsd/eoUwTkAsS1duOtSUYxiTGE+OAzW6Hcb3CYb/H+lJcOFEibC4uMIwDVszOzNFgtoZq0BLUqI3yzx7zksSyLBmxf4+SOVqCxm2aptebbOuYxGUSNXQcmBwj9KeltEpAbR0ZYJh0XtfHcnrDxF6LQrFyRL7HGppskU7Ur4vfpJMvvTuto3k8SMp8uVC3DPxBYFzr58sIzu9xkLIWJxUll2I85Oz3PJhp6sulPWsOnRWiyoWLWpgB4Jyxu9viC5//AgABqevn19jdqZvWoodkS4H7pQMrIHCzApcmcwQsIgCDmlBT5QgjWff4qX3qJuwayYhjTJP05L2AWd2HS4mwWq3x9OlTvP76G/jgBz+Iy8tLvPHmB7yM9WaDNIxI4xo5y1X0z95+jpvbW3zuC5/HF7/wRewOE/aHg3PcQbupYGdtQzNwRRtJbAeOgbEsEVdrTGUCDrs97m5u8Pztt3H1NV8jwAq4scnNzRZ3dzt87uc/h2fPnuHn/u//xfXzF9je3uHu+gZlmlCmA8ohA6VgNSTNvjJCaaRqgr5eYRhHrC8vcfHkApeXl/jar/96PHntCb5m97W4eCJOcgFgXK/U0GIApYQ918smbayMweByRKw4GSrzVU3ia4hGKb4t4h+B0XtIifeoytrAmPtdsXxO8ZdH3hhXqgB0bl3sEL7n/iUAlXcS+r11iktdvTJTuCVdaBXNgfzM8B4HKaCVZmj2SH4fN2aNoVezWVbt3lHNb1YW92n4CBfmfGFluJrqdlQSJt1X8d4UFFk5cfPmPR0m4bpLCdfEIxBP8/mmZ3GWpCgK0SGcoJ87KVUt5Oec4oLuvrf9AQfXgA6+tiMzLJVhpCTXdVxsVri42ODiYoPLywtcXFxgvV77WKw2G7FwWz9BKUAujCkz0rjCbpqw2x8wlYIpZ7Xuqw6uvBYJDkKiYm3VgbVabb/59h6rS6cs927lROCDnMm6vb3B3e0ttre3cvWH7rVtb+9wd3uH58+e4fmzt/Hi2TNR1+52OOx3ruLjLDcsT0YUaoeCJ6Ao4qYygVLClCdM0x7Tfo/NeoXpsMcwDGpAIj4P15uN7GHRRm49huyvcVGpJ0hFfWjocZCe4zhH1ZsxALM8jHixeTax80o2cdpyo3FENOFu1+RxEtgDQqxJFOBMjVclq3AuMMxx9t9d21xXXLM/GkzKDPnZejhKQIAmTc1r/qwvno889/aeqKelW1Rz2junTvF5nQuJjH6dF94HIBWDNX0BaGas/lJoVT1AKw25mt6iWWeT7IW0w38coGJNyf/tua44jHZlnkgvfjiTxJ3PYb/H7c0NmBl5ymK2XeQKd1IJwLn74kbgdaFFgkt1kkkqRlEVF2tcOYyr53tSEnWGETSiRoUWuUkiQmHhoH0iKyqKG5rY4WI6vx4GrNYjXnv6BE+fXOK1J5d4+uQSm4sLvZlX8l6tRozjBuuLS4DkmhAa1rh8stM9oIRhXGF/mLDb7ZD19t84IJU5YMkDDOg5LemhBLvmpOmz0kqkpO6QGAWYMgjAi2fP8OL5M7z2+lMBE5Wkrl7c4vrqBj//mf8fnj97hi9+/nO+/1QOk0pkYrwPlv21gai5mdnwlpnBB2nCdnuH8XaF2/UKKBmXl5fY77Z4/QMfwNM33gAAXD55gssnFxgGQloNGImQUXDgYCmp4+zmw1E67uaOdmKYv1GyiZdlNin0PZQpScYJ2irytsVQ55Z8em9QdcK8KMmYFC1NgB3c9roEJlCakxbK9m+BKTPfmqp5qQtc5/a89Q4IkbGzOjRr4XTwESkIyBVy7PaSmrQuwta9WNMa9HWNn4laP5/xuzEfDQNIRgNE42vy1jnhfQZS5wVqvr28aO2DimWuAjhPdF88INocla+gV9TkXG7IZezu7nB3t8VuuwNYzgsNlABK6tFbzcWVkBauXtB7nulYTY1IF2ZM6iE8c7W2SymhB6O6zuLhPYPkJGdlAfh9PjbplSibKujiYo31ZoUnTy5xcbHBajWIBw3IlfTWPcNQkFMWjldcMmCzWcs18sOIUoDNxSVSGnF7e4PnL55jv99iUqmCIN7cc1YrPrv8MFxXUpkRtDwEtXFS0uVXGEiMPE24vb7Bi2fPcHl5gZLZrRY/97kv4tmzF/jcZz4jB45vblQBJV4zRJ8q5ucoFTTbw5VFCS67BDNgAE+MjIy7myvkww6ZMw6HA7Zqxm73Yo3jiIv1BsN6BBiYpqw3KxdQGgBeMEpqcVmf6cNuI92klyPyiwY5RG0Se08kj1ulzddeJKinVG1VUiGv/vHYPa1g9CvGLVjBsB67b6vgdJ88MAQR7z7LwFiWaQ0ssQiB8/5utQcGxgpqBAhbbA6Tm4XjAOX98YDNqa8+kHIacwSgXgKz+mGPZzeMIeoT+FmnAFAUcyOT3OrKMYsaM5xgALvdHvutXOgHQJ2mKg0NEpQbWzCHxbQUuPm0Kvi5qFLN0IE5wNa9s1jvSM+r/Bg6o7ZcuSwiwpAIq9WA9WqF1WqFcdBDtCxeGA50qPmmAxiEaXUADQClFYgSxpFwQZd47bXXAUrY7w9YbdYoYNzeJuz3O68kZz28rC6kmCvhcb074WxvzqwAU3LGbrvFzc0Nrl5cIdHod1E9f/ttPHv7udwSvN2KqXkS45ES9xJtb7Gx0rRQHd8aw8AkUnHJjMNuC1aHuKQm+oAcX7i5usIbb76BkjNGjD42Trsj4Q7FhtfN86O0hzpyHSUW/4wGOlVKAlwAmmcbuXWLh2qOTfawSdOFk4NZCXcFs+MOXZ1idzzm8dUW6ql5VuHmFCFvB8OhdjFJfMjd02jMQB7lWNFV6myoFWCA1TBPUaaiPvaxys7CVxFIHV099Ws8oLnQjdR/mlhM1XxVnssiSbxw3gAGUHZvLy2KvXarqklPdTKTnJEpQJkyvvD5L+Bzn/s8nn/xbWlCEa8EVDIGAigLoQRn2H6UgRdTuxkuT+vtUNkJttQjF5bDsfF8lLr68b01ima9XDNukEhE/bD7oK/qgh2HhHGV8PRSJKlxYDBPmA53uLoqQEooJfniHMZLDOMa68srrDYXGNcXWK0uMKQRq/UGT5++hovLJ3jtjdew3e1wffUCNzcv3HR/e3eHKz2fNB32YhWYD9jvd84ssPZvvydlRhL1p3rjYBbrvTzhkPcAMa6eX+Hp09dc3ffs2QsBr6trN07gUpDBorZlVo/rrLSPUUglY1hfeifL1SwpgZlUDcjY3t0h7Xc47PfY7fe41jN1l5fXGIcVNpcXuLi8xAc2G1BKGJIcRmaIC6sqa1QS587cuSGXR8J9xIhDGUbMlp70a8mIKnUV7Iydlr+G+MfrtRTc+i+Cz9JtqoSjHjWk2sudF5fLyZr5YNRP0uf+6lg+LjjNM7Gla3uENR9GY+XrzwSgYlPi+jcKlwr5lskZCiYPX0UgtRSWRrB7trjRWieDs51UCS6jnXyn5lvPCQJLxc0zs9im/rPJVBriOU8eNz8b6tMlatsQJtexdtApStByUAvLwp822TQSZq1bYQYVRtFzTQyASgbUKCKVglSKSDyJXe0jDlhHjOMkh1+HEWUUlaE5Zk1kbo9aAAUwYziOBd9s1yoz9PBzLshZDBeYRZKS36KmrArYAIK+EarPW/rtvRJp5LyeQhVsnhgYllLUtVXw1qD9PuMrqJWII307jx8+FrOva21g1/tHiX3LTsZ4rZRyTm2WwzIAAXHec+2oWeojasfF9feAKh3rzl6Qi3OlQZI+i8iG23rUGcn9qtWV16kP50B1qozzwvsOpJbGrmEY/FkQe5sJ1A/G+ZDfH+59QNL5BvTRiCdez/T4p7OZpzmHiBwr+lS8Wmnm8Jvi+5cIfIIAvEQ4ekL/WPF8mgieGqzF/nLutc1hqYy+bCnpDKrH54PtfeEI/3ay7P7HsfbZu3NGVqYUh9/sfWmvTk+RWIsz1+F7NJza2/tSh3Or8T4BKVEINNq6WbiPoNQ49r3XozYxGYgnzuMmbeR9Xy5w2H+SWhPJ/gJgPu3k5f5uKw5I9R2xKOyIGEnVDS5fm8oK1X6vrauSOtt41ivUmYHCGZlz4w4pDUlNICIIFZRSr5Ko/cHBcwDF4jyI5Q9hSEnUfWQunSDOVlPCngigOzATplKJC6UVKK0wrK5wcfkU64snePraG1itN7i8zGA9g5US4WK9weqNEa8/eYKiV9Bv77Z48frruLm+wna7xfX1FbZ3W9zeXONw2CGXDGJGzqICS/2mvDdJCGQpYiGXBnGBVKYstwEXxm63c+u8u9stdvtDOIdEKGVC4VLVeGQ5a0+qtw8LfrUJ20pQV0sg6XVimAf33fYWe/UAP+33WK/X+NDXfQhvvvk63vzAG0irEWNKKFB/l1zgRw0WCPjSnbKNm8w6Qh6nSWAD3+UT054MZs0XUnLtsmPCDayn0OyBdQWbpLjEvIAAKqHW4vTZfIS+2zBwDGx6YD9a966Gvd6gTzeTqJTZZAgdFPqruRy5vkUdBAGA2aueFd7zIFXPsXR6sBqjVUt0s1Ymc41AHCNjPoE5SGFkvFd7rUWbbJndFLWKxWzNbWeiH0VI0TLVYu+w38tmu98aW5ykuBVf8NoA7y/bfYol1Vao0yApJxBGd87KdiB4mQfmvk3oFxD3Q+EsQaJU1W5CJ8XSDVkIPospe86hj9IE0B40iCPZaRL3QSXLJ0g8gY+rFYY0YHWxApcRZnSwXq2RCBiGhPVmLS1ISdRhYGCawPkgRJE76Yqs79AsXo+koCXX3YuHEFv0+/2EPJUal2p/sxFR6yuGqwVjv1bzcCUaTEjq/omJUQohJe3DCeopXqbV3c0Ntrc32N7eIk8H0CDeKwB32asD1/qGc0MCjkNo6h1DbNI1EuQ8rnHjPmudIaEfzhJqdA5TKyGS9RovJD25UQM4g3q0TCXyVMk9+fOi9yZ17XsZ1FoEGK3hkuUiL6dp09e6RyMIQhyHUNYCWPmZyW58IvQdk4zj57md8t4GKSe40PWy3DVqxgAHrOBhs3X/Ema0gxEBSG6KbDcaySSsLk9me7cAYKadhp9c0xLVs+60wFPYFM/2iyBXLrCUl6cs56Our3B7fYXd3W2oeHvaG6TESgFHT7+IDNTMtPbMCQk/rf8VoGThnBWsiAkDzAREwqDfzfcbEUCDmogsclhhiStKJRASDUg0YDowSp4AsICkujxihpzTslxoAoOQcYft3Q3GccRht8Xm4hL73U48ha9W+MAbX4PVMGC13oB55f20Xm1wcXGB115/Hfv9Dq+98Tpurq/x7MkzPH/+XKzzrl8ARfqtIAWzbEJjPGPzkll886mhxb7scdhPOBwOlcCwjIY7spVBQZU7KhtBJPtwlAjjkLyMaZraucMA54wEwkAqQaIAiVUC1KKnPXa3L/Ds8z+Pi4sVvu4bvhaXeB3rp09V8La9TsKg86juecpg+f2PPpMJSOQMWJ1ejN6VR2XU2mCEkPU6F+nWNmIzlVTar8BGsN3KAW2Z3q1Ngf1LYwyWvdgkY0qUjvhxCmZdMfasZQEXWjqr1P24bMRkmfmNeSzx69oCZQbrs6Wy+uO48Wf1k8jB9bOBcX3nxarlKatBDtMS97Ac3tsgFYkUQpsXGLM6+QJBPDFADek2bAtpOPxjuVYJKyzEmRkmMN8DM6oReIyg7mMUFK52gARSgLrB3c0tdttdVRdpdkbkJJ+yOJ/bzfnaWNY2mCVbNYXWCgUAdGs+XzxwDtN/18f+wN435zaUAyUkgAlU1KS+CKQWVT+W4PXCc08MU29lNQ7Ybe8gZtkDhnGNcVxhs9qIqjINSGM93yWexMVx6zRdQA4Ir91P3na7xWo1YrfbYXt3Jybd2m+5ZLmcUJkg6vrT+lpUgNominNBPRqwqkGcCbD5ZP4Ta3+VMHZOImzcqc5ZuUqlBNApIDOcIOBw2OPu7hbXV1fY7bZYX1zoeLJbYDZjGFcLBQmBTP6wuVzngaus66ijF3Hi3Ihn67ibI/asJZhAK3lxjeT922RRl2hnmFP3TAEEprLWu0tvT/v94C7+rP6eRwcpQZOzCDYNbaPuES+nWajZjA7O9rPDAACwMxiNA2BGsw77MpfGrRm7rwqQAqpEM+O1+hCFTbnE774UTTJbV77vVPeNtCKBMKuTVCJ/LpxLy5k1p7i7RRTzLkWI12CLjiD7Ji9e4ObqCtvbu8brgUg2EaCqz75Zs5qyWR2wshK44hZhpRS9Cj4c0gPcaq4FY3/rgGm9JsS2veoinrlwP18MsTqbCjKEwGaI+bsDeOhvuSOJUDAAVFCmjN1wizzJjbbjsMY4rrEaR+RpwjAM2KQLJHXUOo4DVpuVyIxcsF6tcfnkCS4uNnhy+QTb7RaXl5e4u73D3c0N7rY3etEisN/tsMs7AXMFKWuLcZE+zqWomlLHakigZGeg6v6fMR2kxDwl9SJNcs4plzqfjc2xQ5kOVM7VFxjPK1Kw3uIMxm6/xc3NNS5ePMf27k4c0uptzsLxpoaGJS+1ZbiqxLMcuJsZNj/8SWB+mvcBTOrT0HJtfKN5ZmMZvWeampwOpjavteamVIRnrP8vA1RkX88O96jrThkbzHhydMz7iSL7fGfFBPAOu6OBMT+WMBZ0RmUWwisHqV/0i34R/tf/+l+z53/uz/05/OAP/iC+7du+DT/+4z/evPszf+bP4O///b//EqVx93ciHgG9uuHUALaTswJTX3r9wXWC6XeHT3MJszAR+hx59q2epRIvE4RSMq5evMDnPvc5XF1dYbu9BdgUg7qZzXJGpxS9PVdBCyYZgYGipIObKigAsKqeSABQdFzgrA7Vi9ZMAbke5tUeU9BwkGIjONSUNWOJCeCkkoKZRrMQ+Oz7YrV/nNMuAuBsrCExtrs7DPmAQ55ANCKlEfs84eLiArfbWzx57TWs1xsAEK8WT59ic3mBYRjx9Km8e3L5BG++/ib2+wO+5mte4O72Fnc3t3j+/DkO0wGAgNR+t8Nut0XOGXm/rwYmU0Yp7IAmzaz7CVOewBMABRLfFSS43OxXqRTxQjGfNzLTojfCJEMnaicd9wF675alLhm8z3hx9QJICc+evY3VZoMPfOhrvYBGogkoVIEw1OKBBKhh2QLT4fXrp0oX/KyWrpBm9RAtHl1qV7z1duRE1Wej/js/tBoAKtaK6npYfP8qwz2kzj7M5dhZwNAZrzwokLEbykQfMZx42fDKQeo//af/1CzI//pf/yu+/du/HX/gD/wBf/an/tSfwt/8m3/Tfz958uQlS1vu1LohGDh+/yek8w+Nv9i3ncgaOIrTgx8n/v2D3wNgWwMjX8Ly5Ek8GNze3OBw2MvZoCY9C8fs17/rguEKVEGX2FWEa5qeB2C4xMbxudUNLjyK1CjsvXtrmLuvaXtKeAkDUQrP4HViBayZ4YBe3idedQFAzwABClwZRBPSOCKXDPNyafs5OW/8dlvzKL5arbwtq9UBRIzVuMJqWIGZ1RAC2K932K13WK1GTIcJh2GFUvQuq8MELqXZN4oeOnIxQ4l6vFmk14gGtZeKXmQZRIxZfwIIe6QU+qybvxCG47AXV0l3d3fY7XbKhTRyTlg/3Rh6/7b1jCwWmicxas+1La9DXoob6+XOIo1BkXVyGjSj8jEyobG+ZvJBXavmzXEApwqe5wU68SuEWXlLcWhWxTnEHsl6YWzPCY2aH6hr/VSaBxbzykHq677u65rff+tv/S180zd9E37bb/tt/uzJkyf48Ic/fHaeu91OFo+GFy9eLEc0c2sT2XHCo6+GY3GOTbJAPhv4afdWdNJ3BOEs9WLIw5KnIYFoAGUgTxNurq7x/O1nePaFLyAfDkDOXg6x+nzLGWWaZO9E/BfIXgQX5DKJFKWSi6shheI4fhVrQxFwyjmDc/EDxGUYxCyeyWceUcIwVJWeilxHQ++oVdqcUYjUByG5K7jGdLanb7lojAyo70LmLGqxCSg8gTkhgzHs7rDd3eFut3VJanN5gZ0C/sXFBTabDcwNVKKE1bjGa6+9gYv1JZ5cPMFms8GkktRhv8dut8Pd3R3yNCEfxNCDuSAfRN04TVMA2lr5rGXIYd+MPB1wt73F4bDDtN+7dFnY9nfmVCCaIxdTA5IYNSQnPkkBTm5nrl2YMB0y7m63ePsLX8STy6dS/2FAIvGyr0iN3pBgxt8cH+bFtw0kLICQaR+4eXCkIG7X43A2R9+C0vECzgk6BhTchj2Y8CvSNXJxm/85OfQpX3WQaRXBvY9QR+PhfdCGd3VPar/f44d/+Ifx8Y9/vCHi/+gf/SP88A//MD784Q/jO77jO/BX/+pfPSlNffKTn8T3f//3L7xRUPKzQFA3JC8X4jQ9HtqJnPxR4Fu447+4Y1R0MzeqBnQXybO2YbdchUAJsbu5vsbtzY1eEW8GDdnLQs7gPIlVHGeIKql6G2D1A0d6J1KhWi6M62bbwDcCylDXcZI+/AnnrW1P1cCj7a/QB1z9GpbQZgs5q4PZJOeaWkkLLg02AGc8L1VQLMggLsgMuGF+TvB9tSG5ym7Kk9x4O6zUJJyUIWf1JQiMaQDGgpJHuQZe97OGJJaIRISiQG7DXNSjw2Se0XuQyqLC9KtB8oTbuxvs93ptfJ70eg3xbi/eKtr+st4VCY0bCdlGxgCXEvt4E8J+IjNurq9xc32N/e6A1QUhjcl5DNIr5iNhYv/sTO51nJOvgGV5ikIiI2mNO62alaTlPi2aa+9rSR1ZbDasvEMWRDbStWcSia1frunaBMovtHO0JdpVppyFiL4NIxYcGcf1c2xDSqPYPiLNXy2Gk5B3Uhokr3Klt4TwUJ7E/u0lwQeIU+8qSP2Lf/Ev8OzZM/yxP/bH/Nkf/sN/GN/4jd+Ij370o/ipn/op/MW/+Bfx0z/90/jn//yfH83nE5/4BD7+8Y/77xcvXuCtt95CA1C6VI42vZn7RvYfHlqrnnp+RYIuuLgTyRIPCkjRUijKWLZwWsIbl7Cetdnv1e/cNe5ub8VTtTox9aAErUwTSAHKcjMDCFPZRUIBkBM421OpIFUqKBmYFRaCrN8lB4UopwGRK9RFqNcf2MEALxpKTDMDCUjqFbsZVQPFoLL0fiKgUJH9MK97Ess/00MWQuEBU55EqhpEQj8cDiAkDGlEnjLGYURSU+rVIJcCDkMCpwHDMGC93nibDaTSkGT/j9lVhtUxb2n61sKkhh2bzcYEFtzcXLnJ+36/x3Z7h+12q9+32kfxOIOpDyOhqH9GNMXfYvK5mdRPn8xhxvWLa1xfXYvJ/mpEGkmudkl1tUTmya9v8b84o40RsQMKlZTVjMzIo4Wy9lD8wprmSsyNcWnzreYdre+5rhFe2ZpD6/7HJNTKBNc07IXa3lWDIZ7lnM5QG6Fpu9SB2vyCLyxLWy0DW3qx7IooNrV9swhEDF+jRwMD1ABUbckiRxKTnkTHeXhXQeof/IN/gN/ze34PPvrRj/qzP/2n/7R//9W/+lfjIx/5CH7H7/gd+PSnP41v+qZvWsxns9lgs9mcLozglmH1EQWAiD1TJ/GDAnU/jvR2c6iQaP6cgc7Qb8ZhV+s+8n2R3WHCdrvFsy++jZurKzGxLpOINyVszJcJKFWKkj0PBaqwL0Vs9wRZHQs4V27fnaSGuqWUKpE1Q4xS3GGq1bvxadZyCE2/NH2lay41RLcG9z0X99Y8rRLMXLz8khJAuselKkDkCUBGYcKU9SoKiDf5wz7jcJhwcXGJfJiwUpP0C5Wa1uu1qPOm3JQ9pAHr9Rqj3YVE1VdgrG+0wIyWncyM9XotB5gTMI4Ju4sLbNYrNRGX6+e32y0OhwN2hz22Qf1t4yL+B6s3EmZzhFpP5KlD9Tp+A3ysr148x4tnb+Dm+RUuLi4wXiZMAwcW57wQnQwbMfUDvREDXMUQiKtKUsz16jzfM0ZM+3JMZqyjfMabeVml53ACjpaOb5zqiVltzwhL57GoY4Bjnr2s1I7OO+sZCGjrGjoGVGQWUkfCUosiHXmIa6Z3DaT+1//6X/jUpz51UkICgI997GMAgJ/5mZ85ClIPChS5kp616eMeB5pzQ3/Kvcn7SDxfoPF996h+Vw6OGNPhgN1uh5ubG+x3O0zTpGdgSiV+YJeUjFjBQaotpNKqHhxNgunrIwuiv56jkQD1+zG3Km2Le6aLPW7LWfLid2sLheey9wawGlK0rRUpzLwT5JxBRrEZ2NEW47gCF8bNaiUXKa5GlJyxGlfiJirLTcglgKRJp7VfAEp2MaGa85eiboa6cS9SL5HcRJKysnizdvdSdpvuxcUFGMD+YIYYLTPh566afq3dFfu5AcpSsNdrX3bbLYpe1uhAwQxT03rohrR1E1XNxyu/wrN10aaf52V7cLNU1GBc86KC2WlybX11SvXErvZty9a3i3WKcVwN3WRa49RzZZG5DvlwJ60EbGjM/xEtEfuBOdq8JlIdJ7WSXVjHfbaLgMxWv9mjlwrvGkj90A/9EL7+678ev+/3/b6T8f7Lf/kvAICPfOQjr7wOMlhm6rAsdtO9QPXyMuvS8BqHyMEEoziHqcPOgMUiLYtzwdWLF3j2xS/i85//PK5vbjAdDhjypIczi8dFkWs5/KyLqcWaOkdepxIrZ/a5TkIKBGcYhsbruqmy3EwiF73qIfTDbJ9hfjAY3idwC0EhjrWfjlsm6hc/pFqAkkBjEgEqGKFE7jR6+uA84bDb4Q7AYbfFYXvnKrvLy0usVnLx4kCqIgsrsLB4MR+HEYNaBSYklQhVbULs6rEYpJ0qgRUCEjBQwmoYAZXg5Lp5wsWFHDK+vrlxJVgpBbe3twAz8lQwDKmZ7mVhpCtm6AFjlfL2W7nm/vmzZ/iaD34Q/PrrYryi+USQiSDYCzUUCZ7FWZCo22mxBKuW22kiFw06+pVeTkgDx3KzvUipo6otido6BKAiIixsId1TCsQDCxGaEWLpK+LKGDR9pv/0shQQ/b54Vg+oUOQQFp71XPSxbM4v8ezwroBUKQU/9EM/hD/6R/+oby4DwKc//Wn8yI/8CH7v7/29+NCHPoSf+qmfwvd93/fhW7/1W/HN3/zNr6z8yOF76JmeAE6taqrPbPnxvXXoPmdB1+w868itzjkUoc9zjnyxgKN1v38avTP5si+LsSh1NQVyjR6TnsjVk95T+qk4M/mOax17lRwzVweii1zmcilL134crcGRirbqsPnnUmdFWDhWetsOZZA65uF4OMWxW/1CTrNBi9T2dHmtXwh43vYgmkJHEJZ455LNIBt0klxTXiNN9sT9YSQ6GsJLzpX56mSrmkir1L5v82sFv/DsVPVoKceu0Nmzdz+8KyD1qU99Cj/7sz+LP/En/kTzfL1e41Of+hR+4Ad+ADc3N3jrrbfwXd/1Xfgrf+WvvMLSXyWOL8pCD8uCj0zoM7Nr1/WyKu5U2j5mPUMWFv0Z2LyE+0v5Lry55/2RJFQJRLR6nOXc59uolGRfJuYpn+zRouVWVRkaI1ABKgJXlYZr2/1cWWivAEBgLBb6oe3n4yBnn42a9VTQdt5zNcCRhPY1SE4sxieNpVtHtyR6yxZw2PRvDSvm35bnyDtfz4vm7UTzOXBPmpN5V563M9II4GNl39umMznjBV6WFyvRpXnJLp31yZn5vFMoe1dA6nf9rt+1OMhvvfXWzNvEqwimGrIN2+adfi4tj1mMoH9ozEpRoxzbFp2NFy8yloiWRDVqO3Oiu6CEhJIZ07TH21/8Ir74+c/h7S98HtubG+Rpj6T7Uc26YLlWwtRmWAA1ZlUpmLUebJFVjjSlAVCVkKehWj97brf0yveMnAlIaOKeQ1fdsWjg5ItuYic9IUmM4IkikPeiyjQieW+XAahT1cJFFKwlC7FFAgaAGoMPRkYCpoxpuxfVBQF36zXGYcBms8I4jliNI/rbiFNKWK1H8bA+rnyYkxLDXHI1/Q9WM8MwiNXgeo0hJazHAUQs+1Opxnv69DVcXsoZrsvLJ1iNKwDAQc3TD4eDnsUKzAmruq+I2TmRuo/qrHZk/KQP8mHC7dUL7O7ucDgcMGw2el5O972MK2doP8pkiSq/ji3S+BmMen6up5XvgHY2oRHEicIVKG2oDER7RX38tBwJVK+88VJiJDVyor5F9hmNMywsPYvJTQTtmUxb08Z8fGmkmcVw9qBFY6eHF/Oe9923FKq0oP9Sa9I5V5EECAs6d6XvOpmXuZvmEK9XAPNnqq2Vxc2VUBFkL2IhX9LPKU847Pa4vbnB7c0Ndts7lDyhujqSg7pVDug/W+G/Sib1rEt8X4kv629pOxH5YVFSBGQIAa6OT0dUw+Sl/YAohXBTtoGTG3lA9qTcnFlEnHkbua7X0uRXQCX5VSUMATOrN1MgYEwAE8ok1pKNwMAFOQ3gcsA4jsirlQKItNn2rsAZZRiArGpBNdEVkDK/edz4cBzHEWUQX90lJaAMmPJBDTSy+08E9FDxaoXNRjy2A8A4Tdis18JM2FmsTj3lfddJObbb4+fauF4pYsYYxq45kQnrw5irRfV0t1zieNdaBPHDOZCZn4uH0bV795hDnVjuXWt2BXphodEpVqkSDlxWwY4ZO1KFSnmoixfmMs8fV+YxMKROpI610Nq0TN9eOvjiCIziSZU2ase8BCfyvgApVgnHNqjjPGnOaBDw8F6S9KJbp7NHdwF2ACQ5wQ9gtRpCHcOsjGsChJQIu+0WL549x+d//nP4wue/gJvrG2zGhNU4Ytregkr1bA0WLlxsE+I5EeFiC1epoyzsD1Xzb4ZcFMJIiVCKbShL/dMweP77w94n/zAOGMahKbdKRkughPC7HjgmAEOSPgMZg6GLQv/i+kykZhBZOHYmMT/nxHoRpIxjVms6JDl4S+F8FxGw32fvKxuIaRIjiMN+EEe0q5VfN29tTClhOoxyhmrcSFuJxeuHgScLWETGxvLaDYOfWypl8qvdrYV26DZp+a+99hoAOWf15t0dbm5uABZXTVyKnPFyQigEVu7oAqr8U5DAGAYSqRlAzhl3d3dy+Nj69dScp0q8K3Oj0zgMkK8f85uj67V4XXTrn2w0qoR7Xzi2ok1KOpXHEgG370vHJNpCychDA2Yzd2xdDRfr24PjkfoaLopPUHTSDPn3k/jV8qQvH4IGx7QOyz0tMzGlBF5gyO8L7wuQakMFpcqdRC4oPEfLtSyrpJQHDH7BSDNq5DFuxzvWwCYMkap/QA4Q4khVhnEYwj1XyhGWqeD2+gbPv/g2rl9cYXd3J3c2qVRhn2zWfbHm3QI00PYF1SwuCSk4gnWJhtT5KasHCIYAXqnWeMeDcKsLQmsXi5tfUn+42TYdWQEU/kkgcLI+J2cqzEBQ2qB7K6aiNGlKibFJzRQuF0zafygDOA/gnDGMAlyAqEXTMICQkVNSYNLUk1yaWFAJTByoKQtIDWlwtXVRn4tFGRoDwZQShnFwV00A5BZjPcNlEpD7zlSVbhpkzO2QtY0rpxRARBgiUpP3pKrBHIGEqF0rDijwSVDnQnWUK4CkEh1aSV4+5LMwBxXnecBiZZ2agqeBqjIabf29NKBR9UGkL3+O0PYgDYW8Y/72vkOXtjZWX6IFLxNz6IkYFTUAZ4VQjZMMwbG1Z6+D2nQp3juR3N6HIAXE7mNV0rf9ryPDVZI5RWh7YKtFUMvZdCU0eRguhD2QwuI5oF9iBKp7GdOE7c0tXjx7jtvra+y2OwykF+7FvagTm79xj4vj79Q1mlH95BFQMrnbE9tfmgPcqekXV8wDiI5zg+wMRqvCrd/6nktpCH7TFLA8+3qZIDOAFM3blXwaM6OSqN4SJeDOGVwGddxbzdCHNGIoIwgiheV0cClTVHACtuaVOu7flJIxqBRlxE1ASnwkSr8njUMY8wggNWNqZuqr1Qq73a7xaAEoAJn6uOl4Y+mqQUZSQJRzXib7VhPsRh1uGQYRqjm8bmPAC6PvOMXLv6MIhoU5cizEeNrmUwRUI8aiPBvXAJh2JkgpcW5T/K0PXWvDTaIQ7HDAcaDCYr1P98PsfGKXsueTlkjb0bwXos40JmfkdZqpnYf3OEgl+PUbnGZvm4NylZqjJ3W+59KkvXc6LPxqU3DzUohWUjc0JURw6cbvLWRwAfJ0wItnL/D5n/8cPvt/fw4vnj3DtN9hvRrFo4S6RPJ7NDTLeiFffRbbU4Gmu+WH7HoIDYO8FQ/dAsR2gWIBQAngbl/a1FTjKAtwrpJowzHiUxdUUNedu5giJ2ocvYGdARRIz1VRTFjLzzoOnGG+PEoRyWUYBkyHajgxDKLmS7tK7KNVIAAx4NCzLLHLit5t5XVkuYnYVJ8i4YgK1coehhGDGk5wKViPI+jiAmNKyNOE/X7vbrKIGSMpqBVGGkjVqMKkDEnUxuO4Aq1GrDcbPHn9Naw3a6RhwGTM2JBAKTmIOzbZsNwDArPXhLp2TbayiQXMPLKcCvHMYXPx4pmpT79bIu26ZkPaeS73N8BySwtVaEDk3hzODy8r1cwYnC64alU5/4cC0anwHgcpIHaf8YrtOakoVaHG63v9vlHoQ4tASvAr1+eTIcy2um/WGnHYsrJ0lkc+ZGxv73B3e4vbmxv1al5kz6fY7azsxK0KDIE49tWNtV7iru23T7TQviA9ESqnF4HGDohG82vr7mNm2EvPjGhVNVl1k2MhNR4lLJVxxToWpBJkbCvXuCeN+YMrKftpAGPt02oqA0KusrOx9qsLdPz9nfU1wdV7Nm7ZQQoq2QwoZUB2kCpYBckjEWEcBmC9xnq1AkrBwUASqPNL22+qwkSp7lORANa4GnFxeYFhHGEcRpT8mo33KkB1X7o5Re24xdHyWulEYf+p1oge73RorW47BjIyHwtzryWoQfq39eWLIZQR10Yo38uLuMZH5SUHuko7Qh1C3U9LkucRrjgH5rB7f1r/9DVv+9TCJNQ6LrQ29AkvRzka3gcgFaWh4zxUHWOCcTl1uS0QzTPLjZ+z9wvMfyl2dbhw1YlUzcO1kqImAnbbPZ598RlevP0c1y9egKcJA4CREiYGkNUFUCyEWZ2zLbfA1h6Ftbhc/1IntXYZAeq8Vba1CyWXCyIAiXNa+S1GHKbiasFsFoywqogmeUlbrJ7Jxo7MgWnPaUT7MXnm0g2Ea3XHqH0dOvUoiNUnIPtr2ScyU3KJJ/tACWmA7/00NSACFQIQnABr2E6HSuB07EqZ/GgAUfL9KFPDjasV1usLz3u9lhuH16sV9peXGFPCraoauQT3RkQY0yA+BqH+/gjBZ+CIi4sLfOBr3sRqs9ZLE4u7KH75sDzD6t7nkbkapDbg5SSAlwsGTAiqgpbhvS/cZ7DRlDXjaE/mjHN7YqkOEahCDWZ9fF9NYt7ka5vC767QdxDeFyDVh8Zg4GikyMkxgLkjSbvAWxaSSRA2HEs8m5YZN5gdCoVQFuZ2Ix9Qh8pUTZNJ7ig67A64ei57Ufu7re9D5emAPB1Q8gHEGdFrqEkgRPB9jiphKcfHZjxcQg0ROD4xO8iuMmR/l0ByCS4ISP9/9t415rLkKg9+Vu1zznvp7pnxmJjxfMHC5EL4EYwNioXED1u2hAcJgiCKTIxEhGWjSIjE/kHiCBB2ItkKBEUQFBQpEYkEIcqPOBFJiFAAGYnBwiArEkIRJoYk32eb4LFnpnv6vZxd6/tRtW5Vtc+lZwZ47a7W6fecveuyatWqdatVVRnEdk8RUBfcRTN2Bh21+A5KQ/u3bMItPLtuvK2h6DNlTEggjRKrpWQxjVFDg52mSk7PrngP63manJ6pmrQf1ypw1Ho1kMvaYo1EpN5Ci2sFTli37EKVlUokXE6fzyiuR+TKHNQtlrCaUnU5Jpxs1gBnXF4QwIQs1yaAsJoSVtOEdQ3QoRp0IZGD57fOcfvObTzyaBVSyQJ8Sr8iexRjonsmyGqeMUeMyxqGBpTUdsrYkP2u9Rmd7mDSjTLUntlUgmXa8v3vMvzuLrTGwimwWT+NHxgdGsQ9pS3ezDWgu1iFOTdHYlOetWuICvMIdR4lypcGPoZQFauSYVdz2PJLawOH6U7Nsz3pC05ICe3UX3XU3HAKo66JUPE6ckF1v6hbhN29KOvDKUTYOf97rTXVQZ1qGDBzierbXl7h3vN3cfnCBa4vr7CqlDRvy11ReZ7LZl7k4V0zpW8NF9GFauUMsUwQVFkFWmGejsmjbDQuUU62JqArAuw+DuO7lUsRUOVXZjlLsFgYtr7FtnYmwjHs7Dc4neh2OBlM7oAPx2BdRt271tSpfcylAKd6sCzbdRqHKNWBbt1f6b8MgWc2KSXkeVOv3JiwXpUDcaeUyiKas0ZWU8KqugyBItinaSp3gKWE07MznN+6hfNb51htVmXJd65NURsaZMgZkr9zP/eRbZ6BNfQo84k5CBjS/5Ya7MByQzlQEMiP+SjxwvfmibO+RwJK1z89fCMF10UG7nJBt1Q6yunkTZNKifY5HfA91u8EVKeO+zEzTgJEHA3Qspi+IISUaN3lxGIvqgnAtl2aimXBekNtfB6TXya1iEEfPbQHRj/BHKWVAV8h1YNLpf7nn3sOz3z2GfzfT30Gd597DvP1NVbrwni2V1fgeVv3O2UHGYp7Sud77HgXISc+dzepw5oPykbY2bm2UtXQSrh2fZbi3TPm+kPZr1W73WuwMen9VVwYfGZ3p2NdrxE3X2Lb21Ng9UEQ4h5sJj6VnKYAe2FE4a9fU2tdG23ZntnlDh9LioxsmqW0tORfFRONKmfklMvFjCgnVlxfXyO5UHQiwunJCa4TYbtFPdGcMVGxpDb1PE1KCav1BrRaYVqv8aVPvAp/5lVfgvM7t7DarEET1YEGBNW6Z2ofvXdPBoFNbBnl1O3wfm+dL1VygMiTQ7SKHXzlweG4aalMiD1T+0WlGy6knECSNLRol4xi7h91SXTxIlxEnqk1pKvnC3qNSQSnUaBzfRCgR7gwM+6/8ALu3b2L+y/cx3x9DXMxZc+5A5xqaAgeqtVRJWrNFq0Kap+5KstyWdGguTmGxdoarA2Ji056rYLZV16zup2i0qWcWZfW5C4jP2TM9ZAOZ1HoxXweTH8iQKMiNjq6/SI/RuPnratR1s9G5BjLmgD0rUtElFKICP8AMpuwzrlcRQIg57KvKs+z3us1TcWikquU51zqTVOym3ghG7eB1WrC5vQEd27fxq3z8+ImrpZGbka3PbFAhmEBnfCZafBMFuGTGxSPxs4oCvbmAmfsABpYQs5y5pY2OyB3JzfFxyA4ZdHWb5ZhO6LF5aR+9jF9w4PhgV2Yp/7pEGpqMr2E6QtASNUkEzhcH884JBR0V2LRwlGuFC/MxHlcg5a8TDxWT8nn1uY1vHreljuCcs743GefwTN/9Fnce/55zNdXZUdFrgKqCqlUrUBamh2qIFbBohaOE3AdPYoQYoDKBlNkC3LXSeZdMf6eoYyyJpTZoZ4jZas7B7ourQEXlTFmZszynivO2I5xIo7HMVGuARL1yvkSZef3oImdkmHxlC5VE6ldO+r3HKEPj2aUsP/mepJWOPl3wTJrmLgw6bCNTYVGrp4DEVKM7fVVCSWf11hNE2iacHayKUERALY172qVMK1KkAQg/cg4Od3g9p1beOWXvBKPPvqoWsZ5niFHO8q1L4ggdfJ/J4/qKqB6xqRzEWt9FCo3xWN3O/1+xWabgQdcJ0hU9g5JnYoZ5KITs+3aJ7uyI+kWXoxhicHv47Socy9lbkNjBpb/grg/tJUHTjdcSAG6s1+IMwgDYbbLwxUI2rtoAN0UylUwCaMjz6Sp4SRwpF9f5UrDzNBTCsrpQlU7pgzOwPXlJWR/zb27z+P+C/fKpKsMr0R9zU4QU12rYIQT1BYITOHVfub4ztXBgrvK6u15dgK3ar/UsnyxGFrCpmKFsIVHZ2dJ5erq0zNv2Y+J2+9UIQmMrVoFqVp9lMx9GuFz0HRarUkZGSdMtg5lQjrWA8iJEAaDZBF+5fHAs50OIic9qIu0WlUg1D1JttfMDj0xASg3Jc/zXC7BTEmDZqZpwmq10qOY1ut1DbJweEkTTk/PcOeRR3D70UdwevsWaLXCLP2divDmuq4W51Yc9xG7Uvf7krnlF5Dc2h012bj5bXXX9411GuBZnP4t23Xz39e3k38M3rWIaKwThikqreu4tCcwLDa73PcG1iVX8y4BFuaZ/BkVIJnHhWCJUM/JFP+KAqFKVnvAwCHpZgupgTXQZAjMdpy8/tpbJGZ0GBNRzU+ZMes+HC1O8W9oSr7aKIKZsXWW1OXVJa6vr6DikUivdqd2biFqTkt7K3RiaIk+lQnUPxBXk3rY/ETyE43tua3pBCigx/I06zaeiDtvJmR/kY2501fdZGQgJST2dmtk7Dv1zApw6RKFMwESfLEWt0nXq1pNWa1m7XMrKNvxclQrCo4Iu9oPf0K6rOXZBmKEsHWsJkhgTpoS0hQdiat6aO3J6SnWJxsduFzxZkEThzOWEdnv1LnFuqQ9VsDhIOxMh4eIL5c/NI36Y/TaC6gXA9fOdskJ1UEbOxUOV1kUt065an4tGQDW3mE4fHG+sIfpYXqYHqaH6WF6GdPNtqQ6jxv7xxDrKMh6cZmMnRNBIwjSn4urKgRRiuZOvp1GC6lumhJ80JkV6kRkzri8uACjrINc3L+Pi4v7Jforzyinj+fgKhNtXyDXJtWV5t1KgOgu/oqB1spRTPmue/zp9wUtSCwDqgvuTTa1xKqllH3wgzMyCGVNZpZ1qHpdRVjXc7AZZPXUbpb1OsKkrgbpYRnLVtsjZJSFGn8Qqse1c2uEqIryJ5HAbbbqUJscWMLt8+AqUUIHdN3MmWuZubr7rrFaTXYYbT2pfpZep7KPSy9CSQnrzQnObt/C7UcfxXpzgmlaFVol6e5+C0qt38515W12b9faTGH/pOI15HDuquCG6hAXsNa1pLC6MfUlPHTlC3VtBb6yx9gxS631Wwg9Natn7OmF0FOnlHWWEAWI9ieOs1jrOaj8cofNs2LbJPQQXgB68K4/ZMDxsX3phgsprqvL5WI7cAYROyZRV2vq3T4MxHBzJvjIpYLCOLFYGB7KidJlzUHM/VyZhh8oq0zrE66M6HgkFHdSIsKcM557/tkSVp4zPv/5z+H5554rLr/5GpSvAd5GCAlASiqgxJ3Ezv0jJ0dUyqlCsRKQ+qEaO17X+axDGeX0h4xWWEnD5Ylc9WGMycL0tS6Gwpdd4ITJKxsJqn7vzO60BipT3C/1SjDIRFN5VwM4kigWFKc+o5yjyFYlGND8Ze+ZTDDn7CKbeNInu96k0IVcOaHeaHXDVTpyfD/nikXBYXWt6lh65JKcaJ4Cg5rzjOttcQ2vVuXSxdW0BtEEogkpzcrss2t7tVrh9JHbeMWf+TP4M08+ibPbt7GqlxwCyfwsjr53umhGfBXkNBPJ5PocynPIHo4M2rmu4tZduMkx4PON2mrLyy4/ceyM7GVcShwYAOk/YdolTxRacdddq26Rgk+ubK8C+AcOT/sEAPc/eoHe1h9d6Cxtosy1jFzXUSf4QIyJZM28zhep/ItCSAHYJeH98p3XmDxxe8Ia1RcsLuoZ0lIEFwgg2WBKkSAlf6J6gkPOuLwqN++WI4UyXnjhBVxeXWDOWyQu+1zq0dwielEuBGzFSQBBp4qto7G272FpUcCOiMppB3HNCLCNycQeo/WfO/BXFXI4rXtEoJXy921mHGrIznqEGxdO9fQNJD0J3Jfx63dyjxVSu37l/lYA+n1h7sbdOtS2OF65bkYHuIyRCgBmZ+m6cemYSnY1lKHKOePq6gqcGWlTQs3lRImcM2Yu5z5SXZM6Oz/Hq179arzyVa/CK175SkybDZAmzAzMJNsAqDIgBij1dN525gFSzwybqh6w3kJMA0G1A4YHbupFlNxVo3KnBbx7sdhQ6gOlxTooPmt1AVMuejhH8NBhwwLgC0JI9cncA4cO11hACXdoebhv45BqVUmWB5XRUWWc+XrG9voad+/eBdcw7KurK2y3c3FzaRRB/bjvuwSU/F2KwSqWATVaKLstX2Ld9EENgl357jFIaA6dbZisF1Qt4S9NktFC92jS6DUdbDDlam0qbM1hwB4uZq5C1wmlYOm4Mq37LuxoNNpRz8zCQJkGP9K09/FZCxiRKD8iwpxnrOrp7BL8MM+MNE1YrcsJ6ienp3jk0cdw+84dnJ3fQpomZCI9ikqOQZLvYyqKc6GbcQva8jC4Z8F9xfFVeR+KxZc9DLHCkZJzbBox8X1pV7u7Iwh3A+HnehiXkUbH3ZfligEX+MVVQYZTImrl1JZaqi2mQ4NPviCF1IOn1piu+3CY+9c7kjJaFneje1dPYJgAFK8NYXt1jYt7L+CP/vD/lkNBmXF1cVGvMmf7ZIi6gp1E5kz+uHLTHr0Kf7Gq5hEhyOzuTWVhWFHnXEJJrpcKpgRwTnq2nrfIghdo2A+AqEbqsZw20cAq35Ncj5G10syMqfoTs6zTEJWLAFMq+HECmgJ8fj2trmdFH14DK6EeaghMMCuNHRNx8Asep2QWqVmSFCxUz6R7fJl6wDxju70Cc8Z6tUKa6j1UUzlTbb5inJ2f4+z2LQDAo489hv/nz/5ZvOKVr8TZrXNswZjzDEYqbmRKSJSUetrU8P7+5RGJmr/h3WAdV/njgNG1qsI+cJb0h32pp0WbE0v9OQSeF5vCHHeD1J8KYWrhaHSjY78ksd8nV3rvPH6R6aGQkkTwagP0VAHHxMvaxpicx1r+gLOowJM9IeXK78vLK1zcv4881xth82i1jNGSgwoa99t1ArLwbqHPHjICiN2dUCKcnGCsz/2zuLQduyiMXna8crUw9rvImwnRuBf2aV2KqWDxZWQiPXqImSvzJXAudcoGZwLUXSeBI1GgmEDrQBH8k+X3ysFoBveWBJugVGWCQl7wGPNq2bIdUcVSj4SsA+AN4ez8DLfv3AEA3Lp9G+e3b2O9XgNU3M5ZlSrSMXAQ+i7vToUQYLS3P3UMfeSSdo23LuRhnY0ZL2CRHzDLPK5EhzFaGBEf0QoMx2tZJ7S6JWreud8L7TyIdLq0atYqpqG94W+hHnZ9r32m0o7+XjbV4qMBYF8cgRNLqdNyF+ivIhwV6eFOKAAgAuVK6BSF1YPBZXDICctMCVeXl7i4fx/3770Qrv5O1UdU1u7lzIdaCZtwIkdMPRGa9h8ZaSVDBkZXSID9pt3afqMyRdlbfqVqHcodM1Q18TBRFCTun1FP5uUkif6YB9k0OyR2lpPDi/tqqpYUV/dXShyu1EguAAXMGnVoAgphDauFT8ox4M7bjXlH5cM6H1uAhjELE6S9Xit6g0QuemXEUjlENmHaALfv3MYrXvEKAMCdRx7BnTuPYLU5ATMwZy4bz8Xik/+8sGp45BAPC7C+1Gknk3OChdy8A0wR0KzLZmojoKjLpjhRhDlrfwE8L7RsfvWNcsi/ywZzYmr4un/Y9iHwBsApOpZUl6X6vaEFOZWkF3zcw3AEWdxsIVUxIgOTuGMLIHGhPKBckUROoB1dVmBFmRDEXK6bQHE93bt7F88//ywuLu6XhX9GvYpjC8qzXsdB6obzWyt9aDWMScqPXAkkteTDQ4ZmBa3O7AhMXIDsttiFyD2xUKie/1YPW2VvhVI5vogYiKcO9fAMaVk0OBG+3irxEy0LvoDMGWkuAiulVNwVRBYJmrMy40SkblAfEp0E6aMFEvdcx2UEe9sVAqbieVQ6SSKonSD3dXUCH46+qQS6bOdrpJmQ5oRpPSElYL3e4Oz8Fm4/8igA4Pz2baw2GyCloqakSRUC9SzTaFR29Mf9zeSesRe+L3NiG4NFE16UuqrA5gFwnuEvwS349xPQz/d9losXe4FiapCPwdAqOL0ocOJmD9Tylq3VSttlrhrNSQ0aodjBPVDamt+JyF3xo907ON1sIQXX6SWtpeLRMzBN7DMtt0B+5sHonrBDmwsMy8rJUxFQAOPy4hIX9y8wX1/rWXE81zP6qvVUj3k1xizt+8q1W2wUVomuPRM1ImD02x07JPWwRO4BhRVK0wMWWq09cf3pmo7UXSd38OIsCM3WBaLD1rrMaoCAD8hQS4XqdoNs12cIM5Z6CeUSyVwjL323SDEyClSJfZZnrZiKZ/YZfcjJ7r531LTf4qqHoO67qYM25xmZs17SSFSu5VhvNticnAAAVpsNaJoKrBoZ4dWfttGWi0vuHrDQGwPrIBVv0c3na1+SPd5rsKc9pW9qhg5Qrhzm2E7gqxemMXjigVwNPbgW/HoxB8AQajCad/AMeNASqC0MIqas7TpvqUexF1QWSGP49qotdU8q1yMjqYeBEy7tDmlm7Aqf2oXHsR6xD5hCEOW+H0beznju85/Hs5/7XD27rwqp7TXAueyjQjlMNgWOXv7TMHuncYlQcdkKpI2QgLNGFkCNPnUg3NWgGtegjsxZ7h6Obq4GYS0zlixBYxtE9rWphNg3E7AKqJzF+iMXFl4ASU5YAFT3VpEyATt+qFc6XEuoJoxp1lUjMdT3d5EB9TBcGQcGzM3CzqQKX6PQJtncawc45Zz1HD+9RoUAWk2Y1iusNxsAwHqzLu8paXS8uvpqvVT3ks16N9XucVhKS0d1vVzJL/AP4dEvXlSQU70OS0FYlCc78x/NL45Ih9Y9gjBjEMi0h/dJFp/tEAoRxezQ9AUvpHQR3OGEBt92JVUwS4Wx/oZhuBdo1CAtLoM05xnXV1d44d4LuP/CC6FyWY8SF6YSg1v/kPbLHVI5NDJiZOSKGzBeAFSVsk0Df3LYEBmsuPpfihaP8lFEZhWslcrh2Z1kz8CCFTiYDNUQk6wZJqgksk4/XPZFZWVSsik7l9tsqVi6mes+K9UiR9OwtZpIkKSaqamQUVCJdc1cBJWeTKISbkGr1fISRcjBUmPOYM66766c75cRrMz6kbU7Rt2QiaRIzG5tYslq2pVkTMNv34FRYqHT3UqUy978L22FDCPgAkyqkAkRe6bthr1j5DKP1CIZ48jWu/0ItHnZ9b9/PxLyhlIJrtqtmNt797/QjZo9rVLpB9FB5pvhWFSzs1PeXLlDKekLXkiFFMZ8D4oOVSmOap+VERGVqLOLiwvce/55vHDvnmkYzOW+oOyFlEwgYZPGchmM3AgpEVR+83HRjp1wkNO3Gy13ROCLoeNOWNYmlLeocEwoIeg7mIa57OD6V79rUQ6TQwWiSyQgscGm2agw5LIRWvZ5lTzlMsekukWejVFMqboAq1WR24gx4nK7bYUgMhJxbTLq8SIlmETdUkmNUx70idGjy9iL4Y4o6X1kAJxwmjHPW0x5KvdOcVZakWtRmItw0nWoVAQV1zyyNgUmRA14vwCJvM1bLZFufDLBdkD9tZGRMAl47Mx1FzvbuAjLsyZ7MybxJQJvoZBriXcMgITNdTfBeq7fFhjWO2ixw7Xxj/BoqT3tX6xHZToD5jc1wdfe81IUM3Zemd3pi0tI7Ur7aOpFpeIyIlmQR1lI3F5f44V7d3H3+edxcfcFrCgpsTNlMMndUXZuXaup+ROwtSv1roucM9JqFdY/pJPlUTm+xFthjBhtJsIufOTVgMHI/T1EKxNS9aSC4npCqGOU9mqBlXH6dYMsFw/uSdGK6YWYWN6yz0vQkJygN/y5uuYS2cjEYY4z52pEUVAetO76jpntdtqGSVCyqC2G7FEx2Ns7r6Rv2+0WzIxpmrCdZ2Qi3L17F+fPPw+gnIt4eXmJtFqD0gri4pOjvJjsBI+UpsNlRpMIYwN9KS1FUS7m3/FmkQ82a4PSrv+7lMj9FSWih3b8tKvkTyB58Tj6PtLj5eQaLzvlDzl5NFIu1EKVW0yxPPdH6aGQwrKC39LRyAOxOJlaRuPsXW2vrh3M2y3medbbSdsoNfsW1ZGhkHCCpYexf7ZILG09zffdHKvRDmmAJ94nilx5V2sMoiiMgI+oy5rnoE1378Jfa7sIMwqyRL6r6w7yu2qqOu5jWpE6hwJK/vdaOjnXkbr5XJlahwjaeZ6R6n1T8gHKJZs5S9RoNAi4dsyR4gPz1fZSx0PSsYJqsZ497bbtHKTohO/m+9rlCg1BM/I/HdbeS5nG9ttgNqtOZrMvOuHt68Cu6mqU0g8yog+v6niYHqaH6WF6mP7UpqOF1Ec+8hF88zd/M5588kkQET784Q+H98yMH/qhH8KrX/1qnJ2d4a1vfSt+93d/N+R55pln8I53vAOPPPIIHnvsMbzzne/E3bt3H7gTXru0vUyD/ddHutNjZh9+3b/rPhxVDOa6QdStEc3bLa4uLjFvZ+R61cJ2e4153mKetyV8mCX8vEavySGzdWFc1rmk2egPt7YB0YrIWQBmMXTWA8yltN9SiX2XiDrAzq4zV6HVL3mlTbmZV/rk1wzUxRfGlrRfumGTbfOm/Ib7Le5WGlhRHsbg3szlBuF5ztjOGdfbGdfzFlfba1xtr3G93eJ6u8V2Lp+yDjSHOmoDarqGthQfzlySQIu6lqdxFxSxLclcqxSeAQDXKNJ5O+Pq8gr3X3ihfu7j/r17mLczJkpIadJoPgEj1aCXojG3dL6AP+2nLeT3dILwt3Upx/exbJe3+ZTxy/XE/h5qBmz/Fy30ZlBvm7xlYLTm847wNZpJ3BrQO9ICv3HzymHK3XYtmMjg7qRjCiS32H/Ha8h1KVpIuzsiaDomHS2k7t27h9e97nX4yZ/8yeH7f/SP/hF+/Md/HD/1Uz+Fj370o7h16xa+8Ru/ERcXF5rnHe94B377t38bv/iLv4if//mfx0c+8hG8+93vPhYUS1SZlmJLvvTdM0GGcOuqJsfsfV6VOwtutj5Fxi5lC4MsC+fb6y0u7t8vbpd5xry9Lp/rK+TtNfJc97gInFUo5Tw3QgqBgEJHPBTqdovujTjho1BQ4dr0WeQF64fLR1pVPEXBxdnFNtU8ZRJlXcT3QkUyF2HBNWSvdxyoatIwC2IbZ/8phaiZmRIoEAVVZsacuQin7Yyr7RZX11tcXV+XjwireYvruQgoOew1Zye0fV88jusoM5UPiMo6VCoh4JSSHOXRuQPZ9SPVI59aYZXnjHnOmK9nXF1c4oV7L9TPPdy7ew/z9RYpEaapHHjslZ4kc4uF7cV/Y6Zb0bvzdS+olpk6B7wZTbWKmmfPtn5a2HINEhF6RfOXdvXE+mOKjoGn9BXotoGnPTFGaN3hg+p6ph/D5eWEMUaVlkAeE92YqbpI9RisoNy4edIqwQI+oqqocO/BYejCgZL56DWpp556Ck899dTwHTPjn/yTf4If+IEfwF/9q38VAPCv//W/xpd+6Zfiwx/+MN7+9rfjd37nd/ALv/AL+I3f+A183dd9HQDgJ37iJ/BN3/RN+NEf/VE8+eSTx4IE0Z7IaeqAaBI57MYHsLiI22nW5p13OdpnCyAJBGwDmKYEnouQ2V7PuHf3Lp757DO4f+8erq8unUgV4qyRaLkQOSnR+9MeesHpgynmuQiGGAYuk17ClGdXnxGxHBQ0s9s9UuFitnb83TGSEgE5x3WTihHArSN1JYVJKh69dq0moApCD3e0GJ0lcYBS4etjoC4Gs+KzA1P/q5M1kZ62kad6DQuRHhMl12W04pWQ1EICRYpbmsLtpmBRftp3Ihyvt9eY67l8zz77LK5rNMH9iwvcecVjWK03Zf/U6Rk4eSpM2tEjld+XJNl62+Gtc0sb3L9P3RFb+9NImR3x2EMhHah8L1E6AlcvBwxUtm60vNMfrnxseknXpD75yU/i05/+NN761rfqs0cffRRvfOMb8fTTTwMAnn76aTz22GMqoADgrW99K1JK+OhHPzqs9/LyEs8991z4+MT1P/0bXA7yLNZJUtB9oqYdjHkApBrUQfLf1RsXTKEM/vrqGhcv3Mf2ulpNjeZpbN2EjhdKQePH4Ltqm9VSqdr9LJr1nINA85bDsC1hABSZQc5sIc9d+xxcVIBnIE4tFc1R+4xgwXlYMnO42HEkoMJYDzU2igp4M3YeVubqiswZ8+ATXCpiMQp+ICJfejXAqVOuPKV0k9pp1q227YNTOiGd5RqPLa6vykHG5VPcfpeXF7i+uirKighStQYA2/4ABKRx8yyicCdTasdkyXo4VEB5T0CY950VgcHfMa7FQ2NWEtunFvSWhLhl4eprWIzB5eevvFMdrLWyRh1eeDZssM9i/RzkGbUtU9V92naWeGOLk2PTSxrd9+lPfxoA8KVf+qXh+Zd+6Zfqu09/+tN41ateFYFYrfD4449rnjZ98IMfxPvf//6j4TkOKQPmBj5MIC3UZkSply8UdwoBPAMX9y/x+WefxcXlBa6vr6EXrjKXjTps5/YBcyHnWmkQLs0E9L8zz2DkHhnVAmIe3MRX07YeMjvPVWBVl6poSbLnxt+RW8W5e1ctE2lWm+dhq8b3soJpMs0J4gOYV7SkCEACcz0CaISOjsHUDa4DYQjYOkJKjFVaoWK0YKAyLHGlBA+lmNcoAoTribSp7qHapwaNGLyG44ugD3mKspLnGVeX991ZjIznPv/5cp/U6SlOT09Ba8JECTNnvUFZ18Z8m/JfOwwUfwhax8zrcEv32EQqoASS2pYLNSTQQXcidm7LUIDbB/ZmULcpK3YglxdQL2siG7AHwfhRrjzEeaRLAw/Q7o2I7nvf+96HZ599Vj//+3//7/JCNMaFcoch1U0h5SRuWd7f/FbfRItruVqfj6v2LxPz6uoKl5eXuLy4wKyXG2a1ECD7furZaxholyONXJuv6xoMsXbi6QNiGZR3dTsWx49aSBzXjXQTKEayz6wwIGqtRcg6WAfWGkQIVSEYrKemz12f6qeFpYdNrD/7iLVkMFS8Vb0go+wrmsGFgYPLDbYs78y4KFaaWGrGiAWfs1q0cxTy7G2o+mnHdPAp72BKjlgB7pNSvduLWQNztleXuPvcc3j+2Wfx/LPP4vLyPubtdY3V8FfD8IIWvaCuw80NonKqx451lgcNNV8s15oxAlCbXazPhWrEmuyqEOEdGor1hrj7+vHKo8Fq9SzNpUHlgQ/FNw2P8nhH2aHpazxGQaDuX49WEbgvpdrxklpSTzzxBADgM5/5DF796lfr88985jP4mq/5Gs3zh3/4h6HcdrvFM888o+XbdHJygpN6KGabRnRq6x+mQS1LFAuO2D3dGlVqNLjkvzY+2ErYVHWMy6vLKqQuy0WHmeNt4dnW2Pod3iN3kWuooRrPePWxTI4d1GRuKxFIbi1pYFkAgnvTF0cL/SZkYfmaVISGZJKrI6IlNQRd6gzCKYIhAi64lQjIKPcuwWnhvq/ZtV0CdWqfGSDmcioF99d4+35me1BSykajta6ghgpXhFlKEU/xnitpyxWOwoEz8lz2SV1fX+He3edw97lncXp6iscuXok0TVifnWodxBlyNxhhNFpLHN6E82gGjVLLMEeRiqO+exy061gN+nZAzsvvOObYy4CJdf4HfNVxXaDcru69onvIgki/tZmENJoZCnHJN2B2MBwqeHo8tfzruPpeUkvqta99LZ544gn8t//23/TZc889h49+9KP4+q//egDA13/91+Pzn/88fvM3f1Pz/NIv/RJyznjjG994dJuq+TdaJ+DsoRdpShOAJAystKrPSd/Hz9jSKjbvvM343GefwbOf/zzu3buH7XwN5lnZIsOrrNocMggSIySRStmZ0fbx/4qmnvXvjJm3yJhVEMhnzvGTdQE0KaPK3gqoB7da7eX07e2cseQoZVRrBGaNhfdqvZD7VGEh/SQqN/Gi7bfUIR/v2jMm5i1RbTczWPouOPGCCsA2lyi/mS2QVxyxWawsYo0WA7VwDWi0ZqBy3WDoCUlnwEhUor9KQIZ9pmR/qdKMjHvGXD5cPsyMPG+Rt1fI2ytcX97Hc597Bp/9v5/BZz71/+GPPvMZPPvMM7i+fwHezkhcgwrBIJ7BPCv1aaTaAcnjoB3rlyL5+TdRmafRyuw3pgRr0ExgB9u4D8cwah79q2uwI7rVzgjt7HXXjBp1/RHCNW2r0Bo7xbztqG+T4mc03/pPJ5pVOAI9WIekoy2pu3fv4hOf+IT+/uQnP4mPf/zjePzxx/Ga17wGf+fv/B38w3/4D/EX/sJfwGtf+1r84A/+IJ588kl867d+KwDgq77qq/C2t70N73rXu/BTP/VTuL6+xvd+7/fi7W9/+wNE9pl7xicfSVbJ1KlUxyUpr0eCDLQO++40c/+SvXFcCOPioixWyyWHRKjRe0WPMRYNa9f3wLk64xv3hD359BrcEjYC8wZpoISAEYIoWouGURllgTEQa+veczB3bYJRFq7jugJrDrL1Bde2n6eWBhbFnn7L2HmLKtqrMdyWFe76IQsn1xID93R0mch1LASJ9OtpbXBArVjgREVQ6bww2EreHOkwA/P1FS4vLnD/hXt44e5drNZr3Lm8xOqkrpFhBaJiTQplNijamxbXH48QUrvy+tDnYEGVl/W3N6VRnSE2il250MBSw808R5yFS2Vby6WvV4lvT6p96ySCe08Ik4EYdvwWotVkwskT3fEaPndA9bwntLcnHS2kPvaxj+HNb36z/n7ve98LAPiu7/ou/PRP/zS+//u/H/fu3cO73/1ufP7zn8c3fMM34Bd+4RdwenqqZX7mZ34G3/u934u3vOUtSCnh27/92/HjP/7jx4ISpDOAyuljFnXLkPxqBNrCd/nltzb2BvR+AJUJUfEHy9UJ9+7exdXlJZAzVlM5jU3vOmK3h4tSDUFv+uWYrbivtNWhLzuW3ccflDmrBlUtJY0W9ILQSN0i8jzrBaDuQbhybZt+0lsAgFfuvItTr6FwMINjvhZfrYbtsDKEB4CuwwlsFIQPwJTAKdVrLqL1B1gQRaJkFyfWt4lSudJEbsJIhebItcOKjfGs9iHV5JhD6/LMXDbtik+Aquvv8oV7uJsSPvtHf4ht3uL8zh2cP8I4SWdYr9dV880geFZ/REjxkms2ZDlegRwlC8l3de9sV69RDhl38k9nWQQ38p62Xt5EIB44xkQoARpMIi7lpZt0HySJ8ug3axwv3sbpaCH1pje9aS8D/MAHPoAPfOADi3kef/xx/OzP/uyxTS80iEY4RarRXedcDwAF6loPmkmHRmsY6e+Ov3Qj4CwD5bRRDZYJJPf9iBU1NUIKRHqrrG/bLAIPoVwl4QTC7LRDwQ+bxmmTuL+Yj12+XK/4LXlSCb2tmnqp0q6JMKEO1aJEXokWridqsxOUOrFM0OmJ4sGU4GrEuuCH2SLsstzCK/9MWSynvYtwIvvu7RoZraHW71yGUgclJ+zqqRBqcXFxHdrmUQIngYyCIIEc7ksmOGyRu8KYHA5slB3WZI2MjDaYw3cZEM7O0VKF6vXVFdL9+3ju2c8DKeGR+hcErNYbYEqYkpzW7sWUtC5wOZpnw4UoLosGySHa+j4h1qxfectoWL/iJNgSqhQJOQ739xg5WMRa28SOLr0UZxIugtRGrhI1qwYtfZPO5b0w0v5hWISvxeMRKLjZB8xWBkwDZibfwW7hn5tJXbMAQqvtCCzdHtSnJV1Rg3wq4/FCSqyfKSWAJ4C2BreAL9DXgAT2k8rLQONIOmEDYcik6yG0PmhRCovwIpjZuZNSorISteBe8WZncA0yah+8ECfFc9A1ErUVQWcKQw9GLa/nmiurgAp3iNX66lVRqFqDwwPHgVYma50hESSUiissGZ6DZQUTxrINurhZTHhbP6toq644ohQEVHBleW7TubQFeHafJnEuuNcInQQgY95e4+ryAnfv3kVarXH3+eexOTnBar3C2fktpLRCSit11QpCVdaSo36FTfrK4eOTWLVKQ6OgEKCj2dZCHqWQZ8c7hVn6FDNWAdTQtHz3LKSBySuCQyCOSB5aap6NQfK00SihIT/FsTsEhg6Z+8sGGOtUOQYdN1tISfLrDFwcU+VrPUKI6qJ7ZcKT49gZM9qwiFqRVFc1K2FiFCZhzB2/FxssFZchA6CEfH2F68sr3L9/HxcXF7i6usJpKje0YjJWnbO4zuamZo4/R6Ptfc5Bw0S9gIxU2xfrQd5XwAEqrqhi7WRTOMkYDzHVY45mrSPVExQoA+Tjt+Gvj2mnVgt/qpZKgkQL5lysOBnDYi0ZbtqTFvzffhE9snX5nmCKtVoBsGN12OMzTaYMJMJUr9Mw2yLiMrAQN25+g7h8VEylehpFUFoiytoTGUQJUouQUK0nKjA76S2bUvnyCtt5xpyKS/D27VsAGNfX11iv19icnmFzdqbWmiKsFRDO9VtcnzIX5VioeNrDknAKVWJIIaEOqfOQfVdd0IxTIo9OjSLk2/Drj40qdJBQ8PkljZVjqAIK+CHh5i+UKlmU9zqfguR7EcJ0CcYXm/NmCyk+jLi4Qb46+5z65wlr2JCjFuU5Uvdyqc4SE23Un1AA0ZtrxRZUsI+cx+9VnnRMxJcxZ80wvFe1xxqkUMsGocfmLuz2vvgOLwglcv+N9oOQk5oid30zLVMKzGAwMFrfwjC3zjQRVMv1+L7I+Jlw2DURG33AOsfx/VI5DBjyyFrxWwREsdB3tV3mDM6EPM/I23LQsVwfU85bzApfy3CXU6sCmHDYFUb/oOlBhYzM0pcChg4m+HH2Ns2BYmofsp1wMZ2B+vwhn6NxnZoPfmjBCKRDhfCh+L4Rm3kfpofpYXqYHqYvznSzLSlbNQfnGVPVRO0abQbPGUzlVIWUirtClkQyef9ocNK4Nsp/4nURrdeHkbL4bygUAjjVcM+yzyilCdfXW1zcv8Tzzz6Hq4vLcm05M2aewduttVv8bNHdA6dsizvHL0hT1dgIeqBpCr4hg50AcOJGw/NZSY/4Aap3i8mCFnJZQ0mUdO3f9bxGrSWUK8eLLpTUhk3uaB4He3VYp2lVr1kfuG8aa7DghECUwZRAFA+DDSHnbO4+f4W7r7p1p5X27dKHciAsYUqTuplSqsddVTdKCe6wi0jLehhKMAWVpTZpWmiSaiZ/LJKt+Xi7q653dYfeFqKmBPgACXbh7Erh7PDH0IlwdXGBi9UK955/Dicnp0iJcPHIHSDPmKaEab0BTVMtW0eTCl3o6SbZuZCS9R053iGuGn3rDaEO/TUoJtIBVdOBpE1fjquruo5dbizMuF5VZ4Cs+cJZi369e+TWU79wLa6WZmEuI8vGno3tjRH9+Xe9vwG17xKBmtWId6vxllm9QjsciTzwIIwaVjp19KTP/XgnbVfWahnc84CFdLOFVJ2YXF0WQmbCNalOVq4DBmov3oYLLuCAeH3vqEaEmSKfUa+nYIDccSMiSYSC6jOiVO70uSprUtvr63LSuF7BUSOvuBCdzl0hKhImWyYuNYMsu4pk0qeUaii7Y/ZEdRNuv2+HPbF5t5wwAw18KHm4ugH79bzCJIiSm8QWRF3KROEowrWsRxVBwM3an8AwnsVUcdbvhSrf46dby6kNeCZu+8Ok0zYGU0phLaSEw9c1sizdZonx8Ms10UWcIkxlHU4HpOLZIUAFlVdcSN8XnLPDlTF4dnVa/RmUE0AZ+Trj6uKi3DV17y5WqwkXL9wDJcJ6c4LkBLMtylelCKmG6tv6ZQFXpDE1R96Lq7hmrGO9pDT5qEsNRmJUJdDG0FO0UI8v2wVvOAHlZ7g/LqndsqBrsjBtw8ViQimdbe55eLSHQ2HkAr8a4T3O7UE3BRQQ1tMo38zdGXoj9+ve5NHmwHPn8bunhHZn8oHyCcCNF1IwDiBBEnkG19BuZi6nOaAgD6habzJG4EN1MZgg/qgcq2UAhlswYeFMollnE6RXV1e4urjE9eUV5u0WnOsufr1vBm4sZXIcjxZjwn3hfQvMJSiBgWqHjfIlorI3KDBxNEqbF3opTk5hpK5zKSVnQclEr4yAs+G0hbcyDZvbNPiUcAS5c0nbc32es0QIWp3lPEDWehPZnU0tPot1UcIsRIHQmIUiBxzDM3QxlSAJuHpG4ybD2Vr8Q6Y++O7ZqX++nbfAbLzu7rPPIhHh+uoSJycnuP3CI4U8mXByWmuZVpjWmxogAcw5OzqRrQrGqpgA9iYkTFERJdFb717tUabK9rtKZGX80pYgSpSEY042eJAkWzAcO17kEYAI1h0Zus4clkp/c7TeYKemSPJ7o5b4SqvAHQ3IQsXGF49LN1tIqYpavud6PxJnE1KcZ9tkyQTmMkzqrRN1QCZCi9/CRex3VO77vCGJTmVsIW9nzNdb8DzbgnRXVqatYyte+Q3CRxh/r+2RuD8VnNGU5iBLPB15gt8d9usENBpCdIxLOU3hL4GepUcD1qxlzUXUBwj43C2MKqhAyuRHQSUaOkzm1uw2BFclx7R/1w8ZSlGaIHGmhMxFsGfIticr2wZsQF6rxus14QGSqoIU8jqCUWVALMQOdUYDnDOur8rtvQDhuWefQ2ZgfXKG1WpT9+8B03qNDQi0WtcNiML66kZkISbVNwo83pjSSLsKVDYp4yJBFcLQb51RYu0ojbj8O5LMfRUunl5a7OhcGsyBOh5eSIERgrK6hp2FG6wLa1Ay7utB14bQU1CCpAQjWFEi7I0cHZM5UjNmuDHpIPVQeFo+TGLdcCEVmVbOM3je6gGazFU7pqrKsmwgFcomQHac71R/KvMm6LlgkcjawlR1QVn/qGetgTFfXuH64hJ5O4PnehV8GMTaNfg9WtYGQQSBs7pGgrUmd12haq6FoVXGxg2zCHPDBJ8PF1arUYismcDaLhe/M7ld/T1ZVvImE4qy3icOIRKdeIdwWug+tGJl+lUIdZDUPTkiqAQVLluqLr6kt+TWkmQHxwp8s9wYS8CUCCmjeJtpCuSWK01llEsmhYmLxjtgYUMsYkBDkPJEuoG9v76x9l36kjMuLi6QmXF5cQmkCS/cvwBjwszA+a1yw/bm5BS3GNicAWlaozaDlGAMrnLssoHZPWOjeMMvdwrOboNDaMSNA6C045WagI+Ki5ElbMrkoH2nGFLzsi1VZv54jLgZRn9IxFJ7wzSQUdm9G4iEKqAa3jVoot3fvDNRLUDLRZTywpXGh6ebLaQAmO6QkXoRDt1oWT+LYY9cmaGfJOQI0BWTwbbgiVinuuGrsEq6QRO4vLrCxeVldfVlKxCol0x4cQ0IaCwaPbGBIsCixbebKMeM2Zorf3yfmiyNhRbcm9ROKBFeo8ZaHAcOA38MOFdXX2vVLG0MBWxNatcca62lFtRWq5a6oqtPparVW2W1npaeczn8g+s17M71KA1mZmxzxqST3O1hm7hjnIWpNUqB0rb0DUhcmRYJLUl//BpFqTnXm59TSpjzDLq+xjXKYcL3nr8LooTzs1tYr9fYXl0DAE7OzgAkTNMK6WQqt05TFDx1RKpSVJU15uLeVPiWNet9fNKUmfGpEELP7QkU8RipnkgPCo0eAFcObgUwnkKd7cTN976Am+sL79oWvEoaGgtAUEvwfZ4dim+bfPG9sq1qJ8fIqi8AIQWgau1+UOWxF1J+8b4QkzFa+U5DUupaU4tkKRWloWExDGyvr+stqOLDtzZtcrBz6wnpLU2mlkKMrYqVFBj6qFujWRMeGNcJgszzF3dtRdCqZK4EvsquakI4JkiUjvqcw3N0wsnwUBqLm7qhDKqzBtFEfLVdb9+oXKquV+FGtV21h8noo0QwlRwpl02nmbmucZYXWarnug6oO6ypKF1eCSKxr7FIey1DjlC21FwaCEpAZszbGeI+vLy4wGq9Lrf43jvV9bmcGev1Cc7Pb2O1Wpe1Pog271ilaNrSGqu6VTX2CCXH4qGrkXzGV3J0WHHj3+JJv8Pop0NP+NkrRlGdsW87laSFOtl9fI1LdTlqGAhpqyUsFTRdpA65y2msc9beks83dGDHenjxdZdutpCinmg8oyIirFBPBhAGkyguSztmFliVhpVzGIMsHLgb234IxamRQOCcsZ0zXrh3D/fu3kW+3gKZYTZW1Tpr5ZQIYKo8a5e2V0+FEEgYYA0VtzPtXMEAXQtxlDx2e2gQVFQvCqTqgiO7+Zc9vqp5T8krAKznEkJ1BBkv911blLymhMh7z3doSvUakWK1zDABreH4EjTheZLALXk7nEhWzw4YNbjc6nFj5IXVjGxnFmYqJ58kG7+JCSmXcO4koe6VqforJ8T4IkPUMJlS4nCWZXyW9HuP34w811Mi5gzOd5G3M1ImbK9mnJ6dAwBOz86wvX+F9bRGnmec376j94aoN32qLm9BOTfuRvbWvSgOA9wPNYjaJyLTebQqH2G2pHxo7Tvf7izJcmbi4WX8DGsDKBhjJUxS6wruVLsH70pIo2po4XkAgPxXs9jlZJOj6nPphgup/jt5bRrFtJf5qUsTZMSrBCNqhjsfzTPNruGhWywyKq9sc2bwXKP7rq6UkaRkZoe2pwyld0/63xa9Fye3hYpzzNusHY0nRKc+VrhiOWGmrUUS8sUp6RDOsX3Vhhl6wgWVsdSz73z1YtU4fGcYQ3eALmjS1MFi/SL7rpZX+U/rJ3kWe63TkpwYq3TiL4mUI54AlKg+MBLXIIvMVSCRciWxUsqaT9J+aW+8S49ZuF0j2Gt/Gk7WkoC6VHMGmLDFFa6IcG91F6vNSbGyAGyvtyAk3L57F9NqwnqzwbReY1qvTZAKfnmhQY6WQAm2cKf5a//GR5Z5eK0Scvjp+9slB56ugTr+cFCq/VSXeEMXbdaIDm6es+KCLVOoN4dusfs/Qj0SDEPYZX4PWNrQVgyd4r6TtReqCwrfdXV88QippptiSeleDmYQmZDStSnJ264pOITbArpXtysRwhGyaA/dWFU/fK2Xc0bebnF5eR+Xl/fBPBdwEsote0ItyrArfEKNLWGMUmVUu6LftCutoOgykXZK3HteoDO7Y53a9iqxEwvjd6e0s52I7n+L9qtyWnhNRY0EsFsgiIWmMLgeHNsLGL8eAy1Z/3cqul/vQkMHItTCHqyWdjywEIFVrSJx7VFGJpJD6qUBgBhzzoojgVE2AjNnJNkwy1z2kWlxFy5sA9R8tzGKBimpYGuFu5zJmOcZ83aL7XYLgHBxcgYAODk9xfXVFc7rOX/rzQlOz86wXm8q2Zo1r/MPHrV+L1fVGdj+6kOCbgGRSy+DE70Qk3U/uHZl9vWM1r/3BOLdw0GLPSD5NWqJVAwyshkmuL7W0CBXRyt824JQXUt6ZtzJfncuSuUxNiBOj6l57O+u1V3iBfSosLOxlfwEO+Zo5CEapRsupI5JJst3BU8I3sLiOjAg84ZJhZ9OsAjRMuN6u8XlZbGkAJGZCbxFVY8agnLMJFBDIHxjQqFssLj6rhYCtpM5+mTqmjEbY+TeMhAceXim6p+SNRwBJ1G5yVb3PTnmRTzJCBVhW4dM7v81Ao+arn+ua4GO2XgLy//Obld8IkKua1XgOLmtTIP3+iBz1uADZi6MfS5n3oX9PopLq5tzLqOQ6gkOTEC92p78uYlc9tupGzDJ2AhcxrHcyJtCwAzKHNgGc1Z3cDA+uB6YmyYkqqdpZMb1xWVVqIC83YLnjGef+RyQGSfrDYgZZ2dnmOqJIVdbFxhEUx1FR22eyakliPgXwNyc/u6nRR0g7YMXgOVXtMK4Lcr9u5AErQcKKy+Q9xURGhdyKHRe55OMpRMg4VmlveQnEESokFORo9ginauOvh3gSjt7gNcN1W2HKOgNmkeVkFG5PelmC6mBJGf0CgcAHdSgge6r3mmoxk+b6DZH9fp1oIExGNt5xnbeYjtvo+BzFokxXrMwJJpZOzCQOu1+HhOyDXO1JhWy/qX7Iihg8zG3kYNRM2yIvxFScnyNanmt68rjBax4CEQeJrBMTgp5SZgrXMBM4yIj3zaZYAsY8WtYvo8uTrfgIWswTPnrNrdS6KJp6g7V4mosVqHX8GUc0bRnOCp99Lo0RxpU2opwF4boL8uETqCC6+KJoCok5+trJRy55feFu3exmla4/8i9crFpzroPrAuiQb8G5Xsh1k207mMkKZp3spYsuFPrX94DY/qH0OAyw7R35HUAeznILwKAR42SWa6hGieodAgaJu+fmYVECmPBg+zKk4OEucAdglOqheUsWLhvO4VHwxf6vIYBL9jV3cchy8GC6mYLKaBOAgIjYUb19cuAt1Q4cNPsS+LyM4vKFrZRA7GQJBy8KYNCNgllR/7V9hqX15e4ur6sN50yJghDyyC3rBygbBlOw/B2Rbw1y1B9Pld1m0RLm2EzqF+DikmYDU3leCMk0mOOANvQOoaFFWvi/JuYNEJftHDK3I8jc73RuBxZRBnlhAMiPcuQ6h4nEVrhZt+USoCBCI/sHItiSVVOUULEZ7Okcq5BG+XKknneotwPxUANW09kZ/ylKLGCMDtk6kYh5fDncIwq9DK4nA4CYGahSGsbFQ+qeDGQ55KPK9IzMnLdfJ7q+tA0rXB9eYXVao3L+/eR6ng/+tijmKYJaSXRiWU8ZqQaasIBr174DjoEfdu4JINwcUQea3rpTvc+ODnGPOqVCKUYec/gxVk4Trq04bwEwvB8n0PQiVM42+Z2bWbunjlBU95V3Nc5wuBwct9wh8oR3b3xQkq9XbKaSKSuEGGqtlZRNRDvx/dMfERZptDaA1Xrq+6u2rdrl1FuWUUCJsKcZ1xeXeLq6grX19cQziuHlwKecMsokrZVnplwcEC6SDlJethphUnDgkXIeOS1nfV/AlGzqtq76Mu7xpTug7UU1wzLHjIHgViTVRjLuYbgXIUEoeleHZICmzBkkIVDyxqGRBkVlYbUvaJ9VCxECzGsV2U5bNZUaxFSnHNZy8lZlRnUA1/JWRaCp/IX4W/AJWCw109yNKZ5BBTHyGWtTkjVxs/1R/6vm61VpyOHD92ITUCejcRpRp63uLq8QJoSnn/uWdy+cxsv3LuH0/NzJKykNqU7KGSC13reJtszQPb0oSHriCBSupEJTIpnE3yt+ROT4ie04X+R5ouwVOFu2m94DkO1Kh6mL0f70YlhFbyEeuYvzAoiQN19SUr6+RUwqFC7lqE4aqaOg2EnsoLVFZVVoSuouRSCpl6kpnDjhRRQ1wQ466B4JhjN6EJpRRGnQN+aqdPQASMdcZnI4NgRS15/UYWVi0ZBK2DOW7xw8QLuX97H5fVVbaYc4UQSqi2EoJOszozKqLWFRngIdJKiFSbmvTEKlrJN/iiUxrhuH3cRdYIZ7YjYkgoOiAjTNCnhW78KgxcJV464EreZUwOczCuRgE5zq6eNl8O/DdoEOVGirlMJIZgoKtCynCbdCCcUYWTav6Wc2QRVHRM9jVpcg5kxWmkmquf6USQ9EU7F8krFEqP4zgCo9JMo4DMwRpO4Vo656mWRq4phUjyBcrFkAuekQHAG8rzF5cULYJ7x7DMrnJ+d4flXfgkee8UrkE5OarvlNmDBiEDIjp5V51MVosyzHGCiOu4mhEkEYBXCduLfHobrUlxL5qp4Ftgm9O658tPNHVHIKimZTunLGMFKD+W5x0PZp10U36lmFjduN++km6EN6709tbUpE8tA/ENKJwJ6G9QQMCrtMId+sig8okWSPfdjfaQz6+F9Ug/Tw/QwPUwP05/edLMtqVarqqnbHwQy7dknUQ/EBI7WvtbttfjRGk+/B6mqViwqVrXq5FpvXYfw1ozaeuU3i2FX2/YuSu1+/2wp+SAQbzi25c0VxOpqONZfXqHrrCyxQizwBME339eA4TlirZOlNVCKtioj18AARLfqAv68fdW6AsUS6HN30Ac4K8LhN7HK0U9Ky2SUPF577x+2eO40+X3DV+lfgxCaUyCiXh0tMYn0nOua3Ha77Wm79k/GQ2rswFoYCx+Zqc98/sbi7avtnx909JGU3wNf15bzowV6V8AjHGJNOe7xQDOug3dXXYIzslHxo92HrqOzyaQtiRgMLkiKZT1sZtQf1subLaQg7hLSRfCyxiOrESy2K0CEJBe2eTdPoIyBL6YmOTdT9+swA3N2ws0YTRIKTTUEOGdst1tcXFzg6uoS19dXmK8vkTgXNxTPQIVbWszVXy8fYyItiJFh9vEEck+VKyPvXB752wdiUENwlZF1kBg8rUnfhoBLJB3YXFiAnKlGIUyZavuJSQl8iIfaIYKMEYNSHW93crlMMwJC2H/nX4et3mXO4dp1zRWkfWXGLIKpBkuQrLERJOyeZx8EwGBO5eSRJGt5siDuGfQRjNX3SeGRaWDKlEYzhvyAv7DOu9CCWCn+PszXl2DOuDclfP7zZ/ijP/wMnvyz/w/Ozs8xESFDToAvDldPibrHzcHs0TrBaGWp76r8NXUI0pSOFuZO+y5G9EaBHMq27rCw5gcVUOVvVBjadvUvm8v5SI+YwnjIqRNhjte/k8NxdBE2ZQE3MwpNzOoSnozQfJn6LLtZdYwQvtlCirkGTACCMI8f4kYjkMt92glROVxHGo7hiAYua1JyUGbJxtBjWiB6iY0WU8Y8b7HdXpWT2uVKEdM/zK9dAK+/Wa0JghF7nHBxyJ2s7BkQ4nM9CJVjeZunDiNKxDYDd018h12t005rCLE/1jbJOkPBL2CxaLr/ox2rZrLJuoaEpEMYXTvtOJ6qrnvBWHBVP8JpUGlNx9R1QSSWF1AwgZmE1TNKHWQMsOAih0hDszVUv7Jwe1piH4KGiI9A6VSCNxI1ecP3OD9U8gOQ9aWSyr4uuRpne32F68tL3L9/HznPOhd1XbCuNfZ7dww+Hw7vAFO6043V7p3ooPIdEPr3czMmVaRGbbl+s32NPIMcXgUGnYYchGPXvFouUUB5b0yYQxT+tAAHbi+C2oW6IOucdfWEfWf1a+PxsArlvT1ghddUGf0jypRvV377hdcjpPCNF1LIrBpgYercLJ9WyyfLHgIZqEaLgYQ49xqT3rlbiZX0ndHljNwQZGFRqLpG3l7j+uqyXMjIs05IEoFUixgtsEhFbcfoqGEiDczGp6IQavc3jYSULw8gambKZOJpAR6PHi/M9dg4ckLK520JtU5uYrMoU9Dbei2M2x/sA0mqUEQN+9ZJ5V2vWYv6DbnmfpUWvcWaQrulPXEEQi3uEvQA/WvM1AmHDDClOFZc6QJV3DZ9DNebjawA97vUJ2NWYhr9JnFqccoya+pPlAFkpKKY1bvaygXAEtwyY76+xuXlBV64fw953iKBy2W8KvxzYYRiSbvmWjLoBEuWTdkAGlpPbAax0YzdHmw9RfguYwHvifCWpwugiLhp8OZ5gvBkuRaogcEAkbqjQlB0HaeEaKW9oPX1efizKyHqgApvawWB6WhdzUhQHItAhl6OaVUEolQFnrN+ZcwTgSRs0dWxL91sISWnGlTXAzLX07g9l83dWVpln1P5nRK5AWsSExgZc2UUnRe97qIH0F8TAqm2CLl8fY2Le/fA8xaJGRORRpwBHIg1gOC++wMml1IrkEYpVwElgnu01lX+1pt5kxGcnKknzDa3ZaWNOYOp7F3aMsApYbVaFaa7w3UzqitmssmWBznE/QNWW6WsWeW698rdDusdi7IHSupXgePa0FMtGkoIUFRySkRICVhR0u9iXXTjIjAXYkau91ikVIULuZMwKMI1UhQ65aRnldpuLWhw+bFh6N4mwlyEUn2fmctetC2BUkZixvXVJa4vLzDPW8x5xlzPdUqVSWWQu9jQg1GjV5WZyhpyQQnXeR1ucXZYF9dqPy6ujtCgWQJDfEB4hOPEQXOr4qvSi0hIPyYF5prdywG2MfNMPMBtrhB4DWU0n8MzIr2bzPoJEwosynjhkcTm0ciOroTWbA6hU66075Bt1IV2k+5xdFnr/2Vbxxebuw9wGyMds1FFvaDErmx2elElAGZdwWimMlm20GIcKKl25IQRDRt5Rt5usb28AtUd+UxFsJkGFl1r4lNQx5ZkZAeBKXZGQPW7X1MRbVE0Ox8M0FmO4pKDaK7telLFO8I54AEnStBcREmuDCfX8+nU1vWFO6pl+59iXuYFpjtItk5UJpEGWTjrO8DuYJH1MAWB5dnY5SbjFDbuJgsfj3tWapnGdVvw5sP25fQQJZNB37n7ZVYU1wsorYs29hF6c5ORSekg7DJYbumjulWYy73zOZd1VSURz+S6s3A4/C+zr4HGmKune6/BSzOurITb64EL5PqizTNExnRzO0oLpeOoRJLL66wo1z9PSgfRKvkvDj9uHHy/tR/t7yBYQ3X2XF55CSZjLflbi7/5bfOm8l03mrq/0zXMzd9j0o0WUsQzJgImFGG1murag7uikj0xyoVx4W7qHAc0CL2qBcNu5E2UwDyXM894BkBISMEPrn+r+2i+usb1/Rdwee8eKGesicqJDDlXX33uCU5OsWCzDrsTwQFwaojJCaEM2bNgQQcygUNNqtCZMBL/f7e4q2fGMYjr6d3udAbBnOSRUxhynfBJLCq3PuFh98PEQNGgmcEJRTvlcgICLOegZeuWaatumgT3Tn1d1zZtBFNxuZXZrKSxbwFePpO4+lKqlpWqPdDTHIByGoezVFHdxsXStRMyWj7re+x0ldotNgbH7iBgAFEbb0t6ld+hS6B2mj1xrm61XOdXwmo94datM6ymcuZfOZewWIZ5Kuw8WlLG2mRMhQayW+M1Lc1gD2cAelzU/pXrRuw0DZ9ZcVP/jMa06AoNTYilCtsgbgpNU1Z61ghJb6Hpd3HrNwrLsRw9172Z4Yonp+DJfEvofEKyeTS0u0tIKbtsqqGQQx5mNJe0HJVutpACY6JykeDVRQbTBRKui/uvJua5sgUvf8zoFLxn1EFyGg2Jq4bI1hhSgqxbCKlSSjX6y6v70NMHtpdXuLq4xPXlZZncRFgR2dH7gApIZWSVCmShXVU/uOEnViMhaljC3GN/GawW5WDeD4VU+7wVbIYtbToKh+rKykTqXpRNscFnXfMCBjsRAnPlhuF64E2oMjrBWiefauTeHeYsyprbFurldItKFkmtqEY4khNQKL53XY+SDbYk5GUMCkBRVrQsgqXj++2RPAzfxqBcFcjFJWt91zzdl0hH5P9jRuasCh6lhBkzeM7FLZxWSClhs9momzKleqoHi3ZNZhHCmByjIki0KIj48izPSRUH50jMdoKnUQAD2fs1qS5/ciXcmIFUcWnLsYdVWiLA+/yiiw6ap9zYLBNa5gLioDQ8IJxfyc3+dIhbVCgbTuGJSotEfXr69E2PFCSrDKh3aAAAxjNJREFUlWOeCJzxMsjpMeP6ltLNFlJUkH19fYnL+0Cm+0i41kAHibrL7I7IcRqNflRkwUkylDPIvJCqZ795xlEEWVkoDQvcmcuieM64urjE1f372F5dAlxPoaBU9UdPsI68RalJcfDDVe2OnrutLXUiIVvf5BOjh3rXlQ8m6QWXJfb5PcdjY/SycA7YGtg8zzUkvFqg3o1Adb3DzZAwTiMhVZm/uSdbASU5rZ3s9qzJq6CjVMkkR0wRQS+o7O83ckKqKjIilHTJk1QPQTiBo55tWDsKPU4JpmnbGqrry4KSIOXkb3bCyojc8BaGDZF8HBoAlCCJLAE/VYjnmUGUMK1YhRSlQnspla0Ds8w7FwgEoCh21UtgsZiiIBicqnK2siQOQJUFkZ57QWLPWWivVZJaQTmoRI8rclYRAl9wwREtIMwRdlFEKZsgbwVnBwKHvggMngfJnFDXHFlUI7vBbgWV1F+GzGbOPsEiJ+YolUpdie23g/vQdKOFFOr+o8/+0f/F//f/XiJt/gBE10hiSVHZp5KrhiHrVzlL+HfWawwKozeCFNeeasl18sj12YXxFCE2UQLLZTeADu6EBGTg6uISz3z2s7h+4T74+hqUZ/B2C85boO6RUk1VUt03xcjafhZXi+QhcvcywTVeARQ1jGGCuKWRgavDiHm/gAIQNtMSqAZMJKdRVhiyKQtzziDKXb1EjKTMjI2uud4GnC2KUmVFFeZ1Wta+R7EkIf9LASUihExoGw5M0IgF2sBcrWiqGaep/q70p2WJYOcq1rFMVCPlKot22oZGGpI4nBXJXbJ+MtTFp+/8aDannreaeosX+Y8Qzlm0Q4NL4fXJCuvNCuv1hKurEoqOaa2RXtvtNTIYW28K1chcDRpBGdNEhClN6gYlmXMOB/40faVX45BujOzoMn3sghqC9iCTw+twIqQ8jiqHT2ml81+7pGc7oqOTgFQXacXCZ1xfQGNr2ero12Wneh8m65Us3HjZuJQTfgcPg0QbO5j9PPHKTZPK2jqp0SDtmBSs7r5cLfFcYIzLLsvpaCH1kY98BD/yIz+C3/zN38SnPvUp/Pt//+/xrd/6rQCA6+tr/MAP/AD+83/+z/if//N/4tFHH8Vb3/pWfOhDH8KTTz6pdXz5l385/uAP/iDU+8EPfhB/7+/9vSOhKRNyW8NfU74A4RpJN5fVw625aOdygnOuFhYj24CKRqUuKBFqxnAKo7TBFCuqCCkEIZWYipBixtX9S2wvJfw818X88pdZmHzQY+Nv5hAhZK2IAIoEpxqRCl2vXrl3/lmoumHUI3W0sURDE94eUS21Mku2zYpiYVnmqnHXQ1lBrHjNXMYqOymb2JhWcSHK7iRvIZh27ANG2lSEUR1vZYClJ0mubh8Ia8VfvZTQW1SCOwmflhOrk9/BrAc2dsr2WHVV+iTNqPQrZTvYajvyrmU+RI4GDR+tEWHWangIEGG1mjBNZTwuLy4wre+Bpk29owu4Rj2ZwkMny05ywG3FDVE9yb7iSwW4D7hpLGX7Kxmk+0mVtK7dOjZSn8ot2CbnYCkpzuq8m8r+NpqMjcrG4ihC2vmFcE0MEek6sVeCFkRcaWf0vnpzNOKWGa0o69arBJ4G0lYuF7bRazTeetKtOg0voyrUfYS01yf2paOF1L179/C6170O3/3d341v+7ZvC+9eeOEF/NZv/RZ+8Ad/EK973evwuc99Dn/7b/9tfMu3fAs+9rGPhbwf+MAH8K53vUt/37lz51hQioCYpjIYOWMCqQAADBHERWrnVDT3yTOyZGjbGRpd9+5MGjgA6MncKZV1MBVStWU5IHV7DZ63wLxFqgeRylUOU4UkEhNXK0r8wk1kjWVTAoiljRiRCqtNYomA3cSPmlPHYB1RWbJDVOEOVLUkuG3KSeRAbW6e59rLgGkQAeuprC0BXPZZVYaRZU9c3VQ7ySmc8AEKcY1BF9K9FqEgud45AdcyvimNmGGkFWYog/XuPhKXJoAppXo6ipXlFguiXVbXX0eTjdbQjs7IUpQb5/U9BklPxpBmXHCAPPZmOKFYzASsVoSz81OsVhPyvMUf/dFnsHr2c3o9x8zlmFqu+JA0TZO60EsUJA2Fglq4Dm9TWkWlctQlEFbT1AlmOYCFiPQ6GRGKEmiRcy63FUj/Z2+BlrpWqxVWq1Vdh7PrTrwyKz1QmCrIaZpq+07pqEJRDsjuO2TjYZvcnWChqYyZbMGQsXYoSk74J0pq/fNgfkQ3vuA6GxlURTL5JgidQl2mfqGVmUq4+yoRpgNPjj1aSD311FN46qmnhu8effRR/OIv/mJ49k//6T/FX/krfwX/63/9L7zmNa/R53fu3METTzxxbPMhiSJenCGFEUcJbadQkIVXmUmKZlg4jGf/HqqQai2FmZt7seRhE1pcLp1KibBeJ9x/YYucr5HU7M5VAwkrY8jZ4LZNxipOVUtqNSD5QUze2wdUHEGjAaNw0v5WpDJQBSiUgamWVgUUuJ680WBM9SmhcWedyvqIMVM3mavQmG0WhEkvrgw5ospwQsroGWXx2RgTm0vXQapRXx5p4TigyhhJ9jiJAHf4Dkhnncci1ORUclVVq6VWGJO8yRXT1SUVNtyJxFg4BbtNQWGwiEQZbq4h4r50SsJYCyMuZUgF1OTpxEchSuVUBMGt8zOcnqxByNheXyLnLZgmZJRTD+a6udXjPKdk7RE5pcHWMcv8LvgK+6QoKQOOM5nUDCBEF6Uld0RWxYfipT6XyFRDrcNtFRarVQkWmVYrE+YsbnVWUACvAEmTnh/JiFVru+KlFcAe99KObm0QoQM3nnAKq8ezCuSpkmRjIQ+sc5M+1QUtG7lrK6W+FZgTMidQ2mrJ1XSBRDMYCfM84eryPq6v5+hR2JFe9jWpZ599FkSExx57LDz/0Ic+hH/wD/4BXvOa1+Bv/I2/gfe85z1YrcbgXF5e4vLyUn8/99xzAIw4i6VQWJXsdNfEDCa7aC5GN4n/Vx8MCNoRv6bW6uKeoBwzp7qIPE0JmWfMeVsRX/aZmC3HBmOFU6bYUBCwBRiMgrFlMbr0q65pcaudD3os9QLQA24DjCIg/USUsuVvdswRoYZSf9brR/z+sPJnZmGuTkgxW0iwjKVaGuaDlzU8EVAM/zemIKgY1Z0RhXer4Y+Vd8OXCCo5TzIkM6+sPiRFFA/oSOpX0Fj2zQk8Ar8LujFzyDy7lWlRw5BCOLRY14BuvZhcHr3IsknracL52RlO1mskYmyvr0Bz0jtIMiXISZRDK0676ehf/lbaHZW1dVY/p6FCxG/Pih6DqVHMYru7vgPG7CexhpwFZTC4vEh72/N1C92MrHb5KuJBtsaoZRSCesoGaj9B5fJPL6xa683WCV1iUXtsy05xbeZygSsRVqsNck5lV02aFdb1dInVlIG0wTyf4uLyAtvrufcSLKSXVUhdXFzg7/7dv4vv+I7vwCOPPKLPv+/7vg9veMMb8Pjjj+PXfu3X8L73vQ+f+tSn8GM/9mPDej74wQ/i/e9//8sJ6sP0MD1MD9PD9KcwvWxC6vr6Gn/9r/91MDP+2T/7Z+Hde9/7Xv3+1V/91dhsNvie7/kefPCDH8TJyUlX1/ve975Q5rnnnsOXfdmX1V/qUHEKQ3SZFAXUtERvG5ml4WvrLV20P5mbl65AOAdF3pk2H6oj0/isRosm7LrTwDFQxiA2lNZXNT3RkCXEe1zWp+p2Ia/R2rsW6g4G55YY5ShVOxuHxd3CdWnGueqc9sl7NLAlDdWvw3VBEERN/2K+uDDP7rv117vYrIdtnghjMawkzHywZ8fVva+Po/zLdfrKnAsKZkXZnBK7PJZhMKZpwjRNWK2mcoU8CaV7VzQPemCQ7uwFLXkTILa8WoxSmYShEztrypnrnTUslpCHkihMcY9CC4Sp4ycXbSqmlJQBiif9KyyMutG3tdLgaMg8BdJu6Z+zesnorlie4qI065Nd3eVEf9ngnPSFuHi9O94jXdfZxb0n7j7OyDzXOlK1pAgJ5iGacV3Wr+kS81xuJp/z3N3OsJReFiElAuoP/uAP8Eu/9EvBihqlN77xjdhut/j93/99fOVXfmX3/uTkZCi8mAh1CaKc7sxZ3VM1B5Rx6NTxCLeUuic9G5akq1Fewqk5DHdkSAm9pGo+z/N1M2Hd4jhH0aJHzOVy/EwnKLm6cmpdNtyeNQjjMwIXo749cy/ggsjhqwioxIAdxso6idpqGDJBhbHwUNbqvhL4rsnaSDmlgFEXoJnrKaKl3szcuTetfH9Yk/bduVH6BWpCu+Ir7pCpW7/K2v/SLMFfwV6Yy0jLKb31UWMaurtDmLTPh0J4+CQyt1EbMg6pdLgoMtV9KfNGFKYURqp81psNzs7OcPv8Fs5OTzBNqR62LDdlJ5Q4V5t/Mub9apFTBBjKPP1N25ahcf/JdPSusQqoD6YKbcBcn3K6u66FIb63d+U9EUAT2Txw8kbmY9Joz7aXVSHi3cqG0Zgpb0A5jYMdfZHgI2+tmzVlx5cM+lolmRATgRFnhCluOmfgqJzLfLuerwECtusNcp7ASFhN5u7b8j2Ar3C9nTDnE1zPz+Lq8gJzvsYh6SUXUiKgfvd3fxe//Mu/jFe+8pV7y3z84x9HSgmvetWrjm6vH2MbEhEEhZn7dYlGryMj0KBgQZisIwi4CCQ0z03tKO9FFlJZK5GFWKIShainQjf121qLEHGr4yv7caVit8qV6gSuG2vViqNyXE1CDese+Nu1DfdKTy1o8FC0swgbSTsMqPivsorrOl2rSmhvuPrbuWx0LofFohwQKxpy5aCNIeQwwpB1CcsUJ1tnTXGs0C/mxzUCBuqRSTbQDPAE3aya3HqX1umYjW839GEcYVq0/DmM1YgerDzHXO54H9n7J4WyXPfNKIERbLuqyCkbXqhK1VNKOD87x63bt3F6dob1ZlP3fLiAAwCgHBilNi8bYlPdHEs6Su4MTi9spU42SeeS1xv0mQo2mRjcrD22Mx6QUykLPllhjUNT7Th2dQOQw8sK/QvuokolM5oRy4Z53D6TstJJFaaGn0QApgYp3iqSEtp1q1si7TwOw2kW9UGS9pghAVXrVaoKBfR0/FVycBMBTBrZR1QE+PRyBU7cvXsXn/jEJ/T3Jz/5SXz84x/H448/jle/+tX4a3/tr+G3fuu38PM///OY5xmf/vSnAQCPP/44NpsNnn76aXz0ox/Fm9/8Zty5cwdPP/003vOe9+A7v/M78YpXvOI4YBrGWZ5F4aGPOwsramIDmndvWkKqrJBcHorkxGyHZhY55IWUIwbHVLwW14bitmLKH1viwVD2pPPYTUayctByLf6EmVYRrYvBbje79rXHmGG1dpJJ/hReDh/ZB9XTEWqT8Fm2q1jgMhFUAAQWQ1aHyoAgpEwItIvTpS4fKlzq9ovLJgBEV24bq8yiRvfJRYteoehdjT2tjgUVhby7lAvAMVSXLwnjlVeEoHmrAuAElW/Pt5ASgaYJJ6enOD87x+bkBNNqJZW4duv4UepCk83Bxm7sjFH3qpnvkyd2E2reco/zIs521kvaHNEoTcsY27g2s0/hUNed0CvY5Sn1MUdPiN/83ikUo7Fvf5DP5hSXNMAYc1QimZ3/kZ2lRwFf0KAVB7e8FMFc/04pmTKDElU8eTdlaUmvrSFAvx+SjhZSH/vYx/DmN79Zf8ta0Xd913fhh3/4h/Ef/+N/BAB8zdd8TSj3y7/8y3jTm96Ek5MT/NzP/Rx++Id/GJeXl3jta1+L97znPWHN6dikjilTVvXNrnQQiqJ8Ko80ZGp33WXAynExYGC7LWGZKSVwlugWcq4CR3CNVj+qf9S7nvXJ7GX9lE2xvRWlJUbRR0uobJmJWG1cwqpB9ehQZRQ7xkQsJK4CQ3zdQLBqJOLOE3mScGZA8erdXH6jrz3zv/VqQjCzRvQN16+k406hICI9AYPqmBORMoTg4mt8SMFCGoy7lT1gfQl7SNPTbrPep5ozcz2OK1bk8bDZnODs1jle8fjjuP3IHaxXZd9SzvVqnLr/iCHzZXBmPulu3qqs1NMhIG17Br7QKUKTTzvq7IwRzlj/DzWHH95F2Kq0kpf0u8pnyFmhHPO7X0uiSFzzi2uTvs0WbNmmIZdxiPDUqmSbQ5FychN2mWORc5D7fzkVWkmihMD2i/I8W/+pbLNZJbMu8zxju73aU39JRwupN73pTTsnyr5J9IY3vAG//uu/fmyze9NScGuL5pEGs0+oicJVsrCjWRmEIgQG8kxrFwuiMK7iEpG1GxFQo/IY1tODK7Tb7Yx3bdunbtzbNY6+ndq+ugxFEzSVN8BnPmvSWSswqBIq9ZLfUCvuldJacpOyWCYlzFY3XgaGXkqRqzcKKRFc9t4LMl+aXJklISXap/32dSNou6z5jW0qnoM3gFSIBNfWkkbSJqEvjm33+eJfwwCU0GX1IamQrZtvyw5qbDZrnJ2d4fTstKwXe8u/EcJdw9ofgh6fo0qMU7SU/0cEyHwcMXrS/9veDyYLvCDpXkG13s7aqW011kKpzyAI8DUDMRZUpab9eshyaTSKoKcvVYTZBBMBetK/FB+qBA1MFB6WfaEEssOyPRxlcb3Wy03Z/elmn91XE+lR9xG56pKKf6pAGYm1BcSRkepoYX48rM1vZltXSQmcUrVoWrKyosogQT2R7xtjXcW1CwCzPyHCuQtHZVmZiH9uf2Sa6Gs3f9v1B/H+lHJ1wjjOWBhDI5CQtJHi2Ss+7CnJ9RXmteisHJYyvbCR/OW7c++BjEE7V2C/6RcgvQvGMU5BRu0wJXZ5IrOMBmrLSG1UevcRlrVsZtc8hWJeIUjdulZw0KrV6U+CkHRyeqp7Gc9v38Kjjz2G23fu4PTsrG7c5npWKtmg94C67y0ec/glSoPdtGupY6IDhcDiLPz3AUiwd0G+Asa8Q0ELHDJh50Fxt3kvt7bUE/e873HTbHiryhEK6svtG95SN4Fags1yFVSpU7JNC41zWfonwtzW7IsCWe7KK7BrjVzvHUPWT9gfuSd9QQgpS1VYNZqU146E+Y60nBHSynwfaWa+fKM9NjD540moDmoUSnWgnQbnGcsueg+k7JRuWVsIDAgW1WcL4W1XWBmMwRCmSA9D46IionIra51NcZL4ABbXAKLi6tEhwiK5Y4WKkIrtGk6qpijaIomgaQVWU8YtqPt8nlGbBWUCRHGZRAC3Wvaym65dexwRZouvoaBqLfkmT+ipyzhRKuu1bOXaTZ6UEtKUcOv2LWxOTgEAZ+dnOL99G5vNBtNqVTZvV/Rx6I9w/n0Cq+LTWbbyuNyN3dTQL3AF16rhLp4GU9lpB48EKDH7Y6vKs13rh2al1LD7zoRdmrxyAkf91fCP9lSYrk0eKGe+dirrhv5aLqYa6Zu8AgcUt2uzNiySzvNQHZ4y0IJnY1sMO9XerWHXfHJjNlV3+upPy4kTf7xJBIBDLGBH8oenTbmFtz3zEMK1Knsi1tid+kj2z9BQC+q0bWEwTmAs9DYkqi2JHuMZVdCyF5LfqxPccNKnQVG/8CoNsbfA1OUIFcZ+j1ApIMy9ZCQNErA2TUC5azCGwsPDVXotBwtHt58JIwHQBFcMsGgDHQwPjZCkskDuBZTH6zEpriNwGDtfnz/lgEJ5Dn81E/sAGDIlTE8WidGMRIRpSlit1zg5PcPZ+RkA4PTsDCenJ1itV5hS0gs2qd4hJccCAVSPgWpovE0evypITUjHKTBUJyOtWYcHeSMvUOueuVoe7nh9Jx6VVNhKGdS1LXXXpoPWD5fSksXsZX983dMlqVJmZUtEsQ9UqT3U/3wN7NAnZzuaS97wKzyn3tqg9wsJTZlPoNCyHL912OF9XxBCyhYIiyT3hGNhop7CwnTeXbf7Imw7ySA5xkjjUtC1qERYrVbIWwbX22pV4xsQo1zbkcSvlRnqnlKX45LgXdgkR87GHLTZYoXah/U3CSSym7HbvCyKmIjNYt4jG7ORaygmf1adxKeiCCpRjovW5S2pqvE7xi/yyu+98etQIXCisaI84F6zHa9HmQBV/NcZT1O981TcHeHyTUbgDL7VIQlW7XM0Tl75INNlwDauoZnqa636rzXIACgpEyEgnM9HRJjWK2xONji/fRuPvuIxnJ/fAgCsNmucnp6BpskYHBGQJnCelXXtEs7CotrR8AJK4WypM1RLO6ZxVBisb00OtrGQ475UsWotQaUR4yV6cD8ZlokmHJKOVWCAuG7lWRvJBK04KzAIrZS5aq5osSjZrjoOXW3pHuadQLkPDihHvjGg3hNu7k0u7vviqs98mGDy6WYLqcCEACP3hmIXaMArD7RA5G1ty/TEw19+XUGvXq8DqQENof5Wg2oYa1jMbTmeg9YxLGAslEbvqM2n3MuEVsV6/8IVKpqS1+5EIGcVUCKgW8iS3FDKMramEXqLKTAaIgeXPevpA4rLGFUXmeowEq/2ozCjfrw9s9fn3r1KDdBS0Fm9cZxiG2O3k0invk3/eyQMi6Cvo8USkVjXoSrOV+s1NicnODs/x8nJadkLBWC1XtUgiir2at/MGuVwqHOB1OjTCyYK75ueLZItGy4ZzRYTq6CtUwvAewYqvDp4jNH0Gqt1vQD0JY4TQL7RtrXBmOv3UdZewNj7KMhC9V1Ze04L7QlLcsMR7QL5r3l/qKV5o4WUzXlhRikQpTHSRgty4+vp9DBBdSDR2UiA2Q6ZncuNX2D3GRZ3Ft/ILGYgHCm0NBnCnqSh+yBwt8FEdBPWzAxj6F7SW5ehp88rd+TQZ2lXrpIXWMS9BBBS8qHg6D6SVJDJ76GwkTJ+Ed7TitvrsSigrJdhGbKq4eadlf4eNgmHLfj1gA6O3vXKygkEQn0I/9WH7suN0xlcDmFOCSlNZb9Tda2enp/h1q1beOyxx3B261yFVJJIP0I5HYQ9edQvKbrN9HQV4XiO8R3DynulyPU5ZPNi0J3JMhhfIqoHGNe5gtbFuADKgEZ88Pu+pK5DBspJJnJI7EBR1TZjUMhSvkKU6ODw671oXu9eA6fKDqRvDIS7xeUA4hSVpgYCKZUPwA9ww4WUMERV0tkYVskgzEJyAwXRcoSNk1ID5UkZRXyo9ZUr4EvDGifgUjuX9Pr5lIrrb571VAUfIm6aNSpDF7gbPZS4CDyYe1A6o4oRyUnowkWydlSOrhlSJdVdQ2RsRES+CqD6xt+HZNfFV3FA9YgncDmJgNmuNgHDTgGv+CGA5DZiZZwUxlRcfUlhE8HkhZcX8u3i94BJgV0fWX1R3gJrLa/uB/tpJ9/icTwd5wsMQrK009o/LzUl1x+5fTy2bapxHDsK9TGgLr7VaqWW1Gq9xnqzxiOPPILz83OcnZ1hvV5rCHqhzwzmpN5nXUfV9459CTPNDgb5w/CYVzzZ90ZJYwhRBRSyjFUzZcvjVtGjCnejfpKcc+nsBvYZHBjWXafsyoPsgWqbbr7K2mDFQN0iQmCETdzuzq+Wtwi87Un+yiugqmKtysbAB5KwbL4fwJzZKX+1QkYC01zqoaqIisamSpadFVqeJoyjq8fpRgspL4RkMFtBs1gMu03xbkd/edrmgrbKLpcyE09gUWsrG3wzOPtcHMs3E98TnAy6wNEzxyaTLlrUKrSwzx+LCqxAPa0g2B9sdQa1ycFCsttGJlzWMSOX3xO+RCWqIBqGgBu7DWXVwuB+qFxev5crvGePYdSJJ2MacgYcuUcu9frjqE0/ciJMc3vKxiCpiHICqq3dAog4WJpwZTPKfjRRoMRqn1YTVusVTk9PsTk50buT9Bw9WMC4jiZDXYtsXBcyHiMURWunzgW/bmzE1JX1AxBkgXd717KLbj+PuFosMGIMSal7qODU+mgpI7AAS8tjhJfphLXBHtbDboZ66DsoQytNTF/9OihbO8i1DHfZbBHDC6i+kx45+zh1ScevYj1MD9PD9DA9TA/TH1O60ZbUoeZiW0qicNrd8YfJ9WhlmYtuSVMTCyLLej28li2Ly9Ey8sm569RSEL1FavKaY9Qiu304D5AoWF4CP4pbYrBYrFYls4X/88hyjc/0XC8N+27gGMDWnuu3PIZ7Rpdbi9gF87PH4UBLLKZDsFBbS3cppFirGFiLS2uI++pabKPFAYn1adp1IsK0XmO9XmO9LnugpmlCWk16zFGABwP/RYdq8TdwOLeuaOTVDjsiwMDQWuZv71Gz/pjF2XhG1FWA7vnIixKoX947V5zVt2R/OdtqMHa7V+WWxtrPeavXljd6WIJ9STIuEcIRaZm/x8ZMQ/eb+lt7jZp3x1LujRZSx6Q+4km+qPMmuqpibvuqlretIWXOSDTFeVYnIzNjztu6ZlQ9sbJPoQaxJSLkAyepA7tENIl7LJBCPyHEjdYT6WHJrHQ2hg4GGh84ATVCsOaQ/9jaNP5A+gyQ49789dbtOlOE2esYXHxXti7h4BH47RnpfVX+GXTSoYT8u/B45Uuo01RxUMtWARWWQ2sJZhEuBtFSn5aSCVDoWlKbomvSjkhq8wTPWI3k099TwuZkg5OzU5ycnmKz2WC92WDarEvPXeFMXkA17KeRWV5AiZ5mzK4+OFhNhJGeiTlrjeoKIztVztFfwVEcD/vWhs0712unp7Uwk9ERA9yuRw4SaR8WOtkqr9oPWfttFNfsBZVzf2tjBHXUurU+AjnhtmssCl2V449knlf4hSfaEwDQ0zeYMFyi25dutJDyRCUBE0Td7AhJlni8wlOU4B3ynTEMVc+clf+2U0XaggQ+MNuJ3sxReFT4bW8qKbFRKtcxc4Ze9mdEWzX+oE0S7OIz6FYIznFD3XIyxEwkh7aaRuRCMhwe4yyiBOTZW3ol3N6WCqgGSNTgh9qkHhyrG3ZJp30rrMaJ0S2yOyEmE7vA1FZW9n7kuuG4dIsRbxrj7n8tvTCvPartuoeBEHUt9NUMuNQezdw3YIETpGt9klJKqmxJwMT57du4dfsWTs/OsDk7w2qzRkoT2rAQUwj8p+mftJV3DZ7jXuZugFS/12hkOOLoZmFYymm3d0RxNFi3gwh6dsPggZJJUMW1WqXSD15eoiEp7vrecPGlsWUQqG7TKE4WlhPQTDhynfye15VLepr6uVw9HxhmAwf86phbySI/z7WZkOR4ppxrkNgCOpbSjRZSwtx3ZIARCjfPJZkEEm0gJhFBqiybFaVvACXGphU/hKOoLT8lPfPIetAq2WSV0vWBEIx7pC7MjrjJ4A5YaBGoxz5Uwe+1MbY+tW6n4D7RJqpGWSd5q3MmkiNoyhu96VQVDsf+DtDATAEZZTaLxnGHNofhSplSRrd026LMufj6570wFFi1jNOaBH3D/VkCH3u6258ITuC3ipxLqUadnpyeFCuqXr9RQs3dHEGkQ4AC/O2AuTf7oIxMW94QOrr1JLaDt+60CYADBKDmoeZB/C23Abd8ppC+H3MDNMxfJ6BaxSb2RsqyCdDaf6MQo10PaXlXPDrM9l0qKU1JqKY2tdi28gLf7iAZXxXvyjEi6oYLKadLus67DaSjNYQhfty+g/aNIJcdPTjGPPJh++i+4rYqmgRQzq0iUe1yWa8ivefaWp+mSWFK1by2MNPjBnlXmf40hfqM5P4hrUCFk9931TGU+p+4mvSswobJiCDyx3fZnihzxSS5MoIM1lGkWt+XZRyMY0A9ndj0E210tFcpElNjHR8oRMTt5HEpNLW4LqVMyEGzAJ8KqCr9E8VoSVFqKFHZE3X7Fm7fuYOzW+WOKKpXcMzgYAwFy91bAgMohtgYof9POC1tfj/ILS5CopbTI7u6kHDfRg09r3mIoNZQaXgXrFKVcoWu/rHk9vkcLOorj8/bmeY4m4jJRgjuhvlBeNeNFlKMciCkt24AjnuG/NE08qW6FIJiU6Rc0FBlrxJg49coQw087rv6GMwvLWfPgYvZm0VDHGjiGsrLRhgeLm3Dtyx/sgnWpVSYV+qsKRFQ5pZyOpKshXRchxwaKbwLi6rqRgPk7qZ2Q6UIIBNkJtBkyoznLrlP/8wElE+tsDLhqxZPvSpe9o+oxaD4qb107kTfny61Y+IE1HBBnYxpqYLUTHZZbxRwbINnFB7FLdxigDGtVlitVsW9d1pOlSgWVFmvYoHN3+rru98CXPvZymoB0V/B4QyB2Efftz1JTklRJ4Cfwzt5YvvyMPs0guRERNRx9F2YNO3x/zXgRvlMAGep704shLZcQIPDcc9BoHQX00AhWhgLGzd2v+RNrEdPPB/Q7iHpRgspwQcL088ZRBkUzq6riPFWgnBhlirkS9xipsyjMl4fPeOWUkwbFZoUgmtOV7D9UXVog5/eBJUyczAyMpgnRGeZJRFkQVB5a6f2UCa/TpEd7h8icnsfCcxzpS83KcjrcI1gYkUvcg6vPC81oeQtJCcEZE1fPzDhFttyEx/UPROZExUQaU8qAmT9UPfccclIFbZSTTywFhC6sDU6W5RHyEccYfAIaa1ST0tkhDqc5KIQjWVG3E9W3NTZ4CKU8/lOT3F25xbOzs+xPtlgWq/quXzGhAUf2q7MKd+eZK4wi6LH3qXuGDuFgvXZkGO7/mLwnqD3IrWCaSlyztQWRxfdlyXRVfnIiIkzgbsihi19xaYomzJiZdujtzxQqq51SpHNdQBAaunCWUDhxXGCQ0fPOqNCS9ewo3ZlvLYQ8cEt3WwhVQc012lXtMzs1Cd2Q1KfNAzEvTiq5dLU0kGuPmO9xwnsjshHuBmXw0RxzCQz5iyHyZJbqI6NxTtj6lPVzFnrOyTZRLOmipWKOqly10WfpJ1WVbC6WbsqQrsXUjZG4RRypuFc8pZIjN6iDset5TaoDSqgRDlRBWKXVh+4bNdG23eXAWMG0T4/bhylzfJJkSSd0rTarHF+67wGS9zGyekppnVhCzlnsGgKeu7kYmM78bNkFDDiQbPhBXrh3bweV9g96ueG5t1tao2bkHnFvTfgyJoEot4rcnQ9ZemgFbqqoUl6gP4uthrmGKmiUH4ZHFTnczkwyZTjQ9MNF1KAMBRWzReDcTYy7VxlR2gTrebseY6ZvlaTkl1Yq6AuB/mSZoZ19XkLhpXhMvziqrihlvzr5W0UCoFxayU2afyyqJG8t17iRGD/ONasL4x5JxVIUUhRB593fbVJrJj4zDG4fRaL/re7nZKNdQysHjeuLV9YFOnNe0dMVfn0jS4KqGIt9YzYPjCAvFuwRvNtTk7cyRLrEvGHvr1l7xO56huB7gbGqmvqhaPp7i0WBoMdjvv+ayvNYx1bN1xGDp0G4UBo3rnKxXJvlaS23S4t6Sf1pc26BqJO1+HmMwBVYDhURi2SbHwRhGvli7IFpbVKyX0KPIcJqi8AIVUSMzBzkdZTMNVHI2NUyiNKPjiREmsO9poBJYQrodXlcX1eNQpKqfjFsg145jmsjZTB1Vg/fZ7VzzMY8BqWCggjQ52UC24+D3r9ryf9pNhbckFCYFXiLMeYJr02wJ3N564zMPeePNs3Ll7Y2MWGTS+6ZNa0Re6VohYIQ6n2b2AsV701CJ8eE430W+iK3gTRNlCxS+zIAt5Ctt/WgUZASH5yVDNNivPNZoNHH30Ujzz2KG7duo2TkxNM00qFTjn3URSpHVaS+6tHyjbKwIh9Wj+KN6BedBLx4dyTfZK50WgFC/kMrmVhFKE0IdTn83mFGuQOqV31j9Lh/Me7y2N5xliYewVCYOHh+92Jmr/Srv/aHHsmpMkDAXVIkzXdcCHVEtFS8kTalJX/ubeGiEhvsi1lJbcMRJlWel5VgMNiyMStNU0TUiJkYaj1OeeymdcHfLApJhDGbrPXC4A4KVgKkwgK0+yKQCRl5suajNlOZb2uErYzO3axBWEdemYccTlZQ7mtWEakcIbyCxZRP86RGXdWoYJrYrXNawYQVZdW6S/JvNclsFKH4NcLi07Uk6fL3cRJlnGQqhvXWSfDiEoyOmvxgeZ5CTMvQmq9XuPs7AwnJyfYnJwgTROo3sRbPKuE2eOQqRsb62g7e+qTarpIMd1r5cjWByXZ2p8TYiOhjIj3EescK1LtAsA47aK10c9lAVa+j9TkcT0PmnykXWvteGWhscwG82+ZYhsRIwJJGuZ+/AFRxFiVdujywWHC+UYLKWMRTWi0e2N5vbBx10O4/51UqD8dYwr6ogiBOsFqdEBc0M1AvW9HDuYsQipVQVWZIhdLKrHpi2U9qhGoZLDpZWtUDkXITf+EYOAYWlnTMaJcElDWV+sLVYvxmGW7KgZNMGaulzfC4Kl5uvmvglWCIKqgAlBMHMGDtBTLWtdEYBB8lGd5Lkff+nJ1tjEj1f5mvb5BpVkvpDvBSKpctOtZY6upfnWWd9MpbVcEVaBxilUFC6+pa5omrNdrAChuvvNznNSIPkoTIGuEcjsvZH8ZRXdRq5A3cw9e2HhXqH6cwsMMqseviKXATb3SaBgJmwYuf4O69hH1P8zzX6KFq+awVCiMoY5Xo5AM77eyt0NYD02MXrAME4U/9ccgijTqjnAaQ5OPQ3lCrLsTxG5JxYKGzNr6ohBS6p5BJZbAOfxgRq0GsFBhf5JEU1xzt2UVwarl5k4XYTDydguguB8JwDzPyJntlOtaeaKEXG+clYd5W3dmB0LJEJeftxLI1UUAeJYd59WSkcmotWAHgbAiIdcrRIhzdxni0hQxATMDDEy1TgJjZrlevR6BlMpkJle2jCep0LcwZdLxFiEw0v+qgapAilNObig1mohrShJ001IKAD1/kAEQ9+zBojt3Mw65AzBY61JHNsVAla3aVaPgSjeBxsuWC4KcfSiWgp1/uJ7q9RubNc7Oz3F6Wq6A35yc4PadR7A5OcVqtQl8NrqJCUDqSSZy69AhjwlVtEGwcBqzZ4qVJhGHLnpTGH1ziojHXjl0wcaEmxx5ENxEFMv4eosSl0zRa/vnv5KNV1F5qO4lO4T5PqCEKg0v1Jh1zsv2HGoHAl5xSaF099X3X/rF9YdTjFRZB2BXC22tIroG+99A2S+6v6MAbrqQklRlRcGjm8RsJGuBBvKqET+tBheqF63fajMNk4elguAIvnk/jbzuy/CaPFG5ysPZ0tavMLoGT6iS7betW8jj8QSx8FCHS+9qGZZaTlQrKtq+WxcZudtqHji3Hotgcu98X7pQY5JPHSmKMMfjXJbYWky6vzEwn4rIBaFE4VunX/YKwlLjBmyk67ZGdj0jK+LxkaaE9XqNzaacJgEA680Gq/UaU3XzcVUi2FG6cG5VdPw6kgefBwNiANY6uOmrKA1+hvnggwFqmvJCHwvyxLT+MPd7ZUHyhKOrqBmqgWIX5LSjpX07rlor5CVJOm+U8ajg7ds3ZmIeoEZpC/mtXseRHK8l7bdaTK6ubvsHDu//zRZSIoQaZhqyaD7RQmmQwb6OtCslO10UzN0gDjmNm7cpEaapuFOKv7/+G6w5gEgjrLR1YZIKoGysLQLK9m+2YdgeHFJd1sLeB3BX7pRgxHWs3qeChRlTSmFtj4ic69O0Of/du8n0ckP311uT+owo1CFemFF4vkEp7DFOJIEiVDSsoxe6piMVilKrd6DQLPrmnXBXEFThiAIrcbK79gQP/mSJREjThLOzM5ydn+H0/BxAWZMq+6GK5ZAogakEChcLzQGZGuwEF6hDV+irw5UesGxz0tlR1ulh4qXprW21Vqx3/x9eZiCo5MVovrs1mLlVEpsT45uWljvzIpJG+/YN7k7s+cFCFk+7hHJpol97rfja1TO19I4ADbjpQgrAyzXgx6Sd+jTtztXCHxbkaWkas3EtLVdrG/E78w263f7L+QdgHZ9ILMLB1QcNcxi7ycgY9Siv/unLigu4JNbfi3VhWfO14fP2URSafR/EIhhW+ZKkbrxpjItByQG+adEq1HSA5biDxXX5lxn4cWlJQL34tL9Xu9IfJ1cK4f44ZG69FGnUQxNiu7J/EQqpQWpdAoCtG4Q3UftpJ488bTXIUEf0BzQlAfASIzssLRJ7564cWUSlz95FM6at3VahnbQRhWet8WWakA5meMJuJ+Ky/bv8u7pydkH+ojq1TyMNkIRirMJtD45HXWzlpP8prtJhHmcp7KJRZzkE5YN8lp5Gdsi3o1I3Xw9QMpbet/vc5JkgMuwz1Pfx9+4GO9N7AaZI3/08XoioDE1xqKN7F160q/Sjb8tpcaa5wjx6iNH7w9KNFlJUA/DlLzAaqGwuq5xLpHHqyU02A/eDICc+1PzMmN1b+8qBmH0LBACJMK2mEoqdaoSWG8xATHUNoUT51b087CelMZVwTiEqQ5DXDD3UOLiH6isJujC4XYgozMQfCUCCwdOKDM/wdJ8YoHcXJSrup+RceTHl6jar4esQJivYLHDZEUms13ssaXgBUnFT7JyWBH+pnT3dbUVFF2t1v6jridGu2zjSVZeLkAKpNepra+QM16sQHCwlotLOcZjWK6xPNji/XY49Ojkra1LldPNKJo1FYmuX9fy+BhO6LSD0hsN7n6/AhgdItsoRIUgOhjZwaf/YloAdWw0rZWpgUh2Adp2vaWDRJBioRIM8y4Jlf+ld7dLgWazNcOn73s3icRs1mw++8vWN1+hfnBp7o4UUYHZO1EgbpLDTUAr3VSbt84S/O3SNPdas0zArSXB9nwYTPi6sGPzq6pNX4qZxhMFSdxQkQVBo4xHyIemzyz4kNieYaOQ2amBv3H1h7WapDvJ9FmZtqBmhK353wr+zJqybfceMKUXy6eEbCaqI/7j4r+9Ve4i1j43gGK3aCqhOCRp1qeJ3tVphtV5hI4ESqzLtKVlkLDzMENHQ48yPpSlWYwj8+Dfe6Z1p7K7r6TvW3Zdpw/+tlnhiiE1X16cGsd0a1WBt0HUg1Nn2ydOtxDh68OMm5yW8LW1V8P2sLfj+eVV6YLXZ1x2CSpiEB1x5k7TXuruVsRztjr3ZQqoyaZKFTco66DWD6LK+EPLo1nOEi1gjb3eaXJyTvUUmvxgOLhRNtQiqakmh7AshyuVCuIZS27WcuNhv2nknoBrZ4ucTVHss9bSBuzo7lQZlIkarwQczWDsN81pIPsChX5sqsCytrSxZL8eue/X20YtL3SI9AXbdiLOkavSchyfywoZOubEApXqnrLRKg3xyzuUk8/UKZ2dnOD8/x+mtc6w3J3o2nyhMGYykHgJT39qzIiOdjc/UW8bRYZaUr1dpfEFolTpNULX1HNpO+2ykXL2YtEuRKO3Kf33JY/E8atvacfxk8O2YSpW/dXhvW4XYBRBv1bHpZgspmWLM5ebZXH7ncAgqaz7AkKjIqq4/UXI9zoOAE9PWaVthoMNgmSCxR5GJiEZOnMB6EdlAG1wgUp1EHKSpk58m1JyREa0PRJHArlzcZMuhAoLd6NrCaP5x0apq9KH0l4q7VS449DfzClQWRYmKJ9Z8YoWFLg5FjolZxUuFQZXAiFFtm8h07V1J++/oC2ht+2qTkPYotE2g6n+tUAUzMDKVaDlGS01cqGwPMK1WODk5wen5GU7OTrGqFxiqK7CaqKQbspxwcO0wy/U3cX3EQzpK5DLtUFusPiccdguZqD62gtzKcvPbPSXA7nryplP5awIKkPuedIOu7zDzTiphUTS6OUwGDPdttnVoqR0Cax//N+XUaPHgNPCsiEXl5+S+KrzicahFlfZniekjH/kIvvmbvxlPPvkkiAgf/vCHw/u/+Tf/ZqfVve1tbwt5nnnmGbzjHe/AI488gsceewzvfOc7cffu3WNBUUYI6XTdfLrrM7RCRu/Yv5OgbVaZoH/1QyjHnNcD6Frtt/5v7i4/QSwZbOieD5MCYwJL+H4i0k2G8g/uG5onIjTk8sFEqX6m8tdd7w6MJ4wXnIVJ2XEtvn5rp3/m34mAA1iMUBV0lccOP8YhwwgBtY4iKP3H9R2Gg+6zNAjaSVlD4dC6p4kwkoYYE8La95qFmjGlOKIF1mTWaf1Mqwmbk3rL7ump3RElO6mJgHrKBAbh/x5G9rB6gEK+nr69HsF7P0tzdmBVVVwfwprtyhy5L8GfFu7r8H+NHxSMjMsM1aMBE46/be4LZQoORvCP0qKwouYzyCD7D3UMR58j0zL8IVfz97B0tJC6d+8eXve61+Enf/InF/O87W1vw6c+9Sn9/Jt/82/C+3e84x347d/+bfziL/4ifv7nfx4f+chH8O53v/tYUAJ/BteTHLIRSHbCJZRTAYQqhOT5qA32P6owBDgXd4y68UaJEkDlaCRU5j6lCVOa7PgBQG+gWBJEcdKWe7NGO+kNK43VdmTapRkdXl+cpOLiaz9+v9Q0TZimpLK+g0mE58BV2ObroHFKh6QWFoMxthPqPLD/y9piU17r3j9WozE1nNR3U0Kqwuns/By3b9/G2a1y9FFarcE0NYpVgUFsQWXFZH8PTg/iy+mq6HF2iMbd41t60mZsH/Rz5UHnzQOnnVbFSwfHYfbOy5dEETk2He3ue+qpp/DUU0/tzHNycoInnnhi+O53fud38Au/8Av4jd/4DXzd130dAOAnfuIn8E3f9E340R/9UTz55JNHQCOaqemtDGNEBBMyDGGwXruRKnhRyEut0l5AMos7QBvrYDMtWyZCArUc2Ge3llSz1gvjvPXnYabIH7Sq4rdSvirCPPauIRyvvncgUiD04mpprqwAFQaX+zpFefPWmm9GrLuk/aH4zlmJHsYR4Y/Wqbxbkly+8q7paYWhY1YMPSaprTtkUoJYVjy04UqX9pW1hpLHLKvROiH0eWWwKWGz2WCz2ZSTzVf1AkOBSIwRijXoQa9OeZK/4qIeidzoGuPwzk8Ma9fR0Esk2FqYdheoechOavHubZJ+trzgSFgdBwgwjda6zM2J8KzNty/tWptldoLKG3X+dwu8VWRZd+Jh/7tjh/xoS+qQ9Cu/8it41ateha/8yq/E3/pbfwuf/exn9d3TTz+Nxx57TAUUALz1rW9FSgkf/ehHh/VdXl7iueeeCx8D3xhVbqwm756DmtSk5RgwywvRClsy2UPS+lCtK/vEVF1mTmt3lSKBMLkTAqj+RwRMU9KQ+TlnzDkjczYLTlwwAaz6bgc1jDV9XqSx6gSD+fHNM5DUuSMuxiixiQBKDBJPE5W/UypnG5QzDoBEjESMiRImSvacAeQSLk/1+w7+79rdYVEhzsND2ACphGSYqykCItYu84ycZz3/sBdiJWVm5DxXGiy2jHdJ6YklKHgueIpWZRLXHYCUygGyt2/fxu07t3F++xamVTk8duaMudz1jAzGXL0PM2tr9axIW43TDRwLCDqU37x4UbQrZfvoJGyFZQ8R6d9qRxLXc4cZ/XUeh6ddHrMhg++EEWF3LQt1PwiWhZQfKC1ZnPsqPHw9CngZAife9ra34du+7dvw2te+Fr/3e7+Hv//3/z6eeuopPP3005imCZ/+9Kfxqle9KgKxWuHxxx/Hpz/96WGdH/zgB/H+979/8KbxMOfCQzVwgoBMciV4G4AgjJSq5tpqryMlYzAglV+2V4PLgm7Qpqh1HxWNuFzXkRFcDlkOO60WVUoaLah7WkSaMUfQ2LXZgitWmE+DbiUaE6DAzWI+Vu1Z9SwyLVFwrhaUutSi46FthpThpo6YdRyZR9uYQvldKVgypRTK+WP2e1nj36O8wISh9MG7KGNVcvyQBGwMrEI4DbiOu4+upEQq3lY1YOL0rKxDrU82JZqUUI88oqYug4lc/eaC8spGb1W2fTaVUERAb1GP0qHBAcckf39BFwQj85achZdRgiMorhEtWSfL60K91aFU5eZFgdHKtDxKfo9wM2by0im4cR201ZYSGJ1F3E8s4zEGo6cPBrv5Q807Cywqz48Z4pdcSL397W/X73/5L/9lfPVXfzX+3J/7c/iVX/kVvOUtb3mgOt/3vvfhve99r/5+7rnn8GVf9mUePUXjE0PAKETJlKsQkfUWhmcWdseMEmwpVia3UlgT82XSqXcPMDqmQzUQQYRROYmZ9XcO5r/9Jd4dZ0bUwBRexmc2KaOl43qtdUbfvGfKFCaLd6Oqywom0GhYn09OMAgcToewdjk+Fy7TwGxtN2Xjg1qc9DeBELP1ZWJdy4LKT/zdEVmOG/Zdsj6Rs+RaPDp3YUpJAyZkX1SWSSHMq1PW0B2Xpb1z8It797AUXVeqUGE/rrw7bMmV6wp7aPWRzIn9EZpQq0llLFVusFB0d41CmmM8eXpQ5a4ynmUXtdW8y523CNgumq7wZqdglGnmlDfrVKzDafQN1IiK3Hi8DxVUL3sI+ld8xVfgS77kS/CJT3wCb3nLW/DEE0/gD//wD0Oe7XaLZ555ZnEd6+TkBCcnJ4M3CYlW5dLAzOB1c98MGq21MiGPb64SpuWJMmYsggpBQbG0oG0BUA1ZMk3ThPVmDeaM9bTCzFvkOVdCXV6sZRGhiQBOem2G9mGXW8/DrPRGiAQvREf2ZwjKuB0iz5DYmI1qVVWWzxlMhJy4ujxlv1UUhOY8jMSvPMSNVGCqbAJ/VwrMwXqh/0fs1rfsBQo37w2y9nnUUnfBlnWwZM0yuGFSCgLK17OdZ6RpwmazxqOPPorT83Oc334E680aoMluGUZbNrLwEE2nH8sxHSCgHN8KSRSjJeXhUKGuefYwXgmMIqprGl2VI/FVx5CB/kpmR6cVVgleCooRtcKt0sSBDDl2vYVQfrtDlNnNiIO4/ngOJzJFPmQTntEojB3c6Cjfzeq6tFCvKbIG9on7CttBuV5E+j//5//gs5/9LF796lcDAL7+678en//85/Gbv/mbmueXfumXkHPGG9/4xuMqV7pxEXrth1lN95a9RKSSCSXN6wIV1EyTAuN1q/6942uARfhNJcovCfMZpaBhm/aynxE374gCcyMnjGioXS8T0NBd2FThXXwk8CpUY02ulYtqfQ26dRhpGyzjVBldO0Bwz/xnUUDFOmO71vZorVNwOQKREMdLxsmsGidEqBxxtF6vcXJ6itPTU6zWq3rskQwC0AyKA8/BOfo4Wo695TAPQgVDrPR4kGdDbC4pX7w8AqO8w47B0SNLP5yg5hbWZdhCP6qFHsgHfX/1o3TVAr2rhwf0Pg7g7o9DUqeftvSvfLB8r+puwSe1bcc2eDwAe9PRltTdu3fxiU98Qn9/8pOfxMc//nE8/vjjePzxx/H+978f3/7t344nnngCv/d7v4fv//7vx5//838e3/iN3wgA+Kqv+iq87W1vw7ve9S781E/9FK6vr/G93/u9ePvb335kZB8AkFo5RVAVlPXB2ZXFZ3ci9oAzeGWBmYekUlxOh7o8Yt1gRkoTplW512cLAnIGz43FwAxkN/Glh+oyoQFhu+TUmuP8+8r20EbyHZrEgjIBQ+F8un2U2bVp/kkEV+tCWoqc8u9cZgePU0h21PugyRiU3CYY/vTjRKW/ozB49vUAQEpYb9Y4v3ULtx+5g9OzM6w3G4DI3RfYNLgL1u6X4U8p0uHjxYQ27/QC+DYeYM7tb1wq9w8WaGVfVWzuOqF79sydBvk8jb4IHEoX/PgcXG6HEucpobA9J4UY6IOHpnIW6ECRZQCgjAeJ0jjakvrYxz6G17/+9Xj9618PAHjve9+L17/+9fihH/ohTNOE//7f/zu+5Vu+BX/xL/5FvPOd78TXfu3X4ld/9VeDu+5nfuZn8Jf+0l/CW97yFnzTN30TvuEbvgH//J//86OBf8nSontrOfvxDbzIdCyMB7i9jqnnmKrUMnoQGAb9bNeaXkqG5Zt7MYJoBFdnoL6Iune+3/uWYkcPoaWXQSb8aUovd/derFJzTDqSNSzX8yLm1ksFwygdbUm96U1v2jkA//W//te9dTz++OP42Z/92WObPjipNi2/60fcxYSWiCz/Uqhw67rR99znbIBpStfaHNNt9Q7Cjvrc+6WF5QjRAwiJYT1te85k70CgB9KYulqcFTWGbWmE0GmqUl+3MD3A05I+6tfYxHofQ7LQd/Y4U1PXxhMPONFJ9t/50x5cm5XwZQ7sgdKMLkW/4aifHxX6JWvV1edTzLZMqxbNOapjN+3vDFhxY2HVNMTMDi4CeiAW+ENf03IXqzXvbNT9ik07Jjtpr7fQY/NHzFNPv01zo776JYoXk74Azu6r3xhqZkbN2IZQHCSTQ5oEepfrK/IOh5L4mpcwPmAxngCUJyVMKZWoq3nG9vo6+vRLpkjllecXbx9XnuBdCwPjWl2DjNGxAaV8U5Li1KLBaqkPiIjlBFSBq2CUmev5iLLfpq/T192+S5QCse+aUt4VNQrxhXO1jB0S3Pct1O1wE+DmDpeCEg5ccDCjc8FPMHT4EK22OEFSSticnOL81m3cefRRnJydY7XeIOfaG0I5CkndTXsEVWfFGiNjnpt3UUDtTiO89kx+FPa/b9F+V1p2o0mffN3NfA2CZR/8glvBiehpkaKC7lYHg+vVNB7qpURduw7IVp/bo9Qtp3526FoUWd0yLpYzO/7oeLPjyQ+SbriQQoPPymiCcKjE6Pyp7PIzWeg5tcJCknE6SBjuKN+IACLtkcIyOtpHNvkWJkrGWCEnDlSB4xjHAMQAUTwRohU4A4CVCts8MbO3KsaTyoSp6ZburQjYgYusdNMEpu3rCbUPU2dBibAShiD04GAc1bFUd4RVrIdS1zBcusKwVKePXyC4kzhggsqvRQEApYRpKlM3pYST09PyOTlBSiVYIus6iOMqjajl7onhut1isKhxh9XygYDx7ouQBrjaJex2avwD1T687d/vZ9asQsew31uL9l1OKXHrsVU/aXvVziTPk9oQ/PYZAxrd6/tBOkXiJGkVkmU69OPY03JDgYMx3YNRAogfbOXtZgspESyQaJmypdEUox1aMbvLEOV/bq2wkvpw5YU6dyVv1FCvKbffcyIguwnC5cQBRmS2e1MTHGAaq0dPq4Lt6EZ1J4ndsdwu9sLZvvGh0QKkTva2FwuLxIuMrnUdHpn24btfLKaO4eyKufGuOn+OoAkq6+80TVjXNd5pNeHOI3dwfusc5+fnSKtykWGuyk2po1hSnKtlN9DHl5Jf5PfWnsLsvi8J/FGfW8G3OG5cvB20L5/VBGB/GLu2/2JU/JDc3Bq1437vrGXgpm4yDGU+8+jp4cnT76j5Egy1ULYd+CafKmJF88LAYbIz3WwhJbqgnnLM9kzes1we1dmmVgWqgOOWGcnAFKzanoQq1rzB1pr1bq1BaqgvCgOaJqRpwrReIc/XYJTjkaRASqmcpJHnYkk54jS3pH2iUE3K4AtKxKlZmJ2sUZgl02pm8n85woOUuLxOaWsfckq84cThgqL2tIs2ZYLmekOqaKcAWYhPs5YT4UbHeMTyNc22vm/XptjXYpM+urvaMq32adfBjFyXMsn9sVip0tiUJqUZL6RKOdOoKRE2mxPcun27lJ8mnN+6pQfIFo5Qr0JJCZRSJfG6fw0sG/i0u7qR1wmkoQVFrVVirNlcz210F5kGz9aXNjH7G7Dh3PcQD/eildHUBB8PlhWCPpsOOQV0hP7q9Rxdz0eptiYkWiuV+RNhdNtTvCXbWE+pmmOC41J8YLXXecwMu36ILNJWW3F4VZ5VtD613iOc9l0uQJKbCcRpzrLHD6Q4UJC6VGFkRr8XbZxutJAieCuqD9m2r8Ki/dtW863mffMMYrMDgaErvfQtNqUbRgXoFfYiqPTII38PeKqs1R15oa1XJrC0fmb2CKnwLQ9cf0SjcSW8gCKy72bZ+D+GzX4xfTDpXE0dowvWhk0gPfqHXN+ddeVblEms3+V4JqdUKACLQicKKW98deuG2nLsd29wtYJK4De8EGHxahCFpd5RktKE1Xqt0bLTNGFzUk6WIEpK6XIqusLuwNzlqm4FVOtRaLVmVf8qQ+wPruxxPBJUrWDzxYMgXPA+jK0V4Q9dc00HPH24ueCJFmO8BUC9WmpampuzozLNk9aD4+eSvG/p1xFqGMcO99YZFg8UnIKhaLMZ2vsrGppnp56zlGiEquclSn/L2z3adKOFFJOcWCadzgBmEPwCLztGU54XDdVPCmP67fSRtYydcBwAq9gySIX5pvW6HBLLGXm+xvYa2F5dC6AFogVNpG006uyk/8pP0hte7Vlx/5ggMoaZnebly6i8aHAhVsNwPabmDefM1SaFCXvGEwIeiIxRNMxiKcn6l7DJpOpyA/vA9TcSQuO1h1EyIWv01jPclMrRu5M7BV+6NwqUiIw3IU0Jm9PTcgXHI3e0zs1mUywmLrfr+jYYwDzPVcClIbGKp6gX0q2gGtzI7HvI/fx5sBRqfXkTc5hny60eAROj7HPUCmlQdDfX4EbouBfhfZuXCRqcsWi1Igqotj5TCg/hbOZBIEyq6DJmE3TCz/Q+r33zKaYbLaRU53GWVDy92ASRj/xTt4cMgrMQPGL1YZBT8gBOivQEbHqVg0fMdlSXXyKkVbWkBq4VY+aVrzo3jTB7Yx8NQYq1hHr1hc9HlYC7sCmujpKmruq6MgFifTRwTdCUPCZsbI7SUBFoU7TiPEJ60HwyjY4VNxpZ5LXNzvJGpaElWKS4lI/wqc8dToN1Fp+0n2hgSdW6THjbM68cTdU1fFoDJDabjbbnT0H3GnGrwYvgD9eoaN+qZe0EktfadR51GFqkwJiF4oO9PKqZCgJ/yLJDqQjpCAERtf/yxYmDvuqRVYj2jENWXjGaO+PyWLYCG/qL0JH7HgOnhB7YPyGf2+aeCLC27xQ64AVdZVjwyxKCy+jx2ieg23SjhVRVWcDwN28ysCCo5BdlrnfklGe7tCc1pHQuOuYh4eCa3xNIr0tzFQoEBicCTeUw0HClN2AaqUgnT37N+HrQEIiFaj1kDLPW6Za7EcUoIUnouHGGDkmemDsOlMiu0tBanYCDhF0sJcckaPDc9aN9ay46PxHrxSzukF7mciGmF1TqWg1NREZjgqqBhVuh7C1EOwneu+CaloysVGiRKhopJaxWK2w2G9y6dQtn5+fYbE4MDdUVmJ3GLl63XMfHW66de4t1l0DolOFGn3SCgHQCUENbaPIZK4y6gRNyim8vwAVMCn9j2QUB5SX2Dpqz8KRFidC13ZGAVwbY2vXKhy+nuVu8EwNOOOSmLYWAG3hC9U7REG1N35vgGDXv54jMI21blVpyYynXotR3JIKqCUKTmAEFbd8WC0s3XEhRiYBjqIbedttIxaM7h7fyLTLQuA/JJMQSYmNdo4H3cHANZtBbYVUbrhSdpwJnYhATEljddoHu1LpyopHjgmnLfDrxSYN3TVfNUo2MpLx0fWdPxLHZw0hSRnGgKbZSf6GkcITsmZgw/lyZdGZYsA0ALN32Szo5W9dkhG18eOqIufpcel29236g1mdKoLp2uTkt18DfvlMOjjVmYpRLyYJlMueip0iDGVUwN91jRr1ECcdquC82jW6XJrF42VyN0zTtrGfkstJow5fxdNKhQM42cZaZMBu23bxg/W85MRodvMtQ+AcN5spwzb7qNSMZz83txlG32RdhTM13qrxB2j+c1m6+kJKZ6Afba4BOyoi+pEnMJJ3JrAy3PapeTdowolHT7rmnaY1LOg8A57LxVdeBrVq41M4Ct3dXBdXU6rbHUrKJtGO46KVYVmFousrsXBkVwa1VELDHcRZ2eGjaCFhkwYNLMkQ7JkgQoyMBLXPEzUHWc8gErGaSyTUttT/BHmTRHMcnMqgYcZaGwRjXeTprglIJOV+vsTk5wXqzwbRa6Xuj9LZmVsNamIO5ZXzXZGBbRa7N5s6iW0o8eisTYEm422tu6Ng0eYO7swwqTC05iFWttL+Ds3eBNaMuDGmupc2+DVp4V0Ab4bs94X0XOAvMpe6lHG3TsCfssBq3OBySjPd4AKgqyDI/OMA7guGQdKOFFDGBaIIiR9ad1IDgOAtggwLH4L2Mivqu0+AkX93JL/LKuJzVHsatMn4xdcW1x5yQuWiT03pTrhpp1khsDCk+B4LpTBgQgQqx+lZdh6kLZMieaLygYlvr8ElDr2Udw2ONqk2rikIGZQrrNoJsaqQSe1yRXDNvAQDaN/WBDJiLexRuCJZ+MRS35CxAzsBcA26iey7Ve78ElVSsMF81CR5zbVdAonprcRVUTs/x+AJgkZ2Omc/bjJNpjfOz23j0scdxdusc02pdrO7JTAQ9ZLkSngasSJsQhi/5W+YqQI0FVXRzjhiLCG+/7cENicSQyzuMvveBGgLJzAgCXgq364olMKV6HTRfjooYerVq1B8A5qUAek4LgGVPZrvOKHQ9Ugpc9Q4tsZ1B3p2poStiu/5nqDvD7RF1NAwVLLvXjTnHfWup3nSetT7yIwBxvufMyHPGPM8oQWxfBEIKgHrICsGWZ42uJrNM3xVeQCGnKfye4UrdUE2K/ezbo1W2efoIMrv/qIQYO8aDMojMprVwPE4cJlQaUFqNrgooYb7at1qs1dycM8IRec/Y1Dvp3jGK9p+qb50wQfdg1ByeCRXGVGFqZ5Ss7TScXUSc77QPmpVxDAvYmt1ccNlXQeWaer9nKOKk/vXa6dJMFqZaGauP3POTe6kaqX+1WmG92eDk9BSr1QopTYVOEqmgYSAspQ35kpANizzy4yUjI8iISPGBSHv5peoAVUHZQZO7XEWjfVqiuCxNOcM3IG77wm7r/77cYkdI2YUJ1doX6rpggokdHYuUVmYt5b2ABfyaUG1oCNYhMkrrhNNzvLLZZeQwOKNvo8bJrpyA76rVSoEl7QX2wHTjhRRg+l0GIzUIKGM12psxUL2HyXNzeGk2VlLQTPPGzTFy0aVUb+xNk8szgxmYszf/40KmPnUE2rYjC/kKWxMMELUmr47tDm9QZjDyuyuTlgd2Cojf+CtwGo7G8hAwKySOG/V5VfpG4UbuPWdGSlSuSXHjw+S3FxLGo9sLMD+WaoXJga9qRw3KeZj9KyqjstmscXp6Wi2oVSmfEkCpXgXvMOlp0qPIKTEqJp2A08Aah1vP4oug6vs5Tk4YsJsfZXK6YJwRCiIttdsTSvP9YbPtml90lzoB5WVEIwCtrlKOBwJDVgZ2p5FAHxXijnyDDG0aogb2pWQCD9r9qIuz2z9Yf2N3v2jwq1XfROUUId/zVKcJHi52Nb3slx4+TA/Tw/QwPUwP04OmG29JHWo4qtld/48LsKILjHfUe+mv9bAvWfOM/QHxUbMgb+6+FLTBXeex91BHWPxfb0GIokOurO9TgAvVazHQokdhwKFfROEQzN6F1B9vM+yf03hVg+5UXNeE/h5jTtslp2b6fjkbQipjJrPiFLFLI2OWyFJ/Ahzlh2vRbQ5I5XSJNCU7f0/1XzRQGmwaHSd5iTtajW1Ht02/au9dZQtrFY2Z4d192mbFHwc8eqjECmz09IZOhq7EFhTXW3XPqx/XrLtgZHhQqH3QTXbYGm+b14+PhNcMbg4Ylov7mkZWlvVzmcZ9qTgNuQRTaMdHSPAt+3eOE3ZtM8L5N8GEbnMen260kGJhvNVNsxjtIvlR0JkawihF5JClkaHeoHbALPsNkr1wouQGmhLSxEBKSJVWTk5OtA/XeQbNFqZb1rMG5jRLv5pUm9eTtMhN9PD/QCg719HSZND5PBAyJaQ6TmBdSGUG8kK51u2F0v8A94JQC4x4ALIy/7pukam4EbJ7nnUMmwmeXbNDdFSXmXPtBVZJIWdwY2XYupnw0jRNmFYTbt2+jVt3bpfTJar7UGiYff0uSd1+S4Yu8CsDdAyfZC+gG3Tp9KiFMV+GF2YS1CO0R0TQo/lGUmGPB0jWcvXMQ17QV6qEmqWMb0xkVFOwC16gZQHYZOvcXvZV3gw7PSjp+BEZPewHo535Dip270ZarJ8wCQMQqcMDSbGAM7cnUbWdIUdywTfHpRstpErqSaULAybUzZyV+VTKLAzNBnKkJ3LVCNj979uNpxDsSKK01IV3LyglBD3Vo22Aokm35y8GhXDgU3dguS+ktNsyzCVNpwgq6mhb83AUbe1aQmnOlSaAkKqvPwFpLPxESIU9Xq7NsLbWldaMg7UGCn/VQnP16MI8AIsUDbUstRjqDzy8gmxh63093qoR7E+rFdabGHI+cxHzcrtW3ANmIIqy1Z0cojKI4QmrnDQxCi8vMMe9R32ot/apagc+0CcKxvEckW57xcnjZmnDrhcui4EWbUYIhp3gYEerwmQDPXdgDROrdlSlnLPcBrnhs7snC9I3/jz6PtFGjqksgdBnzEOV0cR2onSyeWOwswhl7YMf1cPnU5tutJDyWj+AGorLNoE97Tk1wpisEziVOKNlBYj/gN2/dj9MzrkZgyayrMKQ3eS1VqqQohQ2dBZKiQNpk8kWd7tw4g5BJnzJCdwmS5fESvXCVZgQELs7ElK+gRA4wBngNBACTf7Dadhg9nV6XUJ4hnE1KFbaoI1QeAHEAc79yeYtTMY8fTCJB5WLcE1UL8RcYXNygpOzE6xPNpimqYbtMmb2DMHprGSs1/ZweQUM1ueml8aXq04sHLQhlIKjntl4dyW3BELxZ9+6/x2FTLs519y+3h6Eejd1TrTVe+uIXIEWFm77sycUu1WG60fDv9HOtn111BIdfKErizXaPDU52/YyHMOFBldSxuXZ56FwUOvcRqVnazwqN3Gu7U83Wkhl1AMECOCq5Tnjs2Ij10nIytApDGQlLiqx/f4sCiEI1Q7JNOOlkwdqw+GXsQ/POigSVHKMrro2chpQmtRJEobMPfE4Rl9azSUqTCeQgDmsGKCywTQ7ahKNSyKg1DNHkPNqffFWilVhzKWjDLBeH9LjL+xfEsUgRH+V0enBZ/jbY3W/ipapudiPRuxfqvlmWF8tD9W+j0/y9vuT/O8eggBx+Z+KgJrWa5zdOsf5+S2cnJ1htV5jBtdzi/pDdw3VLcP0AzdoOOQd0JCrs0yTMXv0Vu9xzLWNDPUwWM42wm+5Dwpp/CoD7X7LsDKVbQdAsC0t/45mixCtB6u6ORHa3y2fBn1ox3Cca3HNE7IytFO0DkmljZrcAUJNpuiFbERjvtLUuU94+3SjhZR3wsmT+B4As5un9n9ArrcwGtypfjTQ6kJaGJfCnLHsDnSuM0pJJQG5z77h7NeVzJKzOVr/9yrlAqNVpuDlGRvT7f7CaW6V4EMwAEkez1nTIlLiPp7SD2vL4WRklbC174Vd6X7QH4d9B7Nt3vUw6/fxUUc+z6hs26zZ8qYkUSLdG7U+KW4+ci5gsfQJHMYutOKjPLQ/BTH+XqaunAImQHJDHkdxXCvBphjtFDauSwrBgD7Mldh6BEqfff/YiL+htYX+VwYQ2PwCzN49am7dpo97ZCs7XB+TRu7wWI1ZLu1+vIjSigf2v1xlC7pLVBY9HyUju4W+Cy8tiv84T5tutJDSVF0DZa/LiLCjoPCoDTqYKyquOdt4WlLCYdqdFzLy2wByjep3QpqMIaWUimY9TZjz3E1Y9n0evGsnlym5As8Q6sX+iJXXurWqbRH63faXSO9hC89FG+3cdA4viZKyI49TovGVaeEMvBposmtvSZtEUJHzEYngNfgPH3/f19K9qDHrtSJEmFZrnJ6f4ez8HGfnZ5g25fijbZ5BaXIG8ECXbsirrF9p4+ribs5tWEj92ZZ/EmkZx40gWSpv2cMzCzsxpSm5rIWRLsqnCMmSS/KodLwC8JKnFwGC99qId6qNkwUqrh5AMN9wIUXqutE0oF7qVAjWk2C8cGKngQBLmpwRY+oakj/LrMDqLMwwc13kZmNsSujkXYLCPFXhU81oqCh6C80zS6jCo3UtTSr/vhW6fd4olDw+pF1BeAgnZ4crhoseqkzdRTu1MBhjYcUHaFLXrLwbWb59P4obT1xWSQCk0iERTr5sdHP5SC57bnlZ+2XrPka707TCer3G2dkZ1uuVHvEjkav1oHNNmSMsYLigGqtbGbIjHObdW5ZbS+ZQptt6DIwdLajkoWwflBFgGqzdWNnG26DWlo+1pLaY4qW9NMGeu+0HZHV4xbUN1pDfmQ67dXZXdO1CgTq9+yWHqj4t1u+PyAptDsd3GQjhudSedxnBDD+EBtgeHJxutJBSi8hbFNS+j0NnwQ8mcIqgEnffQKtvmLxq2s1EkZ/BaHL1he8e7vokmubxO1ECyF0DL/C7vwYf7BDWlgBbAYWFRNbOTgHFPr8r7u5+6CLzxP/HKP2S56LCtvO7s2SaJtmeiKuvpQWFA+O6xAUhFQYlQ+Dtui4DHthxszbTwtszW+ayJ2q93uBUj0BKrl3HFEENHbnKeQCnuj67ElqsiUl8UXq9Mu1eJjSa1CgNJtCOfCoksIuOq6DhONZUFVy3/cyK1Mr7EeTmt902EAJkaKzgjsFrhYr7PqijE7pN2c7D3NZx8OCO6o/8S1ye0U3LUD+rVxRV8Tyeum60kBKJzJlLBEUGdNORS3ZruiCPg0UECGFGARUYf9dyjN9rNZtSN4azRxiDDioVIWSuNDbVmerNts6NlbPtGfH7qLQfMNGsFypSPyF2sQJCPZ2bbCJ2OHAaqxWU/pQ3rZap2RrrpGYS9aFYqeyUigX3mdQVrZZ2HPqx6fpLdripuVArEZA5iGrrEW4Rjmy5yriwCuFUmWHLXAqaqJ5yvsHp+RnSaoUMYKrjv5Nlqy+7VCbW/cgFe/AiwAMnw/8xLlZAxmWh1sZiKPju34+S3o/m2pFPVCyjMvEgmNpF68fWMfq9VyDvrfvBy2od9T8TOPXDhHC76kuYbriQMu3UL2S2rj0ZGbWp1ATZobQRbKF5sfX4tnXNSVN93T2jYmqj+yZ1+ai7qU6q4H6LiIiQkbl6BE8aTEB1z5Kf7EBwYarF04KtRFp/eFdYZSBjF5uHTlqk6iaqwi3A68tGBtgJOOu1a8vyCChLQQ+tGDIg2P4G6C2/WiPanNBiVutGjclGUBGKcnJyssFmvcFqtQLk9JEmv4tb7USmoVZ+Oask0CW61IYeieDUB67RJTVHdse9GB4llmILk7bhCfCISstSyEIZ7kcWaPDL8V3M5GnM8HyIkF7Kt1RWLEAAi9fVdOtA+2AY5tktpkdjU1DkBVefQ2nkSEl7o4UUgTBN1Xev8slPZfsrjFP0TGVKTlD1rqSWyTrM7iBCIbI8GIxOw3L+9NV6pU2uVmvkuV79QMUi4jlrOaKyAZjyjAzo/pmmNYyOuSlvhGRicIdsPmcAlFzZRvArgybAbuxEPc4nub62WqXhyB776D0Ak2OsGAk7Y1YqMETQOY6qbTWHJyxZUqqVLGiczByvNfH44FZhkPd20gajum1d7avVCqv1Go899gqcnZ8XISV0IYKbeuY/dntWbGSDUrVvIlUgPNw5KBxwVDHAgQPClIm0lOW4JNbS4NW+YIRd7+c8B6Wgt1QqjcHJHAFJ8ogk89M/5CD4y8laeA5bAzosMeT8EHZzuKSBl7zZH4gY8Qp0Iz9+19ZbhY0spmN0oFysSGkwzPmlAjHdaCEFmJZE3BMS3E/OPedhZW52Q6rmd/UFf/Og7iFcToPvJpFobyJQ2T0TohLLCQRQKpy21YqBEunFMYgjXg/E+swTi18mWnQD1v63k9cYoCsT5Pey68byjzOMlKyhq9GxiZF89h4u8jjt8nDzIGrFPXSNaVEfB5co+QCXaKG1PVlPk1pQ0zQhUYrMxiPEGUe73K/tM6UVBZaHZVp3knynQV5j6gxZ7xH6PdbV5wBwtcc2l63yaJFE4eCm0762Gd2SXrf2p8Muc6qjVAAx4ra13A8RUN71PHwHRi9SevuH/Qt5sNB+51nY4XbUuivdT0qYRQFjQaSMHeqZgSjLLOIKP1RK3WwhJZpzkQQgcHc2l3dRFDMzbkALzFolhQ24mKa9gCLLOIRN2FnUrnRh1bXP0rYWrbOLnIASESqatV+DIgohtL0wRpipnRAxX5jrn+XVZwcQldR9EJ8aaM/dRcGx9gMqHTVT1+S68q1pYM+pGfOwTiUTTCehzUnx0rFqrUZTTnJp31fThM16jdU0YUrlQkr1BbDscerHZYwKR9mNHC2P2JQvyeSGXsdOQKwV7MJ6cIXtGfd9o7dT6WuVM0VhT+xelpRxcG3so+EGfeNv0AHn8KDLNbTygiBtmif9fxe2GgYnY5Dad+U52/AYTfulgh2YHwd+UfAoqHICoJ6sAB8kwU4vCsL3QGXmZgspSY3W4ZEv+3N0UbtylM52UGGB9k3EpSrP1WG40xUxMsD9m/GxK0QEShPSNCGtV6DttgA+UbnllotLLWd/8qm1QOVQB8zMxf0GJxRhX4hyYaSi5VVNSxmPe7crLVlUXbfJ6hsFoslzaXo8T1tl4bBkdFDq6KpmH5nnJ1MOz6zdrI0rYyH5IwoFwsHDctyMR9J6s8Hp2RnWm2JNEU1VKanUWE/CVUZQtablflM9p1LcjNbfXlvuSgYd3alBIQe65z7/IlgHZBoUI9LgoOVKHQzsm2pOWIkVlz+d5HF1LQHbCjKvdKrt6RiNi2A174gnyB6nEnATleMSCVoUilJ3OJ0lKKI92Nk35fgWEYMHKA5tB4FsCpso1ARCVnc5Az4EnzOYM2bMyPV4r2Os7S8MIeVSIQE/IwbcjtH5+YcCiqgZ7FaSlR8jbck2oB4DvISWV0uHLGDCehQtKQ9r20nR6q3e1mAwArNH+4ln1KfxsyqQgIbXBe7egm0CardqvRfOURIDqMcWbOJp3bwgoHr2HTQgQMeI2iwdPCWKkoR5wYmCWp3xFccdR+tqBvUYtcz9EmXTqxFeYnTgvh6Nqnbz8eD54AJqDsg7TEG5HOGLBgqrL7yoKUEHxkVXGVU4OlBNhqzc0E3o4GRrO0Y2yvdaBe8ai2VlQnOod2f8blSbf6dii6GHdpt3C54g4URa+XbE9H146eHD9DA9TA/Tw/SnNh0tpD7ykY/gm7/5m/Hkk0+CiPDhD384vA/h0e7zIz/yI5rny7/8y7v3H/rQh46HftFMMbt2sFevLybCf690dwW1DA8/pZ2xpqP5ULw5gClFXAGUfVPTaipn+omrzO2f8tZW/GiWsm0pl86pst92f6EPbX9GZvporNu+y0L/vggti8x8MCuprS/n3Kwl9e682EfrXx4cRVVr1s8IR+C+nQAHs+5dI6pn9a1P1JoSjTO26HTQWk/OWT+jfoTkx2SHxRu2NDS4HAXD9H0fFK442RH7tVToKJdQW44ParOMx5jmpX9m4bpSOiay3vIgoC4FuuhfBnIu+yKZUQ4Zz7W9LDTA9ZPBc/kITXgayTmD298uzzzP4dOWHZUzGDLyzJhz/cxN2yx56qdc/ncwzo529927dw+ve93r8N3f/d34tm/7tu79pz71qfD7v/yX/4J3vvOd+PZv//bw/AMf+ADe9a536e87d+4cC8owmdtGTOI2x8g5zmpC94jz5vmyCex9vKG0W+hdmnSyyO5vwS2h3IQ0TeC0Le+k/iRehhoEEpiLczUIwyLptzuzjA4PAfWeLHViqBejP43CmJ4+cPN8t7BylUD9BAb+QWmIZ24YwCCDHBcU/O2LbURQWfxzFN2/1qYNg2ySBspxSKt6kKytOTkX30Kbo/52IsbRzBDnw8VE56Kt/3lXZD99LEKVh/OngWfZvxbcRx4k75r3jjSptGtTyrt22RFS34WFeYnGHeYUSXb9779BSMGchk5xHQcjWBOCe4ZfK3L5nDfNb4rQ14k7WPSPr8a5FbHQp1CNp2sR8Kirn4wKidy2YHOXqhTPzCZohy2M09FC6qmnnsJTTz21+P6JJ54Iv//Df/gPePOb34yv+IqvCM/v3LnT5V1Kl5eXuLy81N/PPfdc/WZ7E7wg8cwoJR6cyCuTLmqdLa3u5KUH+9YFqvHAkGtXe0BUNnROE1arFXi7xZxqlF8NpWcCkAicBpzBNZbzDIBLPhASTTBhlToBMwLcyxib+CacFq5Scp10rOVIvMXQLBxH3Q+o3crEC7xJrFfhxME6q+sndWCMWRfNkgCkVJldBtZpo7c0b9YnWG9OkNIEidFUhizjXKE4dJNo36nKQKoyE8aAC9yyDmYjVdtEgm5IjsUgc60wc1RaaawOhbedp1K/vG375YVPb8kEgTyS6Cj9sgClsaJJe/ApjHkUusHCh93Fil7RVcXEpRIEEhWYcaX20/KXZ7J9kYFyjUv9KO7tuum+ev/do7ADpecpKmyr1so6/lDBKQKqrJ3b/KgyzY3zyMc1Ti/rmtRnPvMZ/Kf/9J/wzne+s3v3oQ99CK985Svx+te/Hj/yIz+C7Xa7WM8HP/hBPProo/r5si/7svpmRPgx5QWz0tN1+VQ3ikh8lo2OkLe1hLMc/Gdp+dW5eZaTMQinPhYBMKXC0EJ7MC1Zm3aSSRhoo0GpQdW2vuCi9Rt+pai5ExfKeHejB5d818bWV8Qr+s+eNKzXFT1qE2XLA5ui5hKS3wjol/c6OUXiMGGaJqymNVbTGtO0KldyUFIrDPAMRJSult6axA0Q7oUyE2ZH77Gf3m2lH0dDaN7Ffo5dc36B3c+zALQboABj115PDGFaOZzbp+n76LPnfejHCD8dHtCV9Z/iutv1GY2F1Ys9+A78pgd4sW4loQEFLaVlhcmrtI0kdLAfOh9f1ui+f/Wv/hXu3LnTuQW/7/u+D294wxvw+OOP49d+7dfwvve9D5/61KfwYz/2Y8N63ve+9+G9732v/n7uueeqoCKTDwsdLnpL3Pym0r+e4rvoBmJbPwDQaJkxjVyL3i9eh2VU0vZyuD4kImBKmFYrpGkqa1GihFVGxRT3tQjMrsHGAjrWjIn9UzdeZZgppWWeuUDAu/aM7Cr3x5VIETpO6uZwcBaL3Y60Yvdc82SAU7mBebM+KWMKYHNyis3Gfuel5p2Al3Z2d2TAdA5kDkrnTlCNxokHfe3cnDIDFuhkDIAphHthlOzNePi0aIE2c2QMClflwTKGfjdmXOvWG7W9y+1M6iYZv+dFmCt/8IJfMwv/sk4oz5A+DXt/bNqtTZrywdhLvy69rELqX/7Lf4l3vOMdOD09Dc+9wPnqr/5qbDYbfM/3fA8++MEP4uTkpKvn5ORk+DwkApiKOPLOaBoQVp1144Huftu+IWdP6OvAM0JxdnnlfhXH6P2MjQpldaEVpjdNNXCiuvYYVPf2VvcPUXBZFFDzAiEwdIeWquoE3YZCqBq9/BT3ULMGlcrvRBzyayttu02fR0wyMPShc6XCB4drRmARpGNUfeLavGneSzD6ekoIfD2Y1GnRRRERnCRExlyCXQCA5+ws0HpEFFK9+oF0TxRQNvOWyy6hbkIWQeUEDZj3CihufggjinesjV1XpkZRwKt3QC4LA2N8OWc3vuJsMy3e+8PI+8ZCnWQzxzH01kLW1mUDqYNHZzuNrQ5V2/Q4Kwpu7RY1QRGU/rq9QIOTH11/Is2rHtlsBhzujdpxyy2zwF3rcdXJWPkmihvb1V1PiNDRU6B7oRVZWxlMUWQkv6olzJD7W31/wS7YpO3njvSyCalf/dVfxf/4H/8D//bf/tu9ed/4xjdiu93i93//9/GVX/mVB7fRzJmSglVl3Mw77Jh9AEFT1hPFQKt1ekitawybWR3lfy/D2ig4nrkBpXDERASWSDAQsrBemcGqIRHiBdhCMkY6Bq//LRNIqbgytFJv6mCFwiIe5fLKgB9rhwarr2/JbeEQF5NMHuFtA0ETxof8794SiO1ErLUWAWpzYVSdZemfO/6IQERVmK3qWiMApGmq1rCbvOYeULg8w1ZgRkkEgfacQ/eHc0bpoIFfyos1IAAqCLEig9bXLMzT8Ovz+5nq+0VO0CxZJNYImRB0tQtO+1ns6Bbur+JudxJmLIpFqKO+W7Is+7wtXY6/+zFxPztw49xSaLSMX7aQ/21O+cYxSKxlgELfRfkXGrcZJOu0vqgNRTdYO9PLJqT+xb/4F/jar/1avO51r9ub9+Mf/zhSSnjVq151XCMHddSkd2Y7oFWTcNpmAh4OgteEegL3wkpcB1QFkCZ/qqs+K1M4pRKKrm5HchZZhZ2Jg4gix/xbl4xdjCenIixPylaYElE5/UCYoJYd99sYhK9jsbngHj02RaETNdSuDXVF9UmY1z4trxtrht6HVc6JrDhSraJmo3ID72q10XqY6wI4Ofuvli8MICOzCwEfuLMOS3vy6ZEECcgiVCpj4oFQaizj8p6j1VPdp5kzGE0ErPcVNxxZNq0Ddh1Nt4YJwZ/N3daVpnZCOG9L5hDgBfSDUd7u1FqA9SmWx6KVTE5JqnxMrh8pbEsOcx7Xlh1eA90T12OjZAxaBoRKD9a2l+mhORWIpO99sIPTpx8oHS2k7t69i0984hP6+5Of/CQ+/vGP4/HHH8drXvMaAGXN6N/9u3+Hf/yP/3FX/umnn8ZHP/pRvPnNb8adO3fw9NNP4z3veQ++8zu/E694xSuO7gBX9VZ5dkC2Y3pkBBsrEP2vedvwYTb8A4G4Ya6gJg3dW2SujEZxdXVQlZkJmBJWqwnTKmG+LhoIERVGyAOWXonOC0VUxlzcSRIlBpfP9dn+mOYTNHvBgWC64k0PGRUmYSCVKUn6fEiwrRLn5kSXVYfYM622Oqd0uPGKFTdMV2G1lJwGb3IwIkr4X3+1S1tbWZNK06RrUP5UdEZddhRhRyLkxA0UYdVWhoxwKY3mgP0VNuZRRuNSoX3NyO6vNtkQ+qDpPr8g02HTK4QQZQmQU/i9kFSI+wliH7Y/Qskx+4KicxCqPdyRPkcWUvm640QQLxhUYZGJUA+MNpUikL7xl0K5dnWqU0JakGySGXQceZkY1zJhk3BfRhWC7GtU3lxsBVpCb5eOFlIf+9jH8OY3v1l/y/rSd33Xd+Gnf/qnAQA/93M/B2bGd3zHd3TlT05O8HM/93P44R/+YVxeXuK1r30t3vOe94R1qoNTHahEdU8RMeKVxsZGKfX+XmXCjtskP1qBlv26TKtRusz6bPf+IXOXEVoOqyCm8lmvV5g3a1y98ELx91ICY9YNeoFJNQwroQbqi7ZJFIgtukNIGyci5CrQhK/LvDABS3aIZ4Nx+2XEaJPVMxtrsx+PFi9sZ+G5qzEUFoJODGZ/UKtBVpRQC5mOyQS69ZUg7q52QseoR+sypRS0e0kpJaRphWm9xmqzLmWnFZiSTN8iqOQ/ahSciI3uiQ/wCX3yLMhtTvVk5nxD1g9X70hQdWxMhErQD0jHVL6HUGly+RphJkEqpW5HM0TIzOqNKEIjwzsUA8R1LVDRIXo+MRzI8CAYBBb4EZMwB6Ndw1cyJdHjZkdqpoX2s8tX6TbuHLNvOseq1m6uaINnUoXRn9bv+JP2myPgjmcE5UIOwK7KbBkOGQ/UK3+yAVg3SrXXvCylo4XUm970pr2ukHe/+91497vfPXz3hje8Ab/+679+bLPDJChNJOefpUogDRN0gsD7tjuFzzNsx3mK1gYtHw0jY1ktJceFXnNPlN9Lh2YaDDrR6x1NiVJZrPVEIdaQR4qjrX2To1ujYn8YJivxKkPzjLPOTG+DEhKIOLgZ+nZiE+F32x9XftGVIxNMLMYD3MD7Iq7a6L1D01D2yQuiOpYTUiqWFFOhhKJIjBSWY6ykQ6BrhVZhRm0L8ju536N+Lfa3Ka1isR1wpx/FyehB7MeqnccjwGxDqRdYNpe9MCw0HoOrmC0IhF250F6VU50AP5RmdiOwrRWMeDGpR6DR7m63eoCvCg6ZP2FuqxvPt248dLjHTKzY4F5lZcG2qeVwmr7xB8xK5xO5/TuBaUYhFXWepq72TTPS3h0nbe0oXcs4bVsgaxj7cmJlbskFUPgWxe1GrojpWLIeFxmuCp1mstcMrn6LDlKPn9PCQ+89U/EzzwMWq6/aZtPl6rrxcKngcUKo42muT8y9sDJDsxdCHjdaB9wkdu9Ku+LkdMqEZ1QiMFssCX3WNcbYbqNIwdZ2+rWgMf20LlbfbnBVti9HXLJpo9fbD03CMBvh29bWKogMYNB3zb4oBKQ9wf0IT5EISZ95XDTzxmmqredg0IDWNQJT5q0rMDJY2krbClxfRrS8q64BPCqgqobbVCtGUCFtNjcfO7+JE3zcd1Ebs9WDwwTVjRdSAIJrLTJyAjg7BluSd3f5RV/yCkAzl6P1JPuDbA9Vm5bCsPUIIz8pl8rXOlJK5UK8aSrnb82z08wTKGezXLzPuK2zUp7qnxxxYVVIeHsh3ClNJX+7S1yJzTHjOoHTVNfVWkstTMglUd0MViOgRGB4Z0HOWSdbK6QKo7cjj4bWEtjcogsgcR2zwiM6MY2+p1JzrWAi0CohrRJoJS6nurh9AB1Fq73N2HxtUOiktL7cxSJGWOj0ifZ3mDNLtY84V6P0tLJ9QVh5hWtnX5w21K3ichlRo5H4XtoIJN587+YZi+Crq0Qd4pyjuAocmZvmphx5WggdnTUWZak3N2UOSMk5EGudcgOwT/52II/3qIuOcDx4fmD6ghBSbQr7SYSY9FHc8wOI1C/7jcIzIAqrMN5x8OOC4oKLJki6wqIPdQsM/dPSHsWw7gL//noH4SLVAmj6M8q3BMwA7l19bPXXfakVLGOLYmQ97tK+RSlcUnsDb+/eHQa8CMxa9EW78PaUr/COQDum5Z0CwHkJNO/OyhsBNZZXR6W97lAhrj1jVYKMdgNBg+9L+FFSWmwPur40bmG5dZmfx7ihcUgZxzd3Kxt+UtTfR82F49IXpJAaJfYIfeBKlosvrXFIGk2kfeseL8N4m9XtLcZjfANdbVE4vmTrJ/smoHoWomJxaHX73H4726WYT/f09ArkH2uKYfitYH2QcTH995g1hB6uWF9ILxG5/EmnGHn4Iju1z2TFEo2alyJ6iP5kkNzS4yDDQfXccCFF4SOL7geVNJU2+mSd6QxEeiF5sE+J3YP8Emq6W0C5l9XlVlx+nGfkuWpFKVk0m+vXqLbWkSGRO9E/T+VgWxT3WXIRkZkZya3VOA+Ktit5vSvVrJidKPGALU7K5cAJU1v3HRja1hkbHisb7STnBfdcaxgECzwRaCrh5zQluyq40l/Zr7If5oNonBGCA3Sp4Uhe5dW6Qyzz3t33gKbSwNU3Xpt7CZivd/vTYf3cl8qa6e71tFF/ZF0sruHt4SWBj7hSe4RT6/lZer/b8kKlr8p/YYcrey4kVmpw/x+B5hsupAZpaHujIXz52zz07j6HzDbkfImbjCyjETGESd8r4PWtY7xVeJQIPwv1BEEFa3Bxqo/dW0jthKiNt4sXC3Bb7FOrXZdvcf2n7VFg2c1zKE78NzYgl4rAR6XpOoBWQ8Ft0Y5YF80HG5PdFm4N2hgwzNYV2dJcCZqgIpiUOcIElQK6bwbvZs46tJKVEZGzs3oeZLGAn87t1ICzNP2AAS4dTKYAtjWQumqPFUqdUrYEq1MQ+pni2vbzqddvIm3uGcL43uguKHQCU6cpuzqadlrFotv71eJwx3poyb7jnXvfqSOeCFWjbebHgelGCymvQR9RCsf5GAYihD1RHa/N8QFDxHW/F3G55gFpwmo1gecVtnTlDpytwRNuApVrIYoVlFGjxCoHZPZnUzSpU+ZbTheDEWxyzV3wxXi6D3saAVhIpIxBJl47Q3uc8h4TrhMqC+9Zgm/0+QhA1O0BgN//IfoEgCKg2ntNqChBTHmBWZim/2IsBwXjgCHZlaUT9qP3nZRcYEoilYMANQXsMIiarK4upZFFtBJG61BDG8Yxc69QkQRHZF+Cmr+7k6dDf7IIoQ/V7u+XikqD/i9o7eZ003arKLh06LqXtOuVt6hAOuXvyHU04IYLqZLED6uUuVOd06Gt2plfLBxpyIutLpjskoYuItd2/8xbLi4jOaJL5doO1ncozJGdeQ7rU0oJzBmJyI6WgXPBLcDdM0MGhxvtuAo7Y6CjMOm9uOzULyN4NWSX6mDt7SIMrlptbim6r2WjURFxvv7OxdiOp1mCzFwtJwDEdQxFx5XxWGCiCnEck7CPZSl1Fou5zPaxzVG1EkofaYY7C0JYeFz/XeCA/iEvZHDCZpdLKnwf0ctSp/W5V7xsfNv52Y05S49J56HW2A2Bn98HMuqGZx1U3itTAsiOQR+530Z8aTy36vgq3RfRZF6NduzJCfrDFa4vACFVkvo7Cf06TffLhUE3jKrTJtATibVZJ681cjCs47qEIdUKmcMVHZSSrWfI5YfStmdeqYbMMiFx0c5SSkVL00no+uiJyD2LAqv0Naz/UIp+ZvTCeel50aqsZo8cVjednyDjUdgVANHkHFpO7aZj/84zqiAbBq7CAL9vS9c65HsJ7R9bFm4Pm2rAfTTqsHdLsrwKc13bcfUF5W6pXohGzh6ottML1Ti66M30pawI8xMipBYMTcnjaXBcaZOii1rqWGqjoy/Ba6jRKYot9xkO0AE0e1QRazcKzMOEYszXI6Krxnk31J1X53U8eX8whzzP2pNutJCKfayEHDRTrkzDS26n/u3S7vYlTwTN/NvvlllmusNUhUVaTaB5qou8FAZabnotRCLH8jCICYkZ81yPjNJmvRo8hrdl0EAO7jy5luGg8PtBvVyq9G9KK3q/dIyaK0Ij72SuS1GGMjFENpZAkoYJqOJHyMhOANanwcXpBF79LtZqEoWcubpdqSrdPRNk5nKETC3EomRYSxCous4spka1qn22PUVeBdhDiWo9tUSO4eERMYvDLzV77Lp2esHdKgFLJymEdg/i6oKfyNDNkpIjtsY1RGV4yQL086tt+7C0W8QeUnLU9u65LvnaXHYJDrscOjFNmUG/MYSr4s0ZGrxzBPe72UKqTz0DEez1i7ZkmNpDN6015d0ZpG2Mj0ta1mLi81H+UndRIcXVV+5yIosGa/zkTATKznbUCTjQhIYwtUjxzBiIh3n2yUdj7db8jcC7dzlOMm8t7dIK97sX/cRqeP2C3tAL3d62Lnh2eDJpFzKNWIT66ZWG6th1rrnW5dizkdiZHdx8j1QSQ77A0RR3TegxQQv1RQpy6ytDshi7eP2pHdG9dyCbW2irbSfWX9VatT4HNTDqRt4ejt0oHilxhwiONqe3Fxt+574cshzReiN8sEW7X1ItwwHU8l4FWchARkvk8+9PXxBCSpgXV1eL0VrlPHV2LWnZktqpX5jY3ln9MqQCt0yQROUa+Wm1wjTPSNNULYFcLCiOpEtcrlqAXkAX1Rdzp+2GQayWlBLAFmWlOZwwGq4PjGp1Y6XM2b1byv+iUx1Hv7aTUlLrp22zPcltZzoIn3Cu3FA4TP6XJTkjqDso44ByI4al82JBp/H8iEEHtLs8z0jbc00jjueD1LtYQhXGHZl4ZDMcl3atWy82iyU0+k0muxTj/QLx6H45hZ89v9mRvmii+woy2ptLZFEfKNOjagn1tbkFioXSSyWHvL0CyqGa+410Xe4drrBegJKaP9aL8n6q1zwAQG6ZhdSRSh0EKneSEwGJgVlOAPfxN7FPtTtB+DAzuAag2afe7Ek+mk2OiqLhJFfZJAPi1rhsfctwMVpHkjWKEdwlOIC6eSLWjrd6/Hjs0lqL4BclhvXZkHl3/a25/PH6zGotysXGRqXsYO3rNZws02VYY3H97QAPhUbAY+/eKi9A9MqSBh+HsLvltcU4luZyXbIQ7BWpT3Kwd5C5p692TYb3o2tkjbVbNXwNEpkq1kqQ+66ezEILsayYud4N7uvvo14DcE2+pkPBIGO0lNmlCkLywNdi1FQYlFPpy4HphgupBK/1iEDnPANwMogaAdBoe7ail5vBaJhhw7TLXzkTLjIHORC2TbY4u0sA1kkP0bzL2XySe7VaKaeXc7aUYTJrSHqJt8hVky1uwJQI85zd3G/dcsYsUlqhFTYCf7FERJCNhNSyFSQCKgRPhL9jhuVYeJhkPt9UgArvhKkHRrjIFC2pg4qF+Qkdlf+GfL1jdP2k5MxgseAaTqX05JWVtv5WmZKczkqM8Hhc7RaqoS2Fz+rqXJ/eSvMwDPFqClALs8Ha9MkyLsIZ2kXorT3xTBQm7L2giu2bhE472m4hZvED6oJWx1GCoNJZ6BTmMsYZI0ES1kUHNNAva7Tj3Y6BCT5rH/3g+yqpyVgZb6deCB9A5VWVT7EIsgPp8GYLKR5o1A1DzSKgYJPME0RgENRtfYvVHujW8e13WiU1s2X4rhnAupmBEtUTC1bg7RazMDq34MREmGgCc65XJyWkZCdIiABlZmSkvq0G/iE87n0bPJFz7vIuufFGyGzDgIFC5HnISMY4G2vH+ye0Mdv+IhVlaAv2Z4DXW2j1f73I0AnI/YnG3517ZW8NRGC/GeZlTGOW5pSLPQKy20awz4obCOVR68fOWWv+cG0/1oGDYT/m/QOFse9uZacn4YDiUGWzsqC9tag2engzN1tIARjoMr1ERyMwhkrqslkrGsDSXpwuBbN68NzUwx7S5hG7nHpfVqqX5GW/MdeEMciOKCkwO9irQOEm7yiV7NwIgnGX9+FlKKgczEHrEooX2at83SklVJiIug1DO40+7QvBTxIKk0ZDiIm17tYSicsyXpGIrkTfy3JBo6rwPRLruCicIoQG5KFV+seEJjNbNdy858FYLDJMh6uK75GAVbBhQnwwCr5Gzd/DSjEfoSsH9TCYUhNnnGur0pHHj+mnC0cuNVJmbOGJgOzx4J7sMgDrux5T9n7XuPjlBSi8h1onO+Gy6gZweTodFHRTzc+6OPXEojpMUt1oITXsYoOocX6/MMjDHMP2BpaRNpu8y2uZAoz8+xH2UyMIHQaYy9XjXM9/AzPmnJEazZxdea5uDrGcdOpScQcOF7PlzpyOOVM10Qd92oEXeT8kyB3oFuW/QCEwSc9qhoGw7xXxkcpC0JtCF1OCx6b0Q9YFguCuf6c6ht5pXCZo7QXVCzrrEVeSobyVzdEmoJ2EqXUadejFdwvy7ChVdSfDknUtQnuRnVMxaouDY3h2wiKQH2ixeIFQibFTsDyNNEJ2WGWQMlbGaCeoiT38woEHAmqp31E4jQHsp5PRYPmb4ZUv73J/adKuMSttigZp3gU3D4NXSrBZz/ZDoedd1776dKOF1GI6hOYPnBdtahnyootup0az710UOosKyz5NhExD3QUGNXNlad2C2vwvWRprkhwmcftsBOehOPfa60hZOSwF33ynNu4GZ7eWTN2TvoJDodzfrzH2eyhae2UfCIvCs+u7WFMm7JaBHbXaju2OpEMTLSXSiXJgXQGMQ7BxeBorgvtKiUX4UgCwt6kD62kVgAPrb9IXppBq0siwWjQ1OU6YcZY/oePvdzQZN9ntqOJQl2VT5ihgdqQX7fOPPo64/hPW0B6g6sUfPQiFp3mtu0+e+dNCoEXbZCcwXH/DPpaOTo1RGhpeCo4lzR+AUF6iwyUx2BQfLPof1oNmbZrDH7NKHVyMwRw+DMyXLP1J8RF2wviY5oPbvFah1Pcy4u2LQkjtSuKa0fDZ+twWyg9Lh+4TepCkTidKAJX1KKprG7JoqYETYGROQdhSsvWost+JggsuroWUCECfkm4k9hGLacAoEYIoHnRx11j/uLxYkTQQVKOaHqT9EuY+dlwRo4SQe8O5yefxIHuy0jRhmiaspoRpKu9ypTGuFbNyZgs/9uuJHTyOTmmHi+nQNML6i6ZntS4PFTmD3wvGaSYgZfdcJwsPC+zaA/TigghkhS3jqD12f8wpBhLRgwlKEXKM6gU24SVryi8lB/wiEVLRfTbyH9v6anQDoBNUI1fRy5da154wUABlHUwj/ORPywwIoBIZSAxQPuAYI3JkVl2GwnRH7rV+4V4g9X93pY4tds/Yf1Qwu2bd98UYzT2RRUEZ92t4tbFuUb/jphwYqjAujzsV+ORxhw59vu0wmhJU4Wla0dFqxwtCXnUSZ3m17quR+2EAx3g+uZpI6hiYOm6cuXtX3u8SbTYyHAclyGpPIK4cOTw0naTa6GKAQGhdsrCeErLTkHUKZYhsGawr900a/bZ7AUu145LjqNdSUcBDg6o4ts3cYa7amihzbiwl60tkXt1oIdWxxV1IIdshXpadD7eS2vbIRRzo8Byk2R/Tkp86s1o+8qGUkFYTMBOIM0C2wRczm6Chmn8qh82CxJICci6alJ6QPgiOIBFSCbAtSKRzS/JYfg7f+0CyY9yNvU5q1h/s0NPyBLKMbjEV7SrK6LuVljd+ickEsTt/j8uBHrE0+59D6zJV4URTPSgYAHHyYrBjysZrx/vZ7LsvuZQv9snolMOzvuz/3963xtp1XPX/1uxz7vX149p5+ZG2CSkqhJA0QGhTqwIhajmJItTSfChVhFJUUREcpD6oUCVIKvgQKBJIoBC+oKR8aAv5EKpGJSJNGkdtnbSE8KdNkNVUgZQ2TmhS29ePe87Ze9b/w8yaWTN79jnn2m58L93LObnn7D2PNTNr1mvWzHA6yElJ5R5uQ1v4TB+T+eaPC2bJitGaQtYliUDKBRmp3wQQN4GOgoFbABHYwhcIUXds5SkV0rKSeVZntjjE2sF1VLqumyqgxaM+BV+PYwywYsRgI89zzpEOv6GFlF6XcB9hjJwnCSBEzfnEK1oDUkY8t4KS9L5MV3Di/iqGrQYBMZ8Qi0oLB2EiH5CP2rMyMXRb/DMysH5js9QvUWVuP5N7bgqRiRo/k22Q1e9jm0uYx+9dAlxjrqPnRNMTKB1fVALNrvUkK/W2HieiXPCk6aQMoZ10bY2nWxxAPHU271/FUIMC0GqTCEGVbioTU6ZES5PLfk6lRVXRlHTFPp5K3vl5BuUCpu0pzBEI4yFWhi6TsmeIp3ZExVPEZmxvDkkcgNZJdN1Z8MK0bii964yEbeGSh82ndKnn8RlZNBn5cPZdcI205mfGXFXlCtd02NhCCl7rVRKdFcGS13xdOv/Ma+chmoeQUF9Jx9NKWdDWpf5E5y5po5oReH05EI7WtmN7VM6QPwiocCrGlIGezcW8wOx6np/FB81fE1zbriWlWXNKwDqvpKUkh96/5XNYfRp51qYupsnq2JmAXXkTbsvlldVR3DCapUtHsoCXrDslTE73V0vdn61Jt7SvUqLSizZu6dtc2ZlSVBcuOXRoCq1+KycLc3atQMn/01ITF2FQUDmpqyXfE9pIByn12mkleT5mvDY5kls+3Ko3VdTbOJSepYERJYRy5ZMRrzHQ83xaY/I9brNhwwspAHB7BiyaxoIoHrTIDJDx2oVcHug7lgKTr9SQa3Y5k0+oFIz0DEHBKxOQauzXYgrnPl5Z14DxJyM4311IL3toujQo2TeV4+nwygWnUqkI0Z3HpNx5OfHm38vsyBWZ1kfILzmN6zow1MXv5gA1sV3kQ6plRpXaz7Mqyyr048Wf0bRSoJYOJpBYw76tnO1xIcDfKqwFvq+d4qVy84Pqtdzc6dJz1P+7xcnZwfQSvdIJbe90FxQiHqebZ+qpUVYTRToTRTfHptUFrRRrFDRlHHnWYYlZelevt2Lm5Fozgbu8FqKwxaPG5iouKARnhtsGF1LRzRctqSiknFBwAip1tXOUKcqPrP6H5BvrDWuyOK4nsP6rf6VMWq8BsGfyba2oDY5B+9MmyJ8LaEzC4JP2GScNS9F3Li2HNZJp+76SNimTJ1g6iSU1HYqC0DP/fH1QphqptGL5dYlDXQcV37o3rvsdU2q5usQFlNBBrlV4Wsr6SYeFJ+1UllpwS1sVWSltyszUaHkHigttJKDFEFu0kCwoZGkjYQOZxZmWKcnKwqxdfMdpL0lhakx1zswyBTL8c+teCVjSdNgqB63+KtJsIDiV9gwXVXK3//TtLKWns+uNiqv/i/kU61lba9hL6CI9UFpnkjf7zq2nZw4bXEhZAA2sHMDKYknJoa+pM849E17EMBXgGAwre6oMpjAB25DWJgvtbvJaxUg0G54GLh35NSFj/eGuGMLUNeAPc2VOlR8yvlaOh8Ei/I6BE7PNbUVuHWts5yqCJ9YogsZbjeRFQFUV3aiSXp3XACTsyf2mxM3g3iX4MyO5Jp7Kk7EEYknl/RFb4oJW2DpLqrEMI+Ol7wIMAso/DDislVnyFPIqC+9uUJyJ2m/aVk6uYXdba/NCmEZBt6H07oMidxa8fdZcF+mYzc6yhWfU2s1YUt5MyCW3VOtbPOcPoDozSzUoOtLnFGlxLQrkWqE7VN8Gr0CiWOHsSGBDCykmYcIMJut/u+ODAGF0kX011h8W5InHQtazGQ0YgEmElQHc+XjsQrjDQadhVugIlgITR7Q2nHCxkRHJ4a5+Ek8dRO+KsMYFQ1Q0QFUNUFU1QBWsYXcNh8ehsdYde0RirYViEtdjOTooTm6r/B7u3Db4G2Q1X6XkCvauvUVaQ9UWUnFNJDCR9GxCImXxevyiAm1TvIKV4CS4Oy0+FVTB4vA1kFjjEO+pjHGuXGSaPwnbjGmJnKXb+GgMthbWeqtfuftcOt0BBBFsSZ8IOmKJly79Sx6RWlKKJkd7xFn1eZo+NlErX9l4UXoEURq45NmnBYyppnLNhDbzd4rxRnwI0R2qrMgWxiXLM1o6YfKRpCSEI6pEyRXlJdQnCpRMXKOxDIqSo1dKrD7JbpEHSCRSuNUeJ2x1/1AIphOFOAgrqK5WfZ4INY7lJMut0qbiocT52Bu31BIKkxV7KVOPRsGFOKcWuKGFFGBBRjpAn1WniJdT4i76mz3BGOhFUz8dvYVmUIFBaFhfc4GZGotmivnAOe3eEWeXly3yGSnAwFT+YwwsGSeUfYQeM8M2teuHRONJpy+BCtcpZA1RbilxXZFnlCHisYPQ8kXmkkGgBZWW9UTxXC/2wkJPfEE1uB/AAY90bjLEVZsKpFhGurnRfxJNO+YJuglREm3IoULNND3D84oRkRN87IVruGq+0gIg76Q2E9b71zRo6yA0XZekpXtrzNKxUhnayXJC5VRRk7ZG97uUnbqnS4V3z6O4eSRUC9UF1D5ouMWsC0okJAvJ3U46oMn5PlxluRBHULqiwqVvGotiK7o2tSDtYs6Mlr5QyBOGIQhJecGhjEBNCc/JlKuAtaTVOGp6Rgsp11eN1tvkDeJEaasMoZlcUpjKsLGFlG87UQVjqoIZKgLMaV0GBnKRrfXaUeNDsR3TLWg2HHmbDk9lJVimCyqZrATye5lU0LX/28EQhGiD21IxNKpQ0QATXk0sGQfxcNTyvhcgRMBl74mVI6UkgLK1AfcoPaan5ZbTEyGbEO2yfAuIwGyCC87aMAiqTIQyKzKBMaaMKRVe0q9dbFrWDDWjDxGPHkFRaPKy0z7we6MCM1H9piu3nsBI1BaNbBqSkVqfaQv0o4JnLtSbMPcidKWYg6UoJMRS/VG4hB02XfSjAksK1uY87rdUAVNKQSguv2hVCQSOe4VeLyjKiDCO3cK5ZIx3Qcml6iKkCdYf1uz6zYZnkcvlWzbmGweBNR2de/fdd+Ntb3sbtm3bhp07d+I973kPDh8+nKRZXV3FgQMHcNFFF2Hr1q245ZZb8PLLLydpXnzxRdx8883YvHkzdu7ciY9//OOo63otqACAHC7tQDQpRlygZiHmSNDCbAycg6by36UsnVesLobXfuVZ+GTaRhnLjr8p6I26YrazPIcwQArKk2hpstBZhilCEE5XFO1XCEkiJVlfANmh3UkN7XWY2G8iaxNGlbn6krD30KUUmDvy9x6pPFw+htWnz4wXFq3RSgYzvhfnnSgvBnHpKP34f9I+X45YNcKk40f6Jo614BF6R3ea76PWlgDdZ/5f7HhRitI6OOCVjHjrdyxGj/g8dJ6m1IxoPoZE6XBknDctFxkz5kAqCO9T4dHeVtHu13JjVM2KHrv7Q9Oqd9sSJfOBO/qlhF/XuxzNkCf5V8BO0U3SaUnqdG4l89J/NLVoHhugI0pRp5xXUK1JSB08eBAHDhzAk08+iUceeQSTyQT79+/HyZMnQ5qPfOQj+MIXvoAHHngABw8exPe//328973vDe+bpsHNN9+M8XiMr33ta/j0pz+N+++/H3feeedaUPHIG5AFXKyE014DY2dZe4rnzQmzMQxUDAyp8h+3FkVqglhmNMxoiFATYWIZtRWB4TSI8Cmwr/Rjsr/5oLZFQBRTEdyCug+iMG5jrgvJSFOWBz9/JmQmgSbu41bn3H6sICihmjoDpESZoPosBh3oML2s2G9ElfuYClABBaC0nXITskvvVBBDJrkawwSBVbAGEU+sMAAqMqjIWWhGTUO5dFU/A8hZRI44ANvAucE4negFthGsZC1QZmjhToim/4g1Du06wvfsYxX9RAVjrSr2rCS67UZ9Usa4FmHYbolCxysEWjHoxqv9vZ1wBibaSiByrncThUCcB/GTKlapW7BLmOa3fWdiJaOzOfuSCBKAFeaJcbduyx7+KJcd8bOiGvahlQn+Rf535leJrMnd9/DDDye/77//fuzcuRNPP/00fvmXfxnHjh3D3/3d3+Ezn/kMfvVXfxUAcN999+FnfuZn8OSTT+Id73gH/uVf/gXPPfccvvSlL2HXrl34uZ/7OfzJn/wJ/uAP/gCf/OQnsbCwMDc+xAaEgTOxmQA2YBh19ba6h0e0LK/d6vHTU1JrcGqlK9YJE7T1UEiwqjT4wZOSvGkMEueX/7+ykNLGRcRcdVErNAwXgm5k8G2cqsK82QDsTW7ZI6YXQ8mvSZFyxxCi9CAgeI0z5HTUpAienK1FI6HA8HKlK2j7XjMraJjpfiYKFeRpdTqKGdrliLtTa4NSLCG4Q2PB8a90E6kxJ9VmIm+ZW4ZsgSJfJzcWbG24sJKNC9hgjgvvXZw1uLOBNblqZkGuc09fO8ogEJ3QtHN3BRu+UFRwYoZ5pIpTa0JJnmlCRFueYS75mVc4RiR3T8d6y2nQRjNUJ0FYOpJPz9cSBHae19FV1xS8YllioUWhPdXNShEH/y2ULyDRwe57fJaDkdNUADA7nqQVE/IBSzSPlluAs7op69ixYwCACy+8EADw9NNPYzKZYN++fSHNlVdeicsuuwyHDh0CABw6dAjXXHMNdu3aFdLccMMNOH78OJ599tliPaPRCMePH08+gj6RCdqJAy25ZUFVMRzRSoLEh2dYaFlFTtMs6mvZpNF1q++M9vM1KanRbUDqd75Hql11OlFjeaoMeaDLVcKSdSakk3mWgIJ6PwtK7iXNQHKNM6LVdkm0XThll2DML27f8ntXQhlprblSeJZ+QtrYSNe2jKiExzorntdEI2uFhI4CfSH5HhQdyRPSFkvLfsfv1O6NVv7yuE7l8dPbR21ayaFrI3t3mSXaKtPurLyxfVR8j2L6Ng23n1NCk5Kuq2/0eGoaKOXL+ypd83fpnfWly83roW4ymAFnLKSstfjwhz+Md77znbj66qsBAEeOHMHCwgJ27NiRpN21axeOHDkS0mgBJe/lXQnuvvtubN++PXze9KY3nSnaPfTQQw89bCA4YyF14MABfOtb38LnPve5c4lPET7xiU/g2LFj4fPd737XvSAg9W2777LC4pYIdAi6d5UQhX0F1r93m/jSdRQJlLCIC6Bho6W4OXLXeOnjKw6BGAyACVppjkETHd52ZR0ErYQQHMdx3T5qsKn1Nwco6yt+FZNfq/5qUX5KJSW3Xbd2nbl9IK5WVr9RVLGTAAHL/oNWR5IfLyqOm2yUTtubBJKwo6xsGTxaJ/6vhP4Hl1CGZx44oa3aaZAEWxSgSwMOmjaV+h3ZsywN5S0t1tDKHy19ZXmo/ii1YqpLT2GRBBlxZsnLBOM1U38ROnGaxzEygz2AIq+ZF9vuMc4QK/T/tDkX51vb0szprsu7od18yfPEsxM7bg4jFsAZhqDfcccdeOihh/DEE0/gjW98Y3i+e/dujMdjHD16NLGmXn75ZezevTuk+frXv56UJ9F/kiaHxcVFLC4utp4zyJ/nZmDh16PUfgwJgBh4Mzpn+BY2HnlX+dMbZBGB2R/m6n4bg+i35ejiEXdiy9URzgp06UP93j9r4QebHXGEQS0QQxviYBtjwGSSNTECgUmiG+GjzDpK0u6L0Iy2oHL4K3PfC+kuOiOSeqn1gmBEW1Av2Jenp6wfR/GlqTKigMxcNyFghkIdiTsJfv0AXMBddZgSUq22gdJxgldessnrxlsLTwtummRNKvYJEpfLVMgZs+TNmYN+Gb62iYESoeKrKPyKCoY8Y0E/KUvGMayycQxL8VNBEA7FuW5vj4hrTyeVKVz0GIs0PDshZdTewxwn5kiT8jZgo9tVHE7fR6EMvc4Uufd0pUPXrPACI25BIeR9kwuqtvvSK/UzxiJXeOJv5X70ZRt2c167cXneSCys0ZJiZtxxxx148MEH8dhjj+GKK65I3l933XUYDod49NFHw7PDhw/jxRdfxN69ewEAe/fuxTe/+U288sorIc0jjzyC5eVlXHXVVWtBBxbkNkjC7alpLPwCdOU+5P66iDjHyJmBurGAqbC4sISFxU0YLi6CzCA56NMRmgpvmcKQy6A0huwNn4s5RARQ5W98NXHCJ2OvLJFCXdNIhBAJKvejTytI/NMtbUrK6cga14DmI1wALQGVhly3tcI0b1YXUciXl5drkrPWMfR6FgFxe4M/Fim3QFtWxxq6QLdnvkSENLruTCqDKkfhXtySQXEqzF1VFPZrSV9809L21w6lNaAYTaoUuumodMI0Ois978JxjpacQZ4E05nli4CK61P+Y9xJQC7aulpT3WuypA4cOIDPfOYz+PznP49t27aFNaTt27djaWkJ27dvxwc/+EF89KMfxYUXXojl5WX83u/9Hvbu3Yt3vOMdAID9+/fjqquuwm/+5m/iU5/6FI4cOYI//MM/xIEDB4rW0izw+/ZBVMGyLB5GaygEUMhk8WHqxgwwWFgE+XDrelJnAkU63AZmp3WOtQiY4F6KDyBH9IiVEyPDYrJcQ9PPBD85daJllXCWKVhWWUUdGlOom+Jvh2eBWZeEDxUOr221Jj5nnSdtaIYntXDuHAsWC1CEbiw/ti9qpUGXbS0Qx+e6paxxSjRMnQmI0ZXRRZrkXSOzyHXjtUFkrMHWbylSnPwJOYmKxChWfKBjsZQ0th0I52Mn1kDwWhB1MGglyJzJ6r9zkiRYzoU+nsYo52WiuuxkvvpmJyQSMsXU8wigUl260PDI87eES7TakVs/xZo0imWY0j3RkpKk3lIPQisV8LNgTULq3nvvBQD8yq/8SvL8vvvuwwc+8AEAwF/+5V/CGINbbrkFo9EIN9xwA/7mb/4mpK2qCg899BBuv/127N27F1u2bMFtt92GP/7jP14LKgDceFgA1XARw8UlWB7Cn8gnKRAnCcFUzvIYVkMsbt6ETUtLGDUjjOsJVpuT4Zw+B95UFQbU+LUKQ2AJ7Z5tiyhfLPQMDgSsBVUru2qF08ZdYutFM1UGZjhABYZt9M28tSeIyhOtTZjl3PqpMqei98z1RzKvrGqAb4xvvWpDWnewMObBQzwgQYpQYQJ5F6sWCN4KptAYJYDDyd6qIP21g3EEJ0oirMV1R9kHHicR+iWt+GzM6XlBCYyCDAXlY6FjNqMrkEPry6XH7K5vdXo9BiFZVmdaomZijFI/UeSCvnzl5lU1E9Hcl2Ym5c6ZLt0eIRiHCdPqH1b/9wWFzpju3izgMPVpXk63gEraIjgk2ly7OHHNZ8f9wtFMFTKQ7DWkWM+PzJKaR+Jv2rQJ99xzD+65557ONJdffjm++MUvrqXqMj7+73A4xMLCIsaTAQg19JHgzMYfLmpQDRYwHAyxecsWDBeGGCwMMTpdg7lG3J+Rz2KW2zw8T0sZTEmzCb80Y9QCKhBwWUMskZa2bNgLBfLn9llj3GZXjxM3cZKH5SNE8tGYFqZ+ASf4nLmoies/yb4Tdpuoo2zWFggrC4fbN0o4CYDirGghnGmSrR8xS4vdBWZAKtFs1UMEdDIXhEd5XiPrlAy4e78CE+0uc37VoZwy8JWpk18W6VNro1S9jHa+t6W1PsOx66Tdca21jHQ3H4k0pDXtdO60h1i7paH1JWmHtnYyT0DRJa2enYmbLQ8wETzydgrysl9RC6ju9TnV/rBugKBFprOmoFB0CCf9u7Q/jIp8aYZHqWVFZvj/KCypdQd+cBYWN2FpaTOsXQBzra5Mt36BzoBQYThcwqZNm3DBRRcDRG7P9OlTsNYd/mlA4XBat5+Ffdgfw1R+mESl1x1MQO5wTyYOCoMfMmZN0vyZ4jNhcvKdCH53eAVjLEw1COVb1CEzJ2hG4VicBKqO0rtc89VuMc0EZmtJHLqxxR9ZfWkxlsj50iqi8Cfj1n0oGx+tiwdNlpMKYzuRjldL4c+tIeEVXjhrNyiLi6xlRZRmtxf62f1NpcgqWdRXiVpKj2LrsQ2Gk7JLQRNRkVFKVQcEclWKnA4qmB+mMeXZhSUMPDNsZqnWXYEIbdzKeBTnNhVoO6TPykqsLULxxPAkbwE7PQDnANp9ns9r2YBOyZyN3qFIj1FRzvpoTjfnxhZScO65U6dO4/jKSdR1g+DaAkDGYGnTMrZsWcK25W3Ytm0bhsMhqmqA06uncWr1FE6v1hiPLeSG3jhBGWB3vI2jGR/TbJwmyuwuHwxmFmy2rhQ1shbTVBAYp9aCvfCNdyJGN1FKzIxqOACzxapYjwwXimhZ0YQwIlaVFZDJJnhZe4pgfBBRglVgFnGKhogvZeESAGVsBbRYBBhROJWe4N01Yq2JAI45E8TJxNu/XD0x4jMc98Qh4FZVnkXcGeMEUhaJB0PJTcgiuAC0PJFsLcgQqqryk9eisQ3IxsixwJYy3WcaU1wTzEge3Me5wJJBkp8tHpPRkV+LyFJ1o9WylDR9RguoZAVNhVxzz+hzejlW0bNOdwbcf4qgykHT8lw1zWHhzd1fc0OmBHkFjJs0DbM7q04diObSq5atBbcNLqQcU6vrBpOJW4cxZoiqkrBxg6Utm7F56xZs2bYNS1s2w1CF8XiCSc0YjWrUE4umZsi+pZTDwJvQzqqKa5IyOePkSrJxm5EEoSZFZBNJ8iVPS1oTy2BzaCMSrdpflUCRNNQx7i1IXRvtZ4p1KH8/hxdR6S8JKlUK52ny1sfK9MTWTkZJEt0FqtWsCD9FOk3ry2dCa6KEJMp12bIWQ92pVumEsA9tL1mTFNNGAamV0CihGEr3yaCLYbfQ0WUHLVjKLzOI5IgoVVer8zJcuhAhCE21LcE0S8pQncxPjxuaz0JP6y/hOrOcMK/17/RRudpsvq9VkZhRnip5Vs7Akc40qjG6brOak3WzpErkkZ1UWsqgXAmbDza2kPLn4NXWomFg67bt2La8gAsuWgYAVIawdeuS52yM1dEE49Fp/ODV11CPx5iMxxiNazSNdRezsYUNE5VRQWuPCedz89XfAuJ4V2Rg+eQ+E2IR7bKUM+gnRMExEBeRKWg2IhfcuoFBOOhJ+0RmIhIJa54c06J2cqYvZSZCkdRmWJ8unxwxmqqD2U7R0oRZi7WWmL9EzgL1IOHi+aJ7sY2hXYiHbhpvdRnf8+Q+Oa4x4lArP66wRJc/S8bnCjmLvGejmNN0/FvrmkgFyrmyDKaW04mep43Z8uG8wBpm8xlDV58RPJ2zjJmbXy7COio9qezPrYHpsMGFlGvq0pYt2HYBYdv2i7Bps0G16A6pJSKMLMNyg8Y2OHXiBEarI6ycPA1bT2CbGnVto3VC6SXx7E1/Yu+qYgBsA9P23qDszCqHGCWWli9PT0I1YQtnYEKCHuS79vOK0u2sLUJwTXmsWa+FcBqTlfvDE9oLvDG3MJRlxIJzZKSz1SOXRuMvBelttVKPIeMNNhVjlgky3aaZTIe8Zgn3Nw0ciKqyXoeWfVaWxSnI/vRz+Jt4VV5J6y0NF4nJAMeAkVAmu03iIvisbdA0Vo1fuQmtNa3MOpH2hUyqg7QCk6Jd0IxZ+koLjXadGsNIUoW9hBTTZKSXAjvGJmgH9UxZ4NOoLChuuRHEab5UKeL4NxvTZB1QpWmDmkvtR/rpjGdzQqkSWQZQcpRKePlfs3pzli7QUhrT/yFuSdA45qPA4Clrbxo2tpACACJs3bGMCy/Ziq07xjDVBDDOlLCwODlZRV1PMKnHOHr0GEarI6yePA1YC2J35DyBMTQms1j9tLdW3SPlzCd3EhEp/kze42YEJTXQZ6Z+aZehTP0gJJSwIHjro1K744239LzCom/gjSeht4XnTCyVXErzqU2NJSuD4P3U0rBYiFcDQjryzILYC4gsaIAywVDElZQ3PBlPxbEMuStesj6wQYyxn0YcrGv2QgjWOjerws2CleC2qPwap/QX+3VJay1sY9GQp9HGwtrGXx/SXo+Q/nC8WlmQyp0iQoVUHmFYiZDLLOj51no01y8LqelKCgWFKqHjQjo5CV7l9PipuvT7hAl2tKGTSFSCIM062qHTJl870vuiOE2sUG2PgU6aU3Yxgjhpb9TGqHA+Q0lcnYWYLBQudO/Vi7y/lFIKIKw9z4vFxhZSXkpsW74AF1xkQAs/QGOBSeMYQGMZx06egm1qNE0DVEMMN1WoMIRtGtimgUENQ8CgYhBbiAONbYNmvIq6cWte1tYgdgcvVcb4K2O8qYto7QhajoZsshbV6QaT5sxsMMl/LoCDye2VsgZVVYXKLSmWxeQSoynGS6zVhSJMPOQHEvfc7CZQtL6y9YG4bkKhltzFJ1ZD7pqbF2ZtnpT3Wls0xsTnPo12AYqikKz7SBnWmV8EcneUWYuJbfw2AXe/WlU3IbhDLLVEIwnCqRumGChwmyfh+/xHsaA+Ba9gxL1+da4J1uL6fl1gPeGyBggL+qJ2aiHkVT1eO78BNrqQAgAiDAdDLCwM0JgKDcdrOywDdd2g8QIJ5C/0GgCN11ANnOFRVQBxHZi7u/6PYNkJO7bx+j7DgJxcgaTjMwYlGnhpsVa5yeadJrmrLppTlJ4zdi7WLgoQmLT8VtWULKmSxpjuwWi7BUQDZW73SQxcSEzVufCmjrD7PF0Lx7x+ZUWSEh4iqFplgsPNvaQEYAicYOutwmj65u6YsAg/TQNH6uJRCQW5Vt5SgMhaYN41MhFQs5KfkzW3M4BzVW8yM+fXPNcdUGF6TeujaLHrcCBOUyTW1No65azuk+qhhx566KGHHyVsbEvKuIXW4eIiNi0tYkJLqCxhyEMA8NaTv2KB/cGejUVzeoJ64qL7mskq2DZgO8Zk0qCpxwCApp7g1MkTGI9GmNRjsG1gAAxAGFQGlTGoKmeZDQYGvIDgcotrRM5GMiZGugCiwXqT2Gu55DWNYJNRqj9Qrk2LFeKvew4L7+LTy9ckcuhY05EzDnWYu7i4ZH9YwEnwSNZHNNJpkETMpJ8pOzJZJ8h8kwRIXHbcb8VpehSsIOkEFRXhakzLl7Y465pilKfSINN9aqklkobtm2CdyMZc7cq0NgZORI9ePGUhW8EAxUTKUs5TtdcyQjoVOJEGRBS++yzSP0aNj9awYzAG4lpdDoGOfI/PsNbKsax5u8T6VkMbHyvnQtv6lHzdwB3fdQXZz8LakPTHbCdtCaYh6Gmpc+4CstevbPkUck1Z250P2u0vAZEe3/YG8mmwwYWUO+F8cWETlpaWYHgJTANAFqXZYrBQwd0RxAC7RevJiRFGq6sYj1Zx+iShnowwWh1jPK4xXh0BACaTMY4dW8FoPEJdjwG2MCAsUoWqIlTGYDhwa0HD4QCAwcD3ZlVVbq8W+TuKSCZpslKPMKieIccVDQ9hTP25dHqR2k8OQwQ2BmbgBKQs0HfPxjaD026rUEeoJjL/3B0W61EMn1Kmwq1zj/JGphF8TiD6ftJXr2euBHGnppF6ys2g5V4uA+HRVg/YekHfxG0IOcg4pi5eJeCC+1YJNs9KjarUWuvWSCGMllpjFvskpkpwaI1v1hceMZKrvTvcfcX9Q9n4JMwlq5f1uBeEEBOUwGi9LkKuGCRsUBWVBicUcPeJKCnT0wgVnFKcP1H9XWSqiu71Wh8B+RUvaRvaZVF82VGXSkdd2MwvbrSMFaVNz8PWOM8bXFOsjAClrEkT53WzbmwhVVvQwKAeW0xWa9RcYzw5hdH4BACgsQ2OHn8Vq6unMDp9Esd++EOMV0c4vbIK29SwdQNrnQByp0s0QRNhtmA7AQAMqiEMOUZThaNACE0NgAmGgKZm5XsXdu8iwBprQcRtvjINkklVfu8Ym4s0EyuOjYGhCtYLavLaM7G+Xm16va01Ih0dKA9bHL8NDDlJwZ+qQGgZSBob2zSQrQBAZPZJ7RzOpU+ESYlBtw0LdT4aaW0XgHHrjvDXntjGFic9FQp2gpVDX4fnzCEcnQAf6MH+QN6oZFRVhUr3OzOABkAVa9K7ewuWVGsEpvO6drsK61IEJO2hEgPNlJkchAaq4tv58CIujO8MCAwX3SJmbTB/KWs9FurscYvlnKlVdHbWlNQet8qkLEEUg8AR1gQbW0jB7WFaOXoM/3sEWK2PYDRZwepoBYCLnDq68hpGqydx+tRJHPdCanR6DAnBM8Q+ug8+Ys+XTPCMowKMv0qR46QRD4e1QNO44ArT+L0vxsBaz+/ER9JJiVO0NBDSqCh5FrVf3wtZ4IQJwsaRjnI7eQsu1FrmOR3dHadByy0VrLGIky40iewpWVbqlQvF9hfksSrVM/ycZYVfnIdwKz3ba3CkGp3zeiJ3R1nQ9kQ71sNHQb6VqglabrJZWX3JowcFt6zHYpdwaq1MhQKXkr1p4LbmOp8mmzlmptKx/hsROutgQmphAYDScrs0H6R7vmJ5iB08BdJtFlr0ZemmlrI2aJ8mktUXyR1ZwohZF0L5fBe3NKaL4dy67Sw3CRLL02kame3+FdjQQqrCAMYS/t83nsF3Dh/BD49/FZP6BCbNKQDOGprUp8F2Amsn4MZdsVFhiMFggMFggE1LQwwGFQaLCxhUFYaV62BDzqXnQiede8btrh7AsUV/pbgFuLaoRg3kXFtmwnBoUVGFsHOhRQEcHzrOnDaObbBYgj1BiXgB4InHEAxHIWUqA2MrJyWtdfuBiOJ+qc7b59KZHpg/QUyqVLNGmQwje5YiY0h5Spgck6hfRm7xVJU0vuQ4FzoIXPMRLSddN6XuItvOaowBLDAw7sQJy2r/kwLddndDCMUXpp1e1vQIzmJjkyoY2kpxbqx8jGzyLGfaXUpH6HOSs2WnXAJfUH4SioicPW9dcEvKGqTQAAP+rJPSDp5EjSjixFPeqdqLz+PaTXYKv06TM8vOqixySk2ITLvvpwBRW9xqmB2BGqhEP41/ztAsSzwEZ1SOWElRWZ3dzh+DzbzMbkPm8WPHMB69gnF9EtaeQmNPSQpUqB3DGwxgBo6oDFUYDAYYDgZYWBigqgyGA0IVNHeXl8VNB4CMHEArwiISKFuLuq6DH5oMUNc1BpW7jTIIm1B4Sogx/FgBGTUlnCZsslEnOBeaM548C7A2WAzGGJC4mEIOg/aEi3ixRiXjKi0/dcQusiVhDMH3rKwWm1kXWjuliJHgT94CcHwvbrAlUCu9hKxba5PJoVFOeJFYM6zflyycMiRrdDCwFK9LL51iLpMyMPpQjVN2yBrAKI1WCdNIktM3MGeiM/IcQlRQKGmy6sPutka3a16/5mbRxcNeGDLlQk6HUaT1p6Vyijtm3Iqd78EKFaS0XaoJUNYElVN1Qyrc2hYQ2paLzhssIk7+ams/rYcyzbCk8OVzNJ3/WdxVkjLVoSVhQUFTllKJI8R+V7+JE160Fhm4wYUUAAZOraxgvPpDDBdWwTwCWRehBwIGQ6CqDAaDymus7iOW1HDoouOqcLkth7KZOZzUXfl1EjnRPO6yclGDTdOEK0KMAep6AB5W4MpTKmmtDgE/TgY2HzpuEXVidgu2enaJm09MEXnVUiTbBK5a77BhZXqUuUmSJykpE1AMHblmAaYwGVu3BeeTMgjxeGpErtQH1x0DTNriknmt/OWAs2aStTbOylPRbrp97P6n8zobxyszoHA/kBakUlZ+MrQjNCsEh9AjpE+acLRRGjGtVXP0j2aDLl2TMh590keiOiSMUFUUGL8uNd3E7vKzKi0HSi6PjfSipXrWwtCPZRGSNVVljUwxlhYFbtJX0hVJWxh6EKeoCKo+X0sQUAUBmrQxFVAxUXkNm+LrBH9R3to45n2QDm6pRaI4JgKO05yCR6iRYy3BY9AhtMTo/LEInGAQGlhsW17C1s3LqMwijAEqMwwpmBoXBWgGcJcEyloTQIZgyB3Xw2FhOpoPcn2CAYO5gYwEM4MJGJB3B6JGM6mjJoQhFobAZMGAKifoIlOeQiLKciGKLgoJeCB/My+MhJn6Wzksg+GjxdjffVXrC+4YxjPWhCxIWlkibAAzaIhDiLnGW5XRWogXKQKQv1JFE2o8fbCBSFgCwZDfjKv+wQuLUBszZDkuaV5MEKcMlzcISh96KQbnD/TMTFkijkFQ+gzuTD4wwIZALFdku+0HTWNRwfj7yqIlK+f4mYF7Fs+INIomydNuDH2RkcxZW3EBK/AZYSDxtwk8OBF30CxZDl3KA2+cHqIZWiYSKBVgHVSW1JlbycjqLEExICiXG6rNae1doNNNmwj5XJEAqsimkzsKGC3hY7xCN3uNpiSC1BxQPCNAvqbN+TOPc9oI/8tmY5Sm0JfBMlwMgDTWwPNLAC7a2vr7+TzvYoN5t+luaCElmtJgUGFhcYABVajMAIMqvrcwYKrANICE+Q484wn7lMCwHXcjMLsoLxsEkJ/szGjQgNnCco2mUe4+YoxGBotLC86KM1XQThO/fkaweVRd+A4kxFdyf6dTL1pAot+zL4NVpeS/l5w4oebC/MzfT5dlWhOLmnvw8qXHVrRqSF8pKV/S9jENF1FthY1miOeuvwwNVl+kj+METtkkJ/jpNhHA6f1i7uw+fxeZFobZGKfuPEKL02X46swJw1fNDAZ4OBBUlcNC6UIn7oWOHdA1yPpPsJpDxaTW2IJ54atRhbEKlkmRnikmSgZL3vgCRcWnxcLn0/JLEEWU/h3fpmcUUjFVeNSFVhg7na3kFlWTjVXQVUtYF5QarYj555yNu54Y7mBoA0Z+DqOj12SJhHnuY802vJAyBhgMDIbDCovDIQaGMaziBhsLC6YBLA38oDLANWK4eRQ+tjDIckkd2MIQYWAqNLU7C3DUTLwQs6jrSRBSo5HBaLSKwWAIA4PFxUUw3EGiWsCkkWaUfY/EYGiAipyWLQvw1t8YDE8QUbMmWDKKKag7rKDD0EVbttBaeaYnq0/WL/OMTyElgf2eIaQTQJIm2iDF0ZH1DSXYOTnj1WPPaStCnyJurnZ/hI3I4q3acM1uNjIb5Y3zPRj+xrJZWcni0pT6mI2SJxWYDeqJDUE2o9EE1XACUy3A+FgXBDcrefe0G0eGs5rhNW+HgftrlNWSdGmCt9xSKdssyB206/s3LkPI2qRQjQgnVYdwq9DVcduvfhzpWvrfZ1ffRIEKTC/wPjeWZy4uZsG5LznOpZwO83QRA0Y6FaiUMH8RCS5k5iwgKgZZ5e0kJD7XLkzVidupkhaVEFe1BchgOFxAbY03ltTc4gGICUQDGBqAkO4VnAUbW0ihAXt3nfN9Ddx+F32bKKyX7gaNdZZPBUDWFdjfGdRYdgIt3IvirlGAnQDcwDa1428M1JMaTd2gbibOtCchCDeUpjIYjyfYtm2E4XCIpYYDo0kZrXejkNI0hPZCC5zmZUGwKoLPVeTwIWOjhmvYEYKxIFoAYxIElqsyEg+j8WJLTqvQGpbHIrvMTOaBTlVU+DwBJxskWwnzxVqXh6qoxaeVUKJB6qg7K2fgSX2Jxk+wMH6NLU5od1WAScsP7KVBcNaQdom2NU9HR1pAUdgS5QJgZD3O0aFtIlOylsANgTEAqAIw8ONBTgojrnYBBDJ+75S/mCoGFhQlVFTM4BQut1QWdy1RNH/C3xhQwhFR9gIn565Ki1fkGx97+mE2kXjS3kNge8qoKImz6apRVzo/M2VecURdxDYQHbCJgAXO6PLCRPnpIn2PS6qnqatdQl8khAyxhpLWBpdrrCgRlcmztL6zgbBu529YbSxguQJzhYoqRAuOwTyGtQbM7r64amBQDebr2w0upGykOu//Z69xOnCziP1hsI31kVRknXCy7uBZay3qhp3LNAgphrU1uBmDbQ3b1GBrYWuLelyjqS0aOwlRdNUgmtJkCKYijEZjbBrXsJbd0UXqCAUJSqCgLfsXLbpxfuFGMWkg1hXLYv+TAapAVMGYCpaiX9lNTKXr6mCE0GWRolMrQL2eh7bCLLOZ1qY1M45PErki93oJc432UGCkYOj7aDhYQqJoyASJfRaFrrQl0zypCpZq7Ct1SaSKAHN7b2LtIvACk/ETmL2gcWtT7mMtxflrxeth4La8VsGKEtoVQeXWuKqkr7RwyWkn2f2iGEriVg6r2NCcMb5LuKhaQ2AOa2ZR4UIiaETwuzvZROCq/g7fnIhobUPinNTmCVnWWyeUgtUhLEq/W307J8R8HZOkNT55j8iYQY2xnpsydmWFRFfC+WPobB3CiQqvS88S3OMctQ37eTgEVQOl4IydF8iGyeP2pFY/BkKKyJ324NaExhhPToN4hBGPVSILC0IDFxbuTpo45VxvtkEzcUKksanzx0l/C+carOGOl4BbEPf+VkNucZCtRVPHkHH/Ek09AniCYcUOD+/jEYZLQnTsfbitCcGev0QKEaEGH0jgtHILtrVHmwGeuA9qgCcgbvw1JHqmumAQ3eqslnQdyEPY7IwyqQvZVh5vFmamZj4l6XWt8Jp+KjyjFco+cg5qo6/PaxxD9eEHkCg80U6J0jPNwqnkSlAaqMAMcspMA6vEX4zCS90z8s+G35C2W29FU+U1Tn8PmWf4Tb2KejKAbVZhzBCAxYAqt6WgGoTrPRgWYONsoIxPRb2izHwIBNmsxz5gJXlfuqQuUW2knkbVQYEIXMBKekqI1Kax6NZucunU9f7sNP+1QKAt2WOYY6ToT2+Riyes2NQ61HmzeahqVWnyd/LefXSUopzIMfNkjkT6lspeOxAYVeVoupGTVAiOZ8rcMjUqamAqxrhuYJsRmnoVzXg0Vx0bW0ghKjqOIcrxRjY8A7O7tpud+87dhFq77/6eKbbWmaqq7OA2sl5IBQHDbrMpJAzBp5PNt4BikM5NFBl6BwGJssv5lR4c3qV5GQmhJeswEScKfCRPK+3rwinikUcGa+wKMkyl4fheZ2T9NQ3HT+pvlaq1dteOGDmW7i9yQiDvL2HC8h0uGiph8PHTFsLxWSkKS8oOgeMSSZm53AT34HYU2mELHWkpfZf2sYwrpUURYn1FKEV15u/J96kSPnqw8jZk1CAh0PnYcpJeI6DamaXJdlchH4kzgVLftGi5M22KFyNTkGT+qjkY50BbaOeiSNfTOp4qmxf6MYe/nPzVUG5Lx5yX1LM6K8MnOLm9DC3zOlaGrU2XAWZAf1VHDz300EMP6xY2tCXlbjW0qEcnMBocxRg/ANtTsM2pmAQWTASLyltNDbheda4uvyYliiwThYVS50lnEGrAXX/oFWRxqBK4aSDuIVNwtjBOgDEEsOKNdAnyQDiJoh3h58FIvJPSR0OQhixK+v1Dhv2+BDjNvRqhHkxA9QSoRrBcu5Mo2J1VKIs8bB3+NripAH1lRG6PlJSp3KiQyD3jVaq4wtPOFK287LWEpnpT2f3xi/1qfMLyv9dimdyNuXEvVVyBi3qwMumUpaDdfYDb80TEIOOPvoKnJTCM4UTb1aG18H0ork73xiT/KrKQq1gqWoAhA1NtgjENDDVufNmAeAhDwNA4UndrO+3WRGM768vQNpWWoIIBtKUe6Y8AH5Sj+o/8JuVET47uLPJnToW6ZA+dWjwPpq6gx7klnbnVQvFrs6Ryz0U6OmssR+GlKScxE0m6PloLRtWtCwwtkYVWbTSRSldosli5sl9O1zu9h1ILLH+Wv5kfxCvTgABUpgKjBtMqbDMO74lOgKoJ6skQljeDMMGgYlRzSp8NLaTIArDs74Y6DeYTaJoTaCYnAcAz4Mbtk0Ll7vBhGyL22FrA71Fh6yZZEFIEVGThTqP26znMYOtPAyACy4lyHE/4dpnh5/8EzCMwjxHcg+SJUQdLhEwpo5N/kaEK4Ue3kFsjYsDEzbxVZYHGgg2jMn5TKlkQbHIauhNOaeC9UUKKmVq0rJtYHpQ4/yRDWCNKSuA0narBsg1My8B3WHBrRVaRuGiCu8VfVQ23902dugS0AiU4EZjurbhoXX8bYsRusEH4MLUnfhpFJcxZOqUCceV2kVBc2yE07kN1VA7YgsmEMAyhSYnszVrhcG/1LwUB5N7LF0oKaK8bIRm8uDYp/Z/mDVeMUKZkJbhwiIJNGKbWvwoBCmfCNmPedHwKp/vMKiCe2B/mthLAiO9E9jKLkibxsu3ZInhF3UBpCVqlKjSegtJGAQf4+iISueTL6HRN/TBPYgZ44qsf+llpAYxifhoBNEHDq2Aew8C6QxXyc946YEMLKUMDAENMxpsALKGxm2FrRu1u2ACD0aAGkwl6NwEYkF/kZg6aKfu9MQLsdAIAfo8UGheoYQh1A6Bhv+/JMTOyaXQfVQZ1sxN1swMN74bsDRA6DPcLhcktEWC+bXAbgJ2wsAEPV0ElB50DMI4BhBU1BvMIFmM0OI1Jc9qFyjdR0MLK4bjCgFPGYYyPOIRJpk8X17D6HatnUqZirPEUcJdYMw+pw5KPCCSE0GUd/hzyZngwA5abUIfV5xYWmKGOXhP+Dcshv1M02FnY8GWrf7Feqc+foqEuuYzh5xUYAzAqVLwY8FidLAGjTVia7ASZRQyGm5wotIRmYnykqItadSzOyqb9AIYl+k/1ufQOe/zgutRyFHoMF9EqAjqx6r00jEEV8Z10dliz0n2arJfFGMz4uMz4ZCdJqZy1CisW/FtPXWnx/N+ygAj1cqTJhE4pvte/dV22K4GsRdr4m7wWxMbPB6awEJNsdtfPRHmSSHe1np0saWW4EcMpWEqBawWBKCOPVb5220V5bjwZVLA8gMUA1SCeOGFxEtyMUfMFsLgENLgYtf1fjEYLeccVYUMLKYsK4CHqZgtQb0fTXALbbEbTbHUJmNFgAibj9smEiWyCRUGy9ygZEgGGLGhLuHNcfFcuHyNBAuR/ukNsm+ZSTOodmEz2wBiDcBYcCRN3xCibKRMhZU3QnABZVJdje4xj/L4t3hSUJqNuRqibMWq7iok95YSUbYJVA783zFqbz0vn4mICGeMiGaUnQro2y+g6VF10RomGlH5mFSFHeXqSSeSbHOyJqCEWAxcg8tcqQejHKcecfJ9zdhgtw1nXHje9cdedzyhl2yAo5T18FJ7D2Ee6ecZO1m2wtlTBUgU0CyC/z8k0S6BmE0b1TqBahLFLQaDA7+0iNoDfoO1u9FUaN+CPYEL6zJtQHBoACS5Mxssisxhib3pGJmUzsgN+nPIkihaSAsI4xRGgFsNMxi8wxmzPFJKvc0Fb3MhTqaskmPLvMR0Tp8IC3iOi3re0NGmUkz4qr0gVaaNSjv04yfYGIiT1sg9tJyZfJ4N1XlLfAw6ZsGeEU/op1ctVF4iCloJOHyIJyfMmcksYTkgZuJhqBw2fBHiCxl6Axu5Gbbej5m2w2Ip5YEMLqQYGzJsxGu9E3TRgO4K1I7B1oY1uzCfBqSUutMa6wTccprOf2XJem4JwskPU1AEEK4fIHQ2omZapKgxpgPF4D0aj7Vhd3YOqqkAmCeCFH1lfnsdFM5ug4jJIEUM+4V0gYRO+TyZj1PUE43oVq5NTqMdjl5c5aJHMctQTqzZHXKhRfRNaprXq2RD7hLL5o0PEY+mByYXDCaQDYl4tfCJk2jtHBUKn1fcBidcr5PT4WLEyOa1PhHpy9XtiTUWhZsK5fRE/wwYVVzDWoLJVeNdUW1CbJQxXL8YYi2iqzX7N03cCRcWK4c5HE4s+tCVRwORZ7A/dRBAl5xvKi4yVSYcAcm0KpUIqvWoh0gVnxXDyYAr9FHTEeelsFrh2K+EQ+meKkBKNSdGmVrTkGULZ8eTJhC6K3D4trxO8MqWx6BIg84JiL1Mh0E1B9qpUXli67S8wBOYKLv7ZxvnWjACuYegCNHYHVid7MOHXYM3puXDe0ELK2QULYN4K5gtAtAtVVYOMHLfBsJgEl4MxfjJb4xVFCpYUwGDTFlLk0wWbmNzpBuFUa88w5Iw/hxXBWuDV1zZh5QTjtR8ehbjuI29Wo+7xas/KuGdHaywS/K4DAxKStxbc+OtD6hrcOO3bMFruNS183FdK5p5mXno/yDyQCLbkWWRe2tevmWjYUgCTTWp1moOC6B6iYD0Ig+Dw1GmbJfxFNrIEXvgjCDQ7CxZUcVUbIeAjXJvi8xC8ZerTaYt5MGhgqtNY+N7/unvAqoG6+CruGHN5xJJKrW5nFabjkjK2CCWrN38U6Auq10UAJYVKnxYqyuoKm5rLSQuEMQcTnwO6rHyB9gG07v8p1a4RKAsWkqARVWRiLWbvtLws7jvj+Dt4YeRxiTZb+MkfGbsC35lVTHA5RiHFxLDsLP6BulPN2BrEFpZfgbXLqPmHOPLyadS8eTau2KBCKt7wugKGO+DVMAMY+MsK3SnobhCrcJWCCcLAH5FjnfAJtEFN68I6ssIA/cdxMnBYH/JMSW2WlfWQU6canDq9imPHR35h1Yb3aSVhgQl6n0S8g6jxJ59rzT2NMAttA+KxTzY59AjEsXkyGZO1FUgzKRhYyd6Jc6XZZgZUwlCDMBfcVMVTtboMv1YapXx0LNKbYDXF58bkZkc3RP4ax08sufSG5XhmGcMvoJqTjjWyZ6zeUrKiiIjQEpdPvnukW1YkYAud11Jy5oZuIaXH03VBPLuveEVDx2Gj817n0IWd4DAtQcnqPJv6nWIpE63b7Dmjss8RjnrsWvfEzZJQnlHI2lYIKALD+vXRgYn9WrE7B6xuFsHYBsYp1HYzGjuZq74NKaRWVlb8t2uABhg3wHhqjh566KGHHtYjrKysYPv27Z3viee9aH4dgbUWhw8fxlVXXYXvfve7WF5ePt8obVg4fvw43vSmN/X9eA6g78tzA30/njtYz33JzFhZWcGll1461WOxIS0pYwze8IY3AACWl5fXXedvROj78dxB35fnBvp+PHewXvtymgUl0B+L1EMPPfTQw7qFXkj10EMPPfSwbmHDCqnFxUXcddddWFxcPN+obGjo+/HcQd+X5wb6fjx38H+hLzdk4EQPPfTQQw8/HrBhLakeeuihhx7+70MvpHrooYceeli30AupHnrooYce1i30QqqHHnrooYd1C72Q6qGHHnroYd3ChhRS99xzD37iJ34CmzZtwvXXX4+vf/3r5xuldQ+f/OQnw82v8rnyyivD+9XVVRw4cAAXXXQRtm7diltuuQUvv/zyecR4fcATTzyBX/u1X8Oll14KIsI//dM/Je+ZGXfeeSf27NmDpaUl7Nu3D9/+9reTNK+99hpuvfVWLC8vY8eOHfjgBz+IEydOvI6tWB8wqy8/8IEPtGj0xhtvTNL0fQncfffdeNvb3oZt27Zh586deM973oPDhw8naeaZzy+++CJuvvlmbN68GTt37sTHP/5x1HX9ejZlLthwQuof/uEf8NGPfhR33XUX/u3f/g3XXnstbrjhBrzyyivnG7V1Dz/7sz+Ll156KXy+8pWvhHcf+chH8IUvfAEPPPAADh48iO9///t473vfex6xXR9w8uRJXHvttbjnnnuK7z/1qU/hr/7qr/C3f/u3eOqpp7BlyxbccMMNWF1dDWluvfVWPPvss3jkkUfw0EMP4YknnsCHPvSh16sJ6wZm9SUA3HjjjQmNfvazn03e930JHDx4EAcOHMCTTz6JRx55BJPJBPv378fJkydDmlnzuWka3HzzzRiPx/ja176GT3/607j//vtx5513no8mTQfeYPD2t7+dDxw4EH43TcOXXnop33333ecRq/UPd911F1977bXFd0ePHuXhcMgPPPBAePaf//mfDIAPHTr0OmG4/gEAP/jgg+G3tZZ3797Nf/7nfx6eHT16lBcXF/mzn/0sMzM/99xzDIC/8Y1vhDT//M//zETE3/ve91433Ncb5H3JzHzbbbfxu9/97s48fV+W4ZVXXmEAfPDgQWaebz5/8YtfZGMMHzlyJKS59957eXl5mUej0evbgBmwoSyp8XiMp59+Gvv27QvPjDHYt28fDh06dB4x2xjw7W9/G5deeine/OY349Zbb8WLL74IAHj66acxmUySfr3yyitx2WWX9f06BV544QUcOXIk6bft27fj+uuvD/126NAh7NixA7/4i78Y0uzbtw/GGDz11FOvO87rHR5//HHs3LkTP/3TP43bb78dr776anjX92UZjh07BgC48MILAcw3nw8dOoRrrrkGu3btCmluuOEGHD9+HM8+++zriP1s2FBC6gc/+AGapkk6FgB27dqFI0eOnCesNgZcf/31uP/++/Hwww/j3nvvxQsvvIBf+qVfwsrKCo4cOYKFhQXs2LEjydP363SQvplGj0eOHMHOnTuT94PBABdeeGHftxnceOON+Pu//3s8+uij+LM/+zMcPHgQN910E5rGXRLZ92UbrLX48Ic/jHe+8524+uqrAWCu+XzkyJEi3cq79QQb8qqOHtYON910U/j+1re+Fddffz0uv/xy/OM//iOWlpbOI2Y99ODgN37jN8L3a665Bm9961vxkz/5k3j88cfxrne96zxitn7hwIED+Na3vpWsL/9fgw1lSV188cWoqqoVpfLyyy9j9+7d5wmrjQk7duzAT/3UT+H555/H7t27MR6PcfTo0SRN36/TQfpmGj3u3r27FdRT1zVee+21vm9nwJvf/GZcfPHFeP755wH0fZnDHXfcgYceeghf/vKX8cY3vjE8n2c+7969u0i38m49wYYSUgsLC7juuuvw6KOPhmfWWjz66KPYu3fvecRs48GJEyfwne98B3v27MF1112H4XCY9Ovhw4fx4osv9v06Ba644grs3r076bfjx4/jqaeeCv22d+9eHD16FE8//XRI89hjj8Fai+uvv/51x3kjwf/8z//g1VdfxZ49ewD0fSnAzLjjjjvw4IMP4rHHHsMVV1yRvJ9nPu/duxff/OY3E6H/yCOPYHl5GVddddXr05B54XxHbqwVPve5z/Hi4iLff//9/Nxzz/GHPvQh3rFjRxKl0kMbPvaxj/Hjjz/OL7zwAn/1q1/lffv28cUXX8yvvPIKMzP/zu/8Dl922WX82GOP8b/+67/y3r17ee/evecZ6/MPKysr/Mwzz/AzzzzDAPgv/uIv+JlnnuH//u//ZmbmP/3TP+UdO3bw5z//ef6P//gPfve7381XXHEFnz59OpRx44038s///M/zU089xV/5ylf4LW95C7///e8/X006bzCtL1dWVvj3f//3+dChQ/zCCy/wl770Jf6FX/gFfstb3sKrq6uhjL4vmW+//Xbevn07P/744/zSSy+Fz6lTp0KaWfO5rmu++uqref/+/fzv//7v/PDDD/Mll1zCn/jEJ85Hk6bChhNSzMx//dd/zZdddhkvLCzw29/+dn7yySfPN0rrHt73vvfxnj17eGFhgd/whjfw+973Pn7++efD+9OnT/Pv/u7v8gUXXMCbN2/mX//1X+eXXnrpPGK8PuDLX/4yA2h9brvtNmZ2Yeh/9Ed/xLt27eLFxUV+17vexYcPH07KePXVV/n9738/b926lZeXl/m3fuu3eGVl5Ty05vzCtL48deoU79+/ny+55BIeDod8+eWX82//9m+3lM++L7nYhwD4vvvuC2nmmc//9V//xTfddBMvLS3xxRdfzB/72Md4Mpm8zq2ZDf19Uj300EMPPaxb2FBrUj300EMPPfx4QS+keuihhx56WLfQC6keeuihhx7WLfRCqoceeuihh3ULvZDqoYceeuhh3UIvpHrooYceeli30AupHnrooYce1i30QqqHHnrooYd1C72Q6qGHHnroYd1CL6R66KGHHnpYt9ALqR566KGHHtYt/H9J7aHmAX/1jQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras.preprocessing import image\n",
        "from matplotlib import pyplot as plt\n",
        "import csv\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\n",
        " \n",
        "# assign directory\n",
        "directory = 'F:\\\\Insaid\\\\ASL\\\\data\\\\asl_alphabet_test\\\\asl_alphabet_test\\\\'\n",
        "line = \"\"\n",
        "# iterate over files in\n",
        "# that directory\n",
        "for filename in os.scandir(directory):\n",
        "        count = 0\n",
        "        if filename.is_file():\n",
        "            path = directory + str(filename.name)\n",
        "            #print(filename.name)\n",
        "            img = tf.keras.utils.load_img(path,target_size=(224,224))\n",
        "            img = np.asarray(img)\n",
        "            plt.imshow(img)\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "            output = saved_model.predict(img)\n",
        "            cat = category(output)\n",
        "            opline = str((filename.name)+\",\" +cat)\n",
        "            line= line + (opline) + '\\n'\n",
        "            #print((line))\n",
        "\n",
        "print((line))\n",
        "writetocsv(line)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 ('tf_gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e158130c5a60551ab02219269a3ba6f0221d66c9004fc00f556200d0c1fafd6e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
